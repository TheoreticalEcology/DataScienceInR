[
  {
    "path": "index.html",
    "id": "webexercises",
    "chapter": "Webexercises",
    "heading": "Webexercises",
    "text": "Web Exercise template created #PsyTeachR team University Glasgow, based ideas Software Carpentry. template shows instructors can easily create interactive web documents students can use self-guided learning.webexercises package provides number functions use inline R code code chunk options create HTML widgets (text boxes, pull menus, buttons reveal hidden content). Examples given next. Render book see work.NOTE: use widgets compiled HTML files, need JavaScript-enabled browser.",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "example-questions",
    "chapter": "Webexercises",
    "heading": "0.1 Example Questions",
    "text": "functions optimised used inline r code, can also use code chunks setting chunk option results = 'asis' using cat() display result widget.function loads package already computer? install.packageinstall.packageslibrarylibraries",
    "code": "\n# echo = FALSE, results = 'asis'\nopts <- c(\"install.package\", \n            \"install.packages\", \n            answer = \"library\", \n            \"libraries\")\n\nq1 <- mcq(opts)\n\ncat(\"What function loads a package that is already on your computer?\", q1)"
  },
  {
    "path": "index.html",
    "id": "fill-in-the-blanks-fitb",
    "chapter": "Webexercises",
    "heading": "0.1.1 Fill-In-The-Blanks (fitb())",
    "text": "Create fill---blank questions using fitb(), providing answer first argument.2 + 2 can also create questions dynamically, using variables R session.square root 49 : blanks case-sensitive; care case, use argument ignore_case = TRUE.letter D? want ignore differences whitespace use, use argument ignore_ws = TRUE (default) include spaces answer anywhere acceptable.load tidyverse package? can set one possible correct answer setting answers vector.Type vowel: can use regular expressions test answers complex rules.Type 3 letters: ",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "multiple-choice-mcq",
    "chapter": "Webexercises",
    "heading": "0.1.2 Multiple Choice (mcq())",
    "text": "\"Never gonna give , never gonna: let goturn downrun awaylet \"\"bless rainsguess rainssense rain Africa\" -Toto",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "true-or-false-torf",
    "chapter": "Webexercises",
    "heading": "0.1.3 True or False (torf())",
    "text": "True False? can permute values vector using sample(). TRUEFALSE",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "longer-mcqs-longmcq",
    "chapter": "Webexercises",
    "heading": "0.1.4 Longer MCQs (longmcq())",
    "text": "answers long, sometimes drop-select box gets formatted oddly. can use longmcq() deal . Since answers long, probably best set options inside R chunk echo=FALSE.p-value?true 95% confidence interval mean?",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "checked-sections",
    "chapter": "Webexercises",
    "heading": "0.2 Checked sections",
    "text": "Create sections class webex-check add button hides feedback pressed. Add class webex-box draw box around section (use styles).going learn lot: TRUEFALSE",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "hidden-solutions-and-hints",
    "chapter": "Webexercises",
    "heading": "0.3 Hidden solutions and hints",
    "text": "can fence solution area hidden behind button using hide() solution unhide() , inline R code. Pass text want appear button hide() function.solution RMarkdown code chunk, instead using hide() unhide(), simply set webex.hide chunk option TRUE, set string wish display button.Recreate scatterplot , using built-cars dataset.See documentation plot() (?plot)",
    "code": "\nplot(cars$speed, cars$dist)"
  },
  {
    "objectID": "1A-Exercise.html#setting-up-the-working-environment-in-rstudio",
    "href": "1A-Exercise.html#setting-up-the-working-environment-in-rstudio",
    "title": "Exercise - R basics",
    "section": "Setting up the working environment in RStudio",
    "text": "Setting up the working environment in RStudio\nYour first task is to open RStudio and create a new project for the course.\n\nClick the ‘File’ button in the menu, then ‘New Project’ (or the second icon in the bar below the menu “Create a project”).\nClick “New Directory”.\nClick “New Project”.\nType in the name of the directory to store your project, e.g. “IntroStatsR”.\n“Browse” to the folder on your computer where you want to have your project created.\nClick the “Create Project” button.\n\n\n\n\n\n\nFor all exercises during this week, use this project! You can open it via the file system as follows (please try this out now):\n\n(Exit RStudio).\nNavigate to the directory where you created your project.\nDouble click on the “IntroStatsR.Rproj” file in that directory.\n\nYou should now be back to RStudio in your project.\nIn the directory of the R project, generate a folder “scripts” and a folder “data”. You can do this either in the file directory or in RStudio. For the latter:\n\nGo to the “Files” panel in R Studio (bottom right panel).\nClick the icon “New Folder” in the upper left corner.\nEnter the folder name.\nThe new folder is now visible in your project directory.\n\nThe idea is that you will create an R script for each exercise and save all these files in the scripts folder. You can do this as follows:\n\nClick the “File” button in the menu, then “New File” and “R Script” (or the first icon in the bar below the menu and then “R Script” in the dropdown menu).\nClick the “File” button in the menu, then “Save” (or the “Save” icon in the menu).\nNavigate to your scripts folder.\nEnter the file name, e.g. “Exercise_01.R”.\nSave the file."
  },
  {
    "objectID": "1A-Exercise.html#a-few-hints-before-you-can-start",
    "href": "1A-Exercise.html#a-few-hints-before-you-can-start",
    "title": "Exercise - R basics",
    "section": "A few hints before you can start",
    "text": "A few hints before you can start\nRemember the different ways of running code:\n\nclick the “Run” button in the top right corner of the top left panel (code editor) OR\nhit “Ctrl”+“Enter” (MAC: “Cmd”+“Return”)\n\nRStudio will then run\n\nthe code that is currently marked OR\nthe line of code where the text cursor currently is (simply click into that line)\n\nIf you face any problems with executing the code, check the following:\n\nall brackets closed?\ncapital letters instead of small letters?\ncomma is missing?\nif RStudio shows red attention signs (next to the code line number), take it seriously\ndo you see a “+” (instead of a “&gt;”) in the console? stop executions with “esc” key and then try again.\n\nHave a look at the shortcuts by clicking “Tools” and than “Keybord Shortcuts Help”!!"
  },
  {
    "objectID": "1A-Exercise.html#basic-data-structures-in-r",
    "href": "1A-Exercise.html#basic-data-structures-in-r",
    "title": "Exercise - R basics",
    "section": "Basic data structures in R",
    "text": "Basic data structures in R\nBefore we work with real data, we should first recap important data structures in R\nA single value (type does not matter) is called a scalar (it is just one value):\n\na = 5\nprint(a)\n## [1] 5\n\nthis_letter = \"A\"\nprint(this_letter)\n## [1] \"A\"\n\n\n\n&lt;- and = are assignment operators, they are equivalent and are used to assign values, data, or objects to a variable.\nIn R you can use any type of name for a variable, you can even mix numbers and dots in the name: test5 or test.5, but there is one restriction, no special symbols (as they are usually operators or functions) and a name cannot start with a number, for example 5test will throw an error.\nHowever, usually we want to assign several values to a variable. For example, a dataset consists of several columns (=variables). We can use the function c(...) to connect (or concatenate) several values:\n\nage = c(20, 50, 30, 70)\nprint(age)\n## [1] 20 50 30 70\nnames = c(\"Anna\", \"Daniel\", \"Martin\", \"Laura\")\nprint(names)\n## [1] \"Anna\"   \"Daniel\" \"Martin\" \"Laura\"\n\n\nVectors\n\n\nYou can only concatenate values from the same data type! If they are different, all will be casted to the same data type!\n\nprint(c(\"Age\", 5, TRUE))\n## [1] \"Age\"  \"5\"    \"TRUE\"\n\nThe c(...) function returns a vector which is a one-dimensional array. You can access elements of the vector by using the square brackets [which_element]:\n\nage[2] # second element\n## [1] 50\nage[1] # first element\n## [1] 20\n\nThis is known as indexing. And there are a few tricks:\n\nUse [-n] to return all elements except for n:\n\nage[-2] # return all except for the second element\n## [1] 20 30 70\n\nUse another vector to return several elements at once:\n\nage[c(1, 3)] # return first and third elements\n## [1] 20 30\nage[-c(1,3)] # return all elements except for first and third elements\n## [1] 50 70\n\nUse &lt;- or = to re-assign/change elements in your vector\n\nage[2] = 99\nprint(age)\n## [1] 20 99 30 70\n\n\n\n\nThe : operator in R is not the division operator. It actually creates a range of integer values with start:end:\n\n1:5\n## [1] 1 2 3 4 5\n\nWhich is really useful for indexing:\n\nage[1:3]\n## [1] 20 99 30\n\n\n\nMatrix\nUsually a dataset consist not of only one variable/vector but of several variables (columns) and observations (rows), for example:\n\nage = c(20, 30, 32, 40)\nweight = c(60, 70, 72, 80)\n\nwe can use higher order data structures to combine these variables in a two dimensional array (like we would, for example, do in excel) using the matrix(...) function:\n\ndataset = matrix(NA, 4, 2)\ndataset # empty dataset\n##      [,1] [,2]\n## [1,]   NA   NA\n## [2,]   NA   NA\n## [3,]   NA   NA\n## [4,]   NA   NA\ndataset[,1] = age\ndataset[,2] = weight\ndataset\n##      [,1] [,2]\n## [1,]   20   60\n## [2,]   30   70\n## [3,]   32   72\n## [4,]   40   80\n\nSimilar to a vector we can index certain elements in the matrix or at the same time entire rows or columns. Since is has now two dimensions, we change [i] to [row_i, col_j]. The first argument specifies which row and the second argument which column should be returned. There are again a few handy tricks, above we left the rows empty (dataset[,1]) which will R interpret as “use all rows”, in that way we can print/return entire columns or rows:\n\ndataset[,1] # first column\n## [1] 20 30 32 40\ndataset[1,] # first row\n## [1] 20 60\n\n\n\nDon’t worry, you don’t have to create your own data sets like we did in this section. When you import your data into R, it is automatically returned as a matrix (or as data.frame, see below).\nA limitation of the matrix() is that is can only consist of one data type (like the vectors), if we mix the data types, all will be cast to the same data type:\n\ncbind(age, names)\n##      age  names   \n## [1,] \"20\" \"Anna\"  \n## [2,] \"30\" \"Daniel\"\n## [3,] \"32\" \"Martin\"\n## [4,] \"40\" \"Laura\"\n\n\n\ncbind() is a function that combines columns (“column binds”), it can be used as a shortcut to create a matrix from several vectors. Another important command is rbind(...) which combines vectors (or matrices) over their rows:\n\nrbind(age, names)\n##       [,1]   [,2]     [,3]     [,4]   \n## age   \"20\"   \"30\"     \"32\"     \"40\"   \n## names \"Anna\" \"Daniel\" \"Martin\" \"Laura\"\n\n\n\nData.frames\nThe data.frame() can handle variables with different data types. Data.frames are similar to matrices, they are two dimensional and the indexing is the same:\n\ndf = data.frame(age, names, weight)\ndf\n##   age  names weight\n## 1  20   Anna     60\n## 2  30 Daniel     70\n## 3  32 Martin     72\n## 4  40  Laura     80\nstr(df)\n## 'data.frame':    4 obs. of  3 variables:\n##  $ age   : num  20 30 32 40\n##  $ names : chr  \"Anna\" \"Daniel\" \"Martin\" \"Laura\"\n##  $ weight: num  60 70 72 80\n\n(we will talk below more about data.frames)"
  },
  {
    "objectID": "1A-Exercise.html#getting-an-overview-of-a-dataset",
    "href": "1A-Exercise.html#getting-an-overview-of-a-dataset",
    "title": "Exercise - R basics",
    "section": "Getting an overview of a dataset",
    "text": "Getting an overview of a dataset\nWe work with the airquality dataset:\n\ndat = airquality\n\n\n\nSeveral example datasets are already available in R. The airquality dataset with daily air quality measurements (see ?airquality). Another famous dataset is the iris dataset with flower trait measurements for three species (see ?iris).\nCopy the code into your code editor and execute it.\nBefore working with a dataset, you should always get an overview of it. Helpful functions for this are:\n\nstr()\nView()\nhead() and tail()\n\nTry out these functions and provide answers to the following questions:\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhat is the most common atomic class in the airquality dataset? integernumericcharacterfactor\nHow many rows does the dataset have? \nWhat is the last value in the column “Temp”? \n\n\nTo see all this, run\n\ndat = airquality\nView(dat)\nstr(dat)\nhead(dat)\ntail(dat)\n\n\n\n\nHints:\n\nRun str(airquality)\nSee ?nrow or ?dim\nRun tail(airquality$Temp)\n\n\n\nClick here to see the solution\n\nWhat is the most common atomic class in the airquality dataset?\n\ninteger\nfunction str() helps to find this out\n\nHow many rows does the dataset have?\n\n153\nthis is easiest to see when using the function str(dat)\ndim(dat) or nrow(dat) give the same information\n\nWhat is the last value in the column “Temp”?\n\n68\ntail(dat) helps to find this out very fast"
  },
  {
    "objectID": "1A-Exercise.html#accessing-rows-and-columns-of-a-data-frame",
    "href": "1A-Exercise.html#accessing-rows-and-columns-of-a-data-frame",
    "title": "Exercise - R basics",
    "section": "Accessing rows and columns of a data frame",
    "text": "Accessing rows and columns of a data frame\nYou have seen how you can use squared brackets [ ] and the dollar sign $ to extract parts of your data. Some people find this confusing, so let’s repeat the basic concepts:\n\nsquared brackets are used as follows: data[rowNumber, columnNumber]\nthe dollar sign helps to extract colums with their name (good for readability): data$columnName\nthis syntax can also be used to assign new columns, simply use a new column name and the assign operator: data$newColName &lt;-)\n\n\n\n\n\n\n\nQuestion\n\n\n\nThe following lines of code assess parts of the data frame. Try out what they do and sort the code lines and their meaning:\nWhich of the following commands\n\ndat[2, ]\ndat[, 2]\ndat[, 1]\ndat$Ozone\nnew = dat[, 3] + dat[, 4]\ndat$new = dat[, 3] + dat[, 4]\ndat$NAs = NA\nNA -&gt; dat$NAs \n\nwill get you\n\nget the second row\nget column Ozone\ngenerate a new column with NA’s\ncalculate the sum of columns 3 and 4 and assign to a new column\n\n\n\n\n\nHint: Some of the code lines actually do the same; chose the preferred way in these cases.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nget second row\n\ndat[2, ] is correct\ndat[, 2] gives the second column\n\nget column Ozone\n\ndat$Ozone is the best option\ndat[, 1] gives the same result, but is much harder to understand later on\n\ngenerate a new column with NA’s\n\ndat$NAs = NA is the best option\nNA -&gt; dat$NAs does the same, but the preferred syntax in R is having the new variable on the left hand side (the arrow should face to the left not right)\n\ncalculate the sum of columns 3 and 4 and assign to a new column\n\ndat$new = dat[, 3] + dat[, 4] is correct\nnew = dat[, 3] + dat[, 4] creates a new object but not a new column in the existing data frame"
  },
  {
    "objectID": "1A-Exercise.html#filtering-data",
    "href": "1A-Exercise.html#filtering-data",
    "title": "Exercise - R basics",
    "section": "Filtering data",
    "text": "Filtering data\nTo use the data, you must also be able to filter it. For example, we may be interested in hot days in July and August only. Hot days are typically defined as days with a temperature equal or &gt; 30°C (or 86°F as in the dataset here). Imagine, your colleague tried to query the data accordingly. She/he also found a mistake in each of the first 4 rows and wants to exclude these, but she/he is very new to R and made a few common errors in the following code:\n\n# Return only rows where the temperature is exactly is 86\ndat[dat$Temp = 86, ]\n\n# Return only rows where the temperature is equal or larger than 86\ndat[dat$Temp &gt;= 86]\n\n# Exclude rows 1 through 4\ndat[-1:4, ]\n\n# Return only rows for the months 7 or 8\ndat[dat$Month == 7 | 8, ]\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you fix his/her mistakes? These hints may help you:\n\nrows or columns can be excluded, if the numbers are given as negative numbers\n== means “equals”\n& means “AND”\n| means “OR” (press “AltGr”+“&lt;” to produce |, or “option”+“7” on MacOS)\nexecuting the erroneous code may help you to spot the problem\nrun parts of the code if you don’t understand what the code does\nthe last question is a bit trickier, no problem if you don’t find a solution\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the corrected code:\n\n# Return only rows where the temperature is exactly is 86\ndat[dat$Temp == 86, ]\n\n# Return only rows where the temperature is equal or larger than 86\ndat[dat$Temp &gt;= 86, ]\n\n# Exclude rows 1 through 4\ndat[-(1:4), ]\n\n# Return only rows for the months 7 or 8\ndat[dat$Month == 7 | dat$Month == 8, ]\ndat[dat$Month %in% 7:8, ] # alternative expression\n\n\n\n\n\n\nThe %in% operator is useful when you want to check whether a value is inside a vector or not:\n\n5 %in% c(1, 2, 3, 4, 5)\n## [1] TRUE\n\nWe will discuss the results together.\nWhen you are finished, save your R script!"
  },
  {
    "objectID": "1A-Exercise.html#last-task---install-ecodata-package",
    "href": "1A-Exercise.html#last-task---install-ecodata-package",
    "title": "Exercise - R basics",
    "section": "Last task - Install EcoData package",
    "text": "Last task - Install EcoData package\nDuring the course, we will use some datasets that we compiled in the \\(EcoData\\) package. To access the datasets, you need to install the package from github. To do this, you will also need to install the \\(devtools\\) package.\nTry the following code to install the two packages:\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = F, build_vignettes = F)\nlibrary(EcoData)\n\nRemember that you have to install a package only once. If you open R Studio the next time, it is enough to run library(EcoData).\n\nAlternative ways to get EcoData\nIf the installation didn’t work, download the package file manually from\nhttps://github.com/TheoreticalEcology/ecodata/releases/download/v0.2.1/EcoData_0.2.1.tar.gz\nStore the file on your computer in the same folder where you created your R project. Then run the following code:\n\ninstall.packages(\"EcoData_0.2.1.tar.gz\", \n                 repos = NULL, type = \"source\")\nlibrary(EcoData)\n\nIf this wasn’t successful either, you can download the combined datasets from elearning (see Organisation and every-day material)\nStore the file on your computer in the same folder where you created your R project. Then run the following code:\n\nload(\"EcoData.Rdata\")\n\n(Note that you will not be able to access the dataset descriptions when you use this option)."
  },
  {
    "objectID": "1A-Exercise.html#bonus---advanced-programming",
    "href": "1A-Exercise.html#bonus---advanced-programming",
    "title": "Exercise - R basics",
    "section": "Bonus - Advanced programming",
    "text": "Bonus - Advanced programming\nUntil now we have only learned how to use functions and indexing of data structures. But what are functions?\n\nFunctions\nA functions are self contained blocks of code that do something, for example, the average of a vector is given by:\n\\[\nAverage = \\frac{1}{N} \\sum_{i=1}^N x_i\n\\]\nIn R we can easily calculate the sum over a vector by using the function sum():\n\nvalues = 1:10\nprint(values)\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\n# Average \nsum(values)/length(values)\n## [1] 5.5\n\nTo do that now more easily and in a comprehensive way for many different variables, we can define a function to calculate the mean:\n\naverage = function(x) {\n  average = sum(x)/length(x)\n  return(average)\n}\naverage(values)\n## [1] 5.5\n\nA function consists of: - An expressive name - Arguments function(arg1, arg2, arg3), the arguments can be used to pass the data to the function, or to change the behaviour of the function (see below) - A function body, inside curly brackets { } where the actual magic happens - return(...) what should be returned from the function\nThe advantages: - you can compress big code blocks within one function call - reproducibility, we avoid writing the same code again and again, if we want to change the way how we calculate the average, we have to change it only in one place - clarity, the name of the function can give us a hint about what the function is doing\nArguments\nArguments can be either used to pass data to the function or to change the behaviour of the function. Moreover, you can set default values to the function. If arguments have default values, they do not have to be specified (specifiying means that we have to fill this argument):\n\n# Should NAs be removed or not\naverage = function(x, remove_na) {\n  if(!remove_na) {\n    average = sum(x)/length(x)\n  } else {\n    average = sum(x, na.rm = TRUE)/length(x[complete.cases(x)])\n  }\n  return(average)\n}\n\nvalues = c(5, 4, 3, NA, 5, 2)\n\n# no default option for remove_na, we have to specify it!\naverage(values, remove_na = TRUE)\n## [1] 3.8\n\n# In this case, it is better to set a default option for remova_na:\naverage = function(x, remove_na = TRUE) {\n  if(!remove_na) {\n    average = sum(x)/length(x)\n  } else {\n    average = sum(x, na.rm = TRUE)/length(x[complete.cases(x)])\n  }\n  return(average)\n}\n\naverage(values)\n## [1] 3.8\n\n\n\nif(condition) {  } else { } the if/else statements runs code if a certain condition is true or not. If the condition is true, the first code block {  } is run, if it is false, the second (after the else) is run:\n\nvalues = 1:5\nif(length(values) == 5) {\n  print(\"This vector has length 5\")\n} else {\n  print(\"This vector has not length 5\")\n}\n## [1] \"This vector has length 5\"\n\nArguments are matched by the name or, if names are not specified, by the order:\nfunc(x1, x2, x3) will be interpreted as func(arg1 = x1, arg2 = x2, arg3 = x3)\nBut be careful, if you are unsure about the correct order, you should pass them by their name (func(arg1 = x1, arg2 = x2, arg3 = x3))\n\n\nLoops\nLoops are another important code structure. Example: We want to go over all values of a vector, calculate the square root of it, and overwrite the old value with the new value:\n\nvalues = c(20, 33, 25, 16)\nvalues[1] = sqrt(values[1])\nvalues[2] = sqrt(values[2])\nvalues[3] = sqrt(values[3])\nvalues[4] = sqrt(values[4])\n\nNow what should we do if we have thousands of observations? Loops are the solution! We can use them to automatically “run” a specific vector and then do something with it (well it sounds cryptic but it is actually quite easy):\n\nfor(i in 1:4) { # i in 1:4 means that i should be 1, 2, 3, and 4\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n\n# Let's use it to automatize the previous computation:\nfor(i in 1:4) {\n  values[i] = sqrt(values[i])\n}\nvalues\n## [1] 2.114743 2.396782 2.236068 2.000000\n\n# Even better: do not hardcode the length of the vector:\nfor(i in 1:length(values)) {\n  values[i] = sqrt(values[i])\n}\nvalues\n## [1] 1.454215 1.548154 1.495349 1.414214\n\nOur code will now always work, even if we change the length of the values variable!\n\n\n\n\n\n\nBonus Question\n\n\n\nWrite functions for:\n\nCalculate the sum for all values in a matrix given by (we want to write our own implementation of the internal sum(...) function):\n\nmy_matrix = matrix(1:200, 20, 10)\n\nUse the internal sum(...) function to check whether your function is correct!\nExtend the function with arguments that specify that the sum should be calculate over rows, columns, or both (if we calculate the sum over rows or columns, then a vector with n sums for n rows or n columns should be returned).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum_matrix function\n\n\n\nsum_matrix = function(X) {\n\n  n_row = nrow(X)\n  n_col = ncol(X)\n  result = 0\n  for(i in 1:n_row) {\n    for(j in 1:n_col) {\n      result = result + X[i,j]\n    }\n  }\n  return(result)\n}\n\n\nsum_matrix_extended function\n\n\n  sum_matrix_extended = function(X, which = \"both\") {\n  if(which == \"both\") {\n    result = sum_matrix(X)\n  } else if(which == \"row\") {\n    result = apply(X, 1, sum)\n  } else if(which == \"row\") {\n    result = apply(X, 2, sum)\n  }\n  return(result)\n}\n\nThe apply(...) function can be used to automatically loop over rows (MARGIN=1) or columns (MARGIN=2) and apply a function on each element (rows or columns) which can be specified via apply(data, MARGIN = 1, FUN = sum)"
  },
  {
    "objectID": "4A-Exercise.html#analyzing-the-regrowth-dataset",
    "href": "4A-Exercise.html#analyzing-the-regrowth-dataset",
    "title": "10  Exercise - Univariate linear regression",
    "section": "10.1 Analyzing the “regrowth” dataset",
    "text": "10.1 Analyzing the “regrowth” dataset\n\n\n\n\n\n\nWarning\n\n\n\nImagine you have a garden with some fruit trees and you were thinking of adding some berry bushes between them. However, you don’t want them to suffer from malnutrition so you want to estimate the volume of root biomass as a function of the fruit biomass.\nCarry out the following tasks\n\nPerform a simple linear regression for the influence of fruit biomass on root biomass.\nVisualize the data and add the regression line to the plot.\n\nYou will need the following functions:\n\nlm()\nsummary()\nplot()\nabline()\n\n\n10.1.1 Question\nYou have performed a simple linear regression for the influence of fruit biomass on root biomass.\nWhich of the following statements are correct? (More than one are correct)\n\n Root biomass is not significantly affected by fruit biomass. Fruit biomass explains most of the variance (&gt;50%) in the root biomass. At a fruit biomass of 70, the model would predict root biomass of about 4.18 + 0.05*70. At a fruit biomass of 0, the model predicts a root biomass of about 4.18.\n\n\n\n\n\n\nClick here to see the solution\n\n### Solution\n\nRoot biomass is not significantly affected by fruit biomass. WRONG: Look at the p-value for the slope (2nd row in the regression table below Pr(&gt;|t|))\nFruit biomass explains most of the variance (&gt;50%) in the root biomass. CORRECT: The proportion of variance explained by the model is given by R2.\nAt a fruit biomass of 70, the model would predict root biomass of about \\(4.18 + 0.05*70\\). CORRECT: The linear equation for the model is: \\(y = a*x + b\\) that is \\(Root = slope*Fruit + intercept\\)\nAt a fruit biomass of 0, the model predicts a root biomass of about 4.18. CORRECT: This is the intercept (1st row in the regression table below Estimate)\n\nThis is the code that you need to interpret the results.\n\nlibrary(EcoData)\n# simple linear regression\nfit &lt;- lm(Root ~ Fruit, data = regrowth)\n\n# check summary for regression coefficient and p-value\nsummary(fit)\n## \n## Call:\n## lm(formula = Root ~ Fruit, data = regrowth)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.25105 -0.69970 -0.01755  0.66982  1.63933 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 4.184256   0.337987  12.380  6.6e-15 ***\n## Fruit       0.050444   0.005264   9.584  1.1e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8111 on 38 degrees of freedom\n## Multiple R-squared:  0.7073, Adjusted R-squared:  0.6996 \n## F-statistic: 91.84 on 1 and 38 DF,  p-value: 1.099e-11\n\n# plot root explained by fruit biomass\nplot(Root ~ Fruit, data = regrowth, \n     ylab = \"Root biomass in cubic meters\",\n     xlab = \"Fruit biomass in g\")\n\nabline(fit) # add regression line\nabline(v = 70, col = \"purple\") # add line at x value (here fruit biomass of 70g)\nabline(h = 4.184256 + 0.050444*70, col = \"brown\") # add line at y value according to x = 70 using the intercept and regression coefficient of x"
  },
  {
    "objectID": "4A-Exercise.html#analyzing-the-birdabundance-dataset",
    "href": "4A-Exercise.html#analyzing-the-birdabundance-dataset",
    "title": "10  Exercise - Univariate linear regression",
    "section": "10.2 Analyzing the “birdabundance” dataset",
    "text": "10.2 Analyzing the “birdabundance” dataset\nThe dataset provides bird abundances in forest fragments with different characteristics in Australia. We want to look at the relationship of the variables “abundance”, “distance” and “grazing”.\n\n\n\n\n\n\nQuestions\n\n\n\nFirst, answer the following questions?:\n\nWhat is the most reasonable research question regarding these variables?\n\n\n How is grazing influenced by distance / abundance? How is distance influenced by grazing / abundance? How is abundance influenced by distance / grazing?\n\n\nWhat is the response variable?\n\n\n abundance distance grazing\n\n\nWhat is the predictor variable?\n\n\n either grazing or distance either abundance or distance either abundance or grazing\n\nThen, perform the following tasks:\n\nFit a simple linear regression relating the response variable to the categorical predictor (that is the one with five levels, make sure that it is indeed a factor using as.factor())\nApply an ANOVA to your model.\n\nYou may need the following functions:\n\nlm()\nsummary()\nanova()\n\nUse your results to chose the correct statement(s):\nYou have now fitted a simple linear regression with a categorical predictor and analyzed it. Which of the following statements are correct? (several statements are correct)\n\n The maximum likelihood estimate of bird abundance for grazing intensity 1 is 28.623. We can see in the regression table that the difference between grazing intensity 3 and 4 is significant. The non-significant p-value for grazing intensity 2 indicates that the data are compatible with the null hypothesis “H0: the bird abundance at grazing intensity 2 is on average 0.” The confidence interval for the estimate of the intercept is the smallest. The difference between grazing intensity 1 and 3 is significant. Grazing intensity in general has a highly significant effect (&lt; 0.001) on bird abundance.\n\n\n\n\n\nClick here to see the solution\n\n\n10.2.1 Solution\n\nThe maximum likelihood estimate of bird abundance for grazing intensity 1 is 28.623. CORRECT: When the predictor is a factor, the intercept equals the first factor level (by default, this follows an alphabetical order).\nWe can see in the regression table that the difference between grazing intensity 3 and 4 is significant. WRONG: Comparisons are always related to the intercept, i.e. to the first factor level. For comparisons among other factor levels we need post-hoc tests.\nThe non-significant p-value for grazing intensity 2 indicates that the data are compatible with the null hypothesis “H0: the bird abundance at grazing intensity 2 is on average 0.” WRONG: Comparisons are always related to the intercept, i.e. to the first factor level. Only the test for the intercept has H0: mean = 0.\n\nA reasonable research question is how abundance is influenced by distance and/or grazing. Here, the response variable is abundance, while the predictors are distance and/or grazing.\nThis is the code that you need to interpret the results.\n\n# change variable from integer to factor\nbirdabundance$GRAZE &lt;- as.factor(birdabundance$GRAZE) \nfit &lt;- lm(ABUND ~ GRAZE, data = birdabundance)\nsummary(fit)\n## \n## Call:\n## lm(formula = ABUND ~ GRAZE, data = birdabundance)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -18.3867  -4.1159   0.0269   5.1484  16.4133 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   28.623      2.086  13.723  &lt; 2e-16 ***\n## GRAZE2        -6.673      3.379  -1.975   0.0537 .  \n## GRAZE3        -7.336      2.850  -2.574   0.0130 *  \n## GRAZE4        -8.052      3.526  -2.284   0.0266 *  \n## GRAZE5       -22.331      2.950  -7.571 6.85e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.52 on 51 degrees of freedom\n## Multiple R-squared:  0.5449, Adjusted R-squared:  0.5092 \n## F-statistic: 15.27 on 4 and 51 DF,  p-value: 2.846e-08\n\n# anova to check global effect of the factor grazing intensity\nanova(fit)\n## Analysis of Variance Table\n## \n## Response: ABUND\n##           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \n## GRAZE      4 3453.7  863.42  15.267 2.846e-08 ***\n## Residuals 51 2884.2   56.55                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# boxplot\nplot(ABUND ~ GRAZE, data = birdabundance)"
  },
  {
    "objectID": "4A-Exercise.html#model-validation-residual-checks",
    "href": "4A-Exercise.html#model-validation-residual-checks",
    "title": "10  Exercise - Univariate linear regression",
    "section": "10.3 Model validation: Residual checks",
    "text": "10.3 Model validation: Residual checks\nNow, we will have a closer look at model diagnostics and residual checks in particular. Of course, we should have done this for all models above as well (we simply didn’t do this because of time restrictions). So remember that you always have to validate your model, if you want to be sure that your conclusions are correct.\nFor this exercise, you can prepare a dataset yourself called “dat” with the variables “x” and “y”. Simply copy the following code to generate the data:\n\nset.seed(234)\nx = rnorm(40, mean = 10, sd = 5)\ny = 10 - 2*x + 0.9 * x^2 + rnorm(40, mean=5, sd = 20)\ndat &lt;- data.frame(x, y)\nhead(dat)\n##           x          y\n## 1 13.303849 152.093910\n## 2 -0.264915   6.831275\n## 3  2.503970  45.207691\n## 4 17.356166 240.274237\n## 5 17.295693 240.917066\n## 6 10.700695 117.691234\n\nPerform the following tasks:\n\n\n\n\n\n\nWarning\n\n\n\n\nFit a simple linear regression.\nCheck the residuals.\nPerform another simple linear regression with a modified formula, if needed.\nCreate a scatter plot of the data and add a regression line for the first fit in black and one for the second fit in red. The second model cannot be plotted with the abline() function. Use the following code instead:\n\n\nlines(sort(x), predict(fit2, newdata = data.frame(x = sort(x))), col = \"red\")\n\nYou may also need the following functions:\n\nlm()\nsummary()\npar(mfrow = c(2, 2))\nplot()\nabline()\n\nUse your results to answer the following questions:\n\nWhat pattern do the residuals of the first regression model show when plotted against the fitted values?\nWhat do you have to do to improve your first regression model?\nIdentify the correct statement(s) about the residuals of the modified model.\n\n\n\n\n\nClick here to see the solution\n\n\nset.seed(234)\nx = rnorm(40, mean = 10, sd = 5)\ny = 10 - 2*x + 0.9 * x^2 + rnorm(40, mean=5, sd = 20)\ndat &lt;- data.frame(x, y)\n\n# simple linear regression\nfit &lt;- lm(y ~ x, dat)\n\n# check residuals\nop = par(mfrow=c(2,2))\nplot(fit) # residuals show a parabolic relationship (see first plot)  -&gt; to improve, fit a quadratic relationship\n\n\n\npar(op)\n\n# scatter plot\nplot(y ~ x, data = dat)\nabline(fit)\n\n\n\n\nsummary(fit) # significantly positively correlated, but this doesn't tell the full story because the residuals are not okay\n## \n## Call:\n## lm(formula = y ~ x, data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -39.884 -22.208  -4.948  10.602 118.164 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   -8.459     10.973  -0.771    0.446    \n## x             11.465      1.019  11.248 1.18e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 32.11 on 38 degrees of freedom\n## Multiple R-squared:  0.769,  Adjusted R-squared:  0.763 \n## F-statistic: 126.5 on 1 and 38 DF,  p-value: 1.176e-13\n\n# improved regression model\nfit2 = lm(y ~ x + I(x^2), dat)\n\n# check residuals\nop = par(mfrow=c(2,2))\nplot(fit2) # no pattern in residuals anymore (first plot) -&gt; fit is fine\n\n\n\npar(op)\n\n# scatter plot\nplot(y ~ x, data = dat)\nabline(fit)\nlines(sort(x), predict(fit2, newdata = data.frame(x = sort(x))), col = \"red\")\n\n\n\n\n\nsummary(fit2) # significantly negatively correlated, trustworthy now, because residuals are sufficiently uniformly distributed (first plot in plot(fit2))\n## \n## Call:\n## lm(formula = y ~ x + I(x^2), data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -33.174 -11.444   0.938  10.164  40.666 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 17.87505    6.00812   2.975  0.00513 ** \n## x           -1.10100    1.27706  -0.862  0.39417    \n## I(x^2)       0.80752    0.07526  10.730 6.49e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 16.05 on 37 degrees of freedom\n## Multiple R-squared:  0.9438, Adjusted R-squared:  0.9408 \n## F-statistic: 310.9 on 2 and 37 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "2A-Exercise.html#contingency-tables",
    "href": "2A-Exercise.html#contingency-tables",
    "title": "Exercise - Plots and summary statistics",
    "section": "Contingency tables",
    "text": "Contingency tables\nHere, we will have a look at the data set “arthritis” from the “EcoData” package. The study was conducted as a double-blind clinical trial investigating a new treatment against arthritis. The improvement was measured as a categorical variable with the possible outcomes “Improved”, “Some” or “None”.\nInstall and then load the package “EcoData” and have a look at the data using the View() function. This will open a new tab next to your R script. To return to your script, close the new tab or click on your script.\n\nlibrary(EcoData)\n# View(arthritis)\n\n# Get the data\ndat &lt;- arthritis\n\n# Coerce columns ’Improved' and 'Treatment' to (ordered) factors\n# (When the factor is ordered, other functions like table() and barplot() will use this order.\n# Otherwise, the levels will be ordered alphabetically.)\ndat$Improved &lt;- factor(dat$Improved, levels = c(\"None\",\"Some\",\"Marked\"), ordered = TRUE)\ndat$Treatment &lt;- as.factor(dat$Treatment)\nstr(dat)\n## 'data.frame':    84 obs. of  5 variables:\n##  $ ID       : int  57 46 77 17 36 23 75 39 33 55 ...\n##  $ Treatment: Factor w/ 2 levels \"Placebo\",\"Treated\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ Sex      : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n##  $ Age      : int  27 29 30 32 46 58 59 59 63 63 ...\n##  $ Improved : Ord.factor w/ 3 levels \"None\"&lt;\"Some\"&lt;..: 2 1 1 3 3 3 1 3 1 1 ...\n\nAn important function for categorical data is table(). It produces a contingency table counting the occurrences of the different categories of one variable or of each combination of the categories of two (or more) variables.\nWe are curious how many patients actually improved in the study and how this is influenced by the treatment. To show this graphically, we produced two plots (see below) using the following functions:\n\ntable()\nplot() and barplot()\n(str())\n(summary())\n\n\n\n\n\n\n\nTask\n\n\n\nYour task is now to reconstruct the two plots shown below by using these functions.\n\n\n\n\n\n\n\n\n\n\nWhat do you think of the study now? Could you already draw conclusions from this plot?\n\n\n\n\nHints for plot 1\nWhat kind of plot is shown here? How many variables are shown in the plot? Approach: First, create a new object consisting of the table of the variable of interest. Then use this object for plotting. Changing plot elements: Have a look at the help of the plotting-function to find out how to change the y- and x-axis labels. What do you notice on the y-axis? You can change the limits of the y-axis using “ylim = c(from, to)” as an argument in the plotting function.\n\nHints for plot 2\nWhat kind of plot can you see here? How many variables does it show? To plot this you need to create a contingency table with the variables of interest. Changing plot elements: You can name the variables in your contingency table (e.g. name = variable, othername = othervariable). The name you assign to your table will be used as the title in the plot.\n\n\n\nClick here to see the solution\n\nPlot 1\n\ncounts &lt;- table(dat$Improved)  # create a table which gives you counts of the three categories in the \"Improved\" variable\n\nbarplot(counts,            # create a barplot of your table\n        ylim = c(0,50),    # change the limits of your y axis: starting from zero to 50\n        xlab = \"Improved\",  # add a label to your x axis\n        ylab = \"Frequency\")  # add a label to your y axis\n\n\n\n\nPlot 2\n\nRatios &lt;- table(Improved = dat$Improved, Treatment = dat$Treatment)  # create a table with the assigned name \"ratios\", give the name \"Improved\" to the first variable which is the variable Improved from the data set Arthritis, give the name \"Treatment\" to the second variable which is the variable Treatment from the data set Arthritis\nplot(Ratios)  # create a plot of the table \"ratios\"\n\n\n\n\nTo view the tables with the names “counts” and “Ratios” you can simply execute their names:\n\ncounts\n## \n##   None   Some Marked \n##     42     14     28\nRatios\n##         Treatment\n## Improved Placebo Treated\n##   None        29      13\n##   Some         7       7\n##   Marked       7      21\n\nCould you already draw conclusions from this plot? No, because this is only a descriptive plot. You can say that a large proportion of the patients that got a placebo treatment did not improve while a large proportion of the patients that got the new treatment improved markedly. However, this could also be the result of random variation and sampling. We need inferential statistics to make conclusions about the effectiveness of the treatment."
  },
  {
    "objectID": "2A-Exercise.html#histogram",
    "href": "2A-Exercise.html#histogram",
    "title": "Exercise - Plots and summary statistics",
    "section": "Histogram",
    "text": "Histogram\nNow let’s have a look at the “birdabundance” dataset, which is in the “EcoData” package. This is not stored at CRAN (the official platform for R packages, but at github where we host our own code collections). If you haven’t done this yet, use the code below to install the package (note that you also need the “devtools” package to do this. Again, to load it and make the data set accessible, execute the function library(EcoData). To view the data set you can use the View() function again.\nYou can also get more explanations on the data set via the help.\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", dependencies = T, build_vignettes = T)\nlibrary(EcoData)\nView(birdabundance)\n\nThe dataset has been assessed by scientists in Australia who counted birds in forest fragments and looked for drivers that influence these bird abundances, e.g. the size of a forest fragment and the distance to the next fragment. We want to see how these distances vary. A histogram is the standard figure to depict one numerical variable, such as for example distance measurements.\n\n\n\n\n\n\nTask\n\n\n\nYour task is now to reconstruct the following histogram including the labels, colors and lines using the functions:\n\nhist() to create a histogram\nabline() to add (vertical) lines to your histogram\n\nThink about what the histogram tells you about the distances between each forest fragment and the next forest fragment. What do the red and purple lines represent? Which site was the furthest away from forest fragments?\n\n\n\n\n\nHints for plotting\nChange the color (look at the help to see how). You can also try other colors, e.g. your favorite color (you can find all available colors with the function colors()). Change the bar widths (breaks). Play around to see how they change. When changing the bar widths, what do you notice on the y-axis? You can change the y-axis limits using “ylim” (see examples for hist(), second last line). Change the title and the x-axis name of your histogram using the arguments “main” and “xlab”.\nTo add the lines, try the abline() function with the argument “v = 90” and look what happens. To remove the line, you have to execute the code producing your histogram again. Remember that abline() is a low level plotting function, which means it only adds an element to an existing plot! Instead of calculating the depicted values with a function and then pasting the values into your abline function, you can also directly use the function in abline().\nSee an example on the right.\n\n\n\n\nClick here to see the solution\n\nYou can plot the histogram like this:\n\nhist(birdabundance$DIST,  # plot a histogram of the varriable distance from the data set birdabundance\n     breaks = 20,  # change bar widths, here we make them smaller\n     col = \"blue\",  # change color to blue\n     ylim = c(0,20),  # change the range of the y axis to from 0 to 20\n     main = \"Distance distribution\",  # change title of the plot\n     xlab = \"Distance to the next forest fragment\")  # change x axis name\nabline(v = mean(birdabundance$DIST), col = \"red\")  # add a vertical line with an x value equal to the mean of the variable distance\nabline(v = median(birdabundance$DIST), col = \"purple\")  # add a vertical line with an x value equal to the median of the variable distance\n\n\n\n\nWhat do the red and purple lines represent?\nThe red line represents the mean distance between forest fragments, while the purple line represents the median.\nWhich site was the furthest away from forest fragments?\n\n# Extract the line in which the variable DIST takes its maximum\nbirdabundance[which.max(birdabundance$DIST),]\n##    Site ABUND AREA DIST LDIST YR.ISOL GRAZE ALT\n## 48   48  39.6   49 1427  1557    1972     1 180\n\nThe site number 48 was the furthest away.\n\n\n\n\nhist(airquality$Temp)\nabline(v = 90, col = \"blue\")\nabline(v = median(airquality$Temp), col = \"red\")"
  },
  {
    "objectID": "2A-Exercise.html#scatterplot",
    "href": "2A-Exercise.html#scatterplot",
    "title": "Exercise - Plots and summary statistics",
    "section": "Scatterplot",
    "text": "Scatterplot\nAs you’ve learned by now, plot() can create different types of plots depending on the type of the input data. It creates a scatterplot when plotting two numerical variables. Now we are interested to see how the abundance of birds is affected by the distance to the next forest fragment, and if there is another variable that is important for this relationship (visualized here by the color of the points).\n\n\n\n\n\n\nTask\n\n\n\nAgain, your task is to reconstruct the following plot using the following functions:\n\nplot()\n(str())\n(summary())\n\nWhat do you notice about the distribution of the colors along bird abundance?\nWhat is the mean bird abundance per color?\n\n\n\n\n\nHints for plotting:\nWhat is plotted on the x-axis, what on the y-axis?\nThere are two different ways to write the plot function. One is to stick with the “Usage” in the help of the plot function (giving coordinates along the x-axis as the first attribute and those along the y-axis as the second attribute). The other way is to write the relationship between x and y as a formula that is: y~x, data = dataset Use google to find out how you can change the point shapes in your plot.\nLook at the dataset to find out which variable is indicated by the color of the points in the plot. Hint: It is a variable indicating 5 intensity levels. To change the color, include the attribute “col” in your plot function and set it equal to the variable.\nTo get a color gradient you can create a function with the following code. Apply it before producing your plot and use the created function rbPal() as the color in the plot.\n\n# Palettes can be created with a function for the grDevices package\ninstall.packages(\"grDevices\")\nlibrary(grDevices)\n\n# Create a function to generate a continuous color palette from red to blue\nrbPal &lt;- colorRampPalette(c('red','blue'))\n\n# Example for color in a plot\nplot(Ozone ~ Solar.R, data = airquality, \n     col = rbPal(12)[Month]) # you can use any number, here it's 12 because we have 12 months\n\n\n\n\n\nClick here to see the solution\n\nYou can either create a scatterplot of two numerical variables like this:\n\n#Create a function to generate a continuous color palette from red to blue\nrbPal &lt;- colorRampPalette(c('red','blue')) # rpPal for red to blue palette\n\nplot(birdabundance$DIST, birdabundance$ABUND,  # create a plot of the variables DIST against ABUND from the data set birdabundance\n     ylab = \"Distance (m)\",  # add the label \"Distance\" to the y axis\n     xlab = \"Bird abundance\",  # add the label \"Bird abundance\" to the x axis\n     col = rbPal(5)[birdabundance$GRAZE],  # color the data points according to their category in the variable GRAZE from the data set birdabundance\n     pch = 17) # change the point shape\n\n\n\n\nOr like this:\n\nplot(ABUND ~ DIST, data = birdabundance,  # create a plot of the variables DIST against ABUND from the data set birdabundance\n     xlab = \"Distance (m)\",  # add the label \"Distance\" to the x axis\n     ylab = \"Bird abundance\",  # add the label \"Bird abundance\" to the y axis\n     col = rbPal(5)[GRAZE],  # color the data points according to their category in the variable GRAZE as a gradient\n     pch = 17) # change the point shape\n\n\n\n\nThe advantage of the second version is that it uses the structure of “response variable (y) explained by (~) explanatory variable (x)”. Also, you tell the plot function which data set to use once and it will automatically draw the variables from there, while in the first version you name the data set and the respective variable each time (even for the color).\nWhat do you notice about the distribution of the colors along bird abundance?\nYou can see that the blue data points are only at the low abundances, whereas the red data points are rather at the higher abundances. Purple data points are throughout all abundances. There thus seems to be a correlation between the grazing classes and bird abundances.\nWhat is the mean bird abundance per color / level of grazing intensity?\n\n# Option 1: Using tidyverse / dplyr as shown at the end of section 2.0.1 Summary statistics:\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nbird_grouped &lt;- birdabundance %&gt;% # define dataset to be summarized\n  group_by(GRAZE) %&gt;% # define grouping factor\n  summarise(mean.abund = mean(ABUND)) # summarize by taking the mean of abundance\n\n# Option 2: Using base R and formula notation:\nbird_grouped &lt;- aggregate(ABUND~GRAZE, data = birdabundance, FUN = mean)\n\nbird_grouped\n##   GRAZE     ABUND\n## 1     1 28.623077\n## 2     2 21.950000\n## 3     3 21.286667\n## 4     4 20.571429\n## 5     5  6.292308"
  },
  {
    "objectID": "2A-Exercise.html#boxplot",
    "href": "2A-Exercise.html#boxplot",
    "title": "Exercise - Plots and summary statistics",
    "section": "Boxplot",
    "text": "Boxplot\nThe boxplot() function can be used to create boxplots - to visualize associations between continuous and categorical variables.\nThe function can be used in two ways (similar to the plot() function): a) by specifcing x and y arguments and b) the formula syntax (the preferred way!):\n\nboxplot(Sepal.Length~Species, data = iris)\n\n\n\n\n(Syntax: continuous variable ~ categorical variable )\n\n\n\n\n\n\nTask\n\n\n\nIn the MASS package there is a dataset (Melanoma) about patients with malignant melanoma:\n\nlibrary(MASS)\n## \n## Attaching package: 'MASS'\n## The following object is masked from 'package:dplyr':\n## \n##     select\n## The following object is masked from 'package:EcoData':\n## \n##     snails\ndf = Melanoma\ndf$sex = ifelse(df$sex == 0, \"female\", \"male\")\ndf$status = as.factor(df$status)\nstr(df)\n## 'data.frame':    205 obs. of  7 variables:\n##  $ time     : int  10 30 35 99 185 204 210 232 232 279 ...\n##  $ status   : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 3 2 3 1 1 1 3 1 1 ...\n##  $ sex      : chr  \"male\" \"male\" \"male\" \"female\" ...\n##  $ age      : int  76 56 41 71 52 28 77 60 49 68 ...\n##  $ year     : int  1972 1968 1977 1968 1965 1971 1972 1974 1968 1971 ...\n##  $ thickness: num  6.76 0.65 1.34 2.9 12.08 ...\n##  $ ulcer    : int  1 0 0 0 1 1 1 1 1 1 ...\n\nWe want to check for associations between the age of the patients and the status of the melanoma, and later, at the same time for their sex:\nTasks:\n\nCreate boxplot with age against status\nCreate boxplot with age against status AND sex (both groups at the same time)\nAdd colors (for sex)\nAdd Notch\nIncrease spacing between the different groups (status)\nBonus: add the observations/points on top of the boxplot using the library(beeswarm) package\n\nCan we use boxplots to make inference (are there significant differences)? What is the Notch?\n\n\n\nHints:\n\nIf you need help with the boxplot for age against status AND sex, check google, for example this query\nSpacing: at argument in the boxplot( ) function\nInstall the beeswarm package via install.packages(\"beeswarm\")\n\n\n\n\nClick here to see the solution\n\nIf we use colors, we also need a legend!\nSometimes it is helpful to add the points/observations into the boxplot:\n\nlibrary(beeswarm) \nb = boxplot(age~sex+status, \n            at = c(1:2, 4:5, 7:8), # x coordinates\n            col = c(\"#FF002387\", \"#5544FF77\"),\n            data = df,\n            las = 1, # rotate labels so that all are horizontal \n            xaxt = \"n\", # omit x-axis labels+ticks, we will add them manually\n            xlab = \"\", # no xlab, unncessary\n            notch = TRUE\n            )\nbeeswarm(age~sex+status, \n         data = df, \n         add = TRUE,\n         at = c(1:2, 4:5, 7:8) # x - coordinates\n         )\naxis(1, # which side of the plot, 1 == x-axis\n     at = c(1.5, 4.5, 7.5), # x coordinates\n     labels = c(\"Status 1\", \"Status 2\", \"Status 3\") # labels\n     )\nlegend(\"bottomright\", # position of the legend (x and y coordinates can be also used)\n       bty = \"n\", # omit border line, personal preference\n       legend = c(\"Female\", \"Male\"),\n       col = c(\"#FF002387\", \"#5544FF77\"),\n       pch = 15 # point type (squares)\n       )\n\n\n\n\nCan we use boxplots to make inference (are there significant differences)? What is the Notch?:\nBox plots by themselves cannot be used to make conclusions about significant differences. Tomorrow we will learn that we have two statistics for hypothesis testing, the effects (e.g., means) and the uncertainties of those effects/estimates. The box plot shows the medians, which we could theoretically use for testing. However, the boxplot doesn’t report the uncertainty of the reports. The uncertainty of the estimate is not the width/height of the box, it is just the distribution of the variable, for example:\n\nset.seed(2)\nvar1 = rnorm(50)\nvar2 = rnorm(10000)\nsd(var1)/sqrt(50)\n## [1] 0.1597612\nsd(var2)/sqrt(10000)\n## [1] 0.009994205\ndf = data.frame(values = c(var1, var2), group = as.factor(c(rep(\"var1\", 50), rep(\"var2\", 10000))))\n\npar(mfrow = c(1,1))\nboxplot(values~group, data = df, notch = TRUE)\ntext(1, 4.5,xpd = NA, labels = paste0(\"N = 50, standard error = \", round(sd(var1)/sqrt(50), 4)))\ntext(2, 4.5,xpd = NA, labels = paste0(\"N = 10000, standard error = \", round(sd(var1)/sqrt(10000), 4)))\n\n\n\n\nThe standard error changes with the number of observations (more on this tomorrow), while the distribution (the box) remains relatively stable!\nThe notch is the confidence interval of the median, so 2*the standard error of the median. We can roughly say that there is a significant difference when the confidence intervals overlap!"
  },
  {
    "objectID": "2A-Exercise.html#correlation",
    "href": "2A-Exercise.html#correlation",
    "title": "Exercise - Plots and summary statistics",
    "section": "Correlation",
    "text": "Correlation\n\n\n\n\n\n\nTask\n\n\n\nIn the previous plot on bird abundance you’ve seen three variables. Now we want to know, how they are correlated with each other. Remember that we can use the function cor() to calculate correlations. Which of the following correlation coefficients (Pearson) belongs to which variable pair? Can you see these correlations in your previous plot?\n\n## [1] 0.2361125\n## [1] -0.6825114\n## [1] -0.2558418\n\nThink about the meaning of the correlation values (positive/negative, strength). Is it what you would have expected by looking at the plot?\n\n\n\n\nClick here to see the solution\n\n#### Solution\n\ncor(birdabundance$ABUND, birdabundance$DIST)\n## [1] 0.2361125\ncor(birdabundance$ABUND, birdabundance$GRAZE)\n## [1] -0.6825114\ncor(birdabundance$GRAZE, birdabundance$DIST)\n## [1] -0.2558418\n\nThe first correlation (abundance to distance) tells us that there is a small positive correlation between the two variables, but it does not tell us whether it is significant or not. We will properly test such relationships later in the course. In the scatter plot we have seen this weak positive correlation already. The second correlation (abundance to grazing) tells us that there is a stronger negative correlation between abundance and grazing. We have already seen a pattern of the color of the data points along bird abundances (red towards higher, blue towards lower abundances). The third correlation (grazing to distance) tells us that there is a small negative correlation between the two variables. However, the color pattern along distance is not as obvious as for abundance."
  },
  {
    "objectID": "2A-Exercise.html#bonus---advanced-visualization-with-the-base-r-package",
    "href": "2A-Exercise.html#bonus---advanced-visualization-with-the-base-r-package",
    "title": "Exercise - Plots and summary statistics",
    "section": "Bonus - Advanced visualization with the base R package",
    "text": "Bonus - Advanced visualization with the base R package\nYou can create any kind of plot or even visualization in R. If you want to plot abstract objects in R, start with an empty plot window (plot(NULL,NULL, xlim = c(0,1), ylim = c(0,1), axes = F) and then fill it however you like, two examples:\nIn Pichler et al. 2020 we created Figure 5 A (an overview over the data used in the paper) using base R:\n\npar(mfrow = c(1,1),mar = c(0.3,3,1,3)+0.0,oma = c(1,0.3,2,1))\n# Fig5a\nplot(NULL,NULL, xlim = c(0,1), ylim = c(0,1), axes = F, xlab = \"\", ylab = \"\")\n#text(x = -0.06, y = 1.03, labels = \"(a)\", cex = 0.9, xpd = NA, font = 2)\npolygon(c(0,0.125,0.17,0.19,0.35,0.51,0.65,0.85), c(0,0.25,0.5,0.42,1,0.35,0.09,0), lwd = 2, col = \"lightgrey\", xpd = NA)\nbaseX = 0.72\nbaseYv = c(0.1,0.4,0.7)\ncols = RColorBrewer::brewer.pal(3, \"Accent\")\naltitude = c(\"50 m.a.s.l\", \"1000 m.a.s.l\", \"2000 m.a.s.l\")\nlineY = baseYv+0.05\nfor(i in 1:3) {\n  baseY = baseYv[i]\n  rect(xleft = baseX, xright = baseX+0.025,ybottom = baseY,ytop = baseY+0.1, col = cols[i], lwd = 0.8)\n  rect(xleft = baseX+0.035, xright = baseX+0.135,ybottom = baseY,ytop = baseY+0.1, col = cols[i], lwd = 0.8)\n  rect(xleft = baseX+0.035, xright = baseX+0.135,ybottom = baseY+0.11,ytop = baseY+0.135, col = cols[i], lwd = 0.8)\n  y = lineY[i]\n  lines(y = rep(y,2), x = c(0.0,baseX-0.05), lty = 3, lwd = 1.2, xpd = NA)\n  text(y = y+0.03, x = -0.15, labels = altitude[i], cex = 0.7, pos = 4, xpd = NA, font = 2)\n}\ntext(x = 0.85, y = baseYv[1]+0.05,pos = 4, xpd = NA, cex = 0.9, labels = \"Low\", font = 2)\ntext(x = 0.85, y = baseYv[2]+0.05,pos = 4, xpd = NA, cex = 0.9, labels = \"Mid\", font = 2)\ntext(x = 0.85, y = baseYv[3]+0.05,pos = 4, xpd = NA, cex = 0.9, labels = \"High\", font = 2)\ntext(x = 0.35, y = 0, pos = 3, labels = \"Costa Rica\", cex = 0.9, font = 2)\n\n\n\n\nGo through the code and try to understand which line or function call creates which element in the plot!\nFigure 3 in the same paper is based on the following function (which we have developed just for this paper!):\n\n\n\nspider = function(x1 = NULL,x2 = NULL, colRec = \"#ff9999\", alphaRec = 0.5,colRecBorder = NULL, titles = NULL, stepsText = NULL,singlePanel = F,\n                  rectangular = F,\n                  colSpider = \"#e6e6e6\" ,alphaSpider = 0.5,\n                  colBorder = c(\"#cccccc\",\"#cccccc\",\"#666798\",\"#cccccc\",\"#cccccc\"),\n                  maxValues = NULL,minValues = NULL,\n                  rad = 5, cexSteps = 1.4, cexProcent = 0.8,\n                  parValues = list(pty = \"s\"), cexPoints = 1.0, circleLines = 5, twistSteps = 90, cexTitles = 0.7){\n  if(is.null(colRecBorder)) colRecBorder = colRec\n  if(is.null(dim(x1))) x1 = matrix(x1, ncol = length(x1), nrow = 1)\n  if(!is.null(x2)){\n    if(is.null(dim(x2))) x2 = matrix(x2, ncol = length(x2), nrow = 1)\n  }\n  if(is.null(maxValues)) maxValues = rep(1, ncol(x1))\n  if(is.null(minValues)) minValues = rep(0, ncol(x1))\n  if(!is.null(colRec)) colRec2 = addA(colRec, 0.5*alphaRec)\n  if(!is.null(colRec)) colRec = addA(colRec, alphaRec)\n  if(length(colRec) == 1) {\n    colRec = rep(colRec, nrow(x1))\n    colRecBorder = rep(colRecBorder, nrow(x1))\n  }\n\n  ## scale:\n  for(i in 1:ncol(x1)){\n    x1[,i] = (x1[,i] + abs(minValues[i]))/(abs(minValues[i]) + maxValues[i])\n    if(!is.null(x2)) x2[,i] = (x2[,i] + abs(minValues[i]))/(abs(minValues[i]) + maxValues[i])\n    if(any(x1[,i] &gt; maxValues[i])) stop(\"Max values are lower than actual values\")\n    if(any(x1[,i] &lt; minValues[i])) stop(\"Min values are higher than actual values\")\n\n  }\n  if(!is.null(x2)){\n    if(nrow(x2) != nrow(x1)) stop(\"x1 and x2 do not have equal number of rows\")\n  }\n\n  ## init:\n  lineSeq = seq(rad*0.1,rad, length.out = 5)\n  tmpSinglePanel = T\n  do.call(par, parValues)\n  nseg=1440\n  nSeg = ncol(x1)\n\n  procent = matrix(0,circleLines,2)\n  colSpider &lt;- addA(colSpider, alphaSpider)\n\n  lines = circleLines\n  lineSeq = seq(rad*0.1,rad, length.out = lines)\n  angles = seq(0+twistSteps,360+twistSteps,length.out = nSeg+1)[1:(nSeg)]\n  baseRadar = function() {\n    plot(NULL, NULL, xlim = c(-5,5), ylim =c(-5,5),pty=\"s\", axes = F, xlab = \"\", ylab = \"\")\n    if(!rectangular)\n      for(i in 1:length(lineSeq)){\n        xx = lineSeq[i]*cos( seq(0,2*pi, length.out=nseg) )\n        yy = lineSeq[i]*sin( seq(0,2*pi, length.out=nseg) )\n        if(i == lines) polygon(xx,yy, col= colSpider, border = colBorder[lines], lty = 2, lwd = 1)\n        else if(i == ceiling(lines)) polygon(xx,yy, border = colBorder[ceiling(lines)], lty = 2)\n        else if(i == 1) polygon(xx,yy,  border = colBorder[lines], lty = 2)\n        else polygon(xx,yy, border = colBorder[i], lty = 2)\n      }\n    else\n      for(i in 1:length(lineSeq)){\n        xx = cos(deg2rad(angles))*lineSeq[i]\n        yy = sin(deg2rad(angles))*lineSeq[i]\n        if(i == lines) polygon(xx,yy, col= colSpider, border = colBorder[lines], lty = 2, lwd = 1)\n        else if(i == ceiling(lines)) polygon(xx,yy, border = colBorder[ceiling(lines)], lty = 2)\n        else if(i == 1) polygon(xx,yy,  border = colBorder[lines], lty = 2)\n        else polygon(xx,yy, border = colBorder[i], lty = 2)\n      }\n\n    for(counter in 1:length(angles)) {\n      segments(x0 = cos(deg2rad(angles[counter]))*lineSeq[1],\n               y0 =  sin(deg2rad(angles[counter]))*lineSeq[1],\n               x1 = cos(deg2rad(angles[counter]))*rad ,\n               y1 = sin(deg2rad(angles[counter]))*rad ,\n               col = colBorder[5])\n    }\n  }\n\n\n  ## plot rect\n  for(data in 1:nrow(x1)){\n    if(data == 1 || !singlePanel) baseRadar()\n    valuesP = matrix(0,nSeg,2)\n    textP = matrix(0,nSeg,2)\n    valuesPtrain = matrix(0,nSeg,2)\n    drTest = x1[data,,drop = F]\n    drTrain = x2[data,,drop = F]\n    for(i in 1:nSeg){\n      valuesP[i,1] = cos(deg2rad(angles[i]))*drTest[1,i]*(rad-lineSeq[1]) +\n        cos(deg2rad(angles[i]))*lineSeq[1]\n      valuesP[i,2] = sin(deg2rad(angles[i]))*drTest[1,i]*(rad-lineSeq[1]) +\n        sin(deg2rad(angles[i]))*lineSeq[1]\n\n      if(!is.null(x2)){\n        valuesPtrain[i,1] = cos(deg2rad(angles[i]))*drTrain[1,i]*(rad-lineSeq[1]) +\n          cos(deg2rad(angles[i]))*lineSeq[1]\n        valuesPtrain[i,2] = sin(deg2rad(angles[i]))*drTrain[1,i]*(rad-lineSeq[1]) +\n          sin(deg2rad(angles[i]))*lineSeq[1]\n      }\n\n      textP[i,1] = cos(deg2rad(angles[i]))*1.0*rad\n      textP[i,2] = sin(deg2rad(angles[i]))*1.08*rad\n    }\n    polygon(y = valuesP[,2], x = valuesP[,1], col = colRec[data],border = colRecBorder[data], lwd = 1.5)\n    points(y = valuesP[,2], x = valuesP[,1], pch = 16, col = colRecBorder[data], cex = cexPoints)\n\n    if(!is.null(x2)){\n      polygon(y = valuesPtrain[,2], x = valuesPtrain[,1], col = colRec2[data],border = colRecBorder[data], lwd = 1.5, lty = 2)\n      points(y = valuesPtrain[,2], x = valuesPtrain[,1], pch = 16, col = colRecBorder[data], cex = cexPoints, lty = 2)\n    }\n\n    ## Text\n    if(data == 1 || !singlePanel){\n      measures = stepsText\n      if(!is.null(measures)){\n        strl = max(sapply(measures,nchar))\n        anglesP = angles - 90\n        pos = sapply(angles, function(x) {\n          if(x &gt;= 45 && x &lt;= 95 ) return(3)\n          if(x&gt;95 && x&lt;240) return(2)\n          if(x&gt;=240 && x&lt;=300) return(1)\n          if(x&gt;300 &&x&lt;=450) return(4)\n          if(x&lt;45) return(4)\n        })\n        if(!is.na(cexSteps)) text(x = textP[,1], y = textP[,2], labels = measures, xpd = T, font = 2, cex = cexSteps, pos = pos)\n      }\n      if(!is.null(titles)) title(main = titles, outer = F, cex.main = cexTitles)\n    }\n    if(nrow(procent) == 3) procentLabels = c(\"  0%\",  \" 50%\",  \"100%\")\n    else procentLabels = c(\"  0%\", \" 25%\", \" 50%\", \" 75%\", \"100%\")\n    if(!singlePanel){\n      procent[,1] = 0.2\n      procent[,2] = lineSeq\n      text(x = procent[,1], y = procent[,2], labels = procentLabels,\n           adj = c(-0.2,0.8), font = 2, cex = cexProcent)\n    } else {\n      if(data == nrow(x1)){\n        procent[,1] = 0.2\n        procent[,2] = lineSeq\n        text(x = procent[,1], y = procent[,2], labels = procentLabels,\n             adj = c(-0.2,0.8), font = 2, cex = cexProcent)\n      }\n    }\n  }\n}\n\ndeg2rad &lt;- function(deg) {(deg * pi) / (180)} # degrees to radial\naddA    &lt;- function(col, alpha = 0.25) apply(sapply(col, col2rgb)/255, 2, function(x) rgb(x[1], x[2], x[3], alpha=alpha)) # change contrast\n\n\nspider(runif(10), runif(10))\n\n\n\n\nTry to roughly understand the different parts of the spider function!\n\n\n\n\n\n\nTask:\n\n\n\nPlot/Draw a donut!\n\n\n\n\nClick here to see the solution\n\n\nplot(NULL, NULL, type = \"n\", axes = FALSE, xlim = c(-1, 1), ylim = c(-1,1), xlab = \"\", ylab = \"\", pty=\"s\")\ntheta = seq(0, 2 * pi, length = 500) \npolygon(x = cos(theta), y =  sin(theta), col = \"#A58800\")\npolygon(x = 0.5*cos(theta), y =  0.5*sin(theta), col = \"#FFFFFF\")"
  },
  {
    "objectID": "3A-Exercise.html#streams",
    "href": "3A-Exercise.html#streams",
    "title": "Exercise - NHST and statistical tests",
    "section": "Streams",
    "text": "Streams\nThe dataset ‘streams’ contains water measurements taken at different location along 16 rivers: ‘up’ and ‘down’ are water quality measurements of the same river taken before and after a water treatment filter, respectively. We want to find out if this water filter is effective. Use the decision tree to identify the appropriate test for this situation.\nThe filters were installed between upstream (‘up’) and downstream (‘down’).\n\n\nHint: Are the up and down observations independent? (paired or unpaired)\n\ndat = read.table(\"https://raw.githubusercontent.com/biometry/APES/master/Data/Simone/streams.txt\", header = T)\n\n\n\n\n\n\n\n1. Task\n\n\n\n1. Visualize the data\nHints: categorical and numerical..(two column == two levels)\n\n\nClick here to see the solution\n\nmatplot() can be used to plot several lines at once\n\npar(mfrow = c(1,2))\nboxplot(dat, notch = TRUE)\n\ncol_num = as.integer(dat[,2] &gt; dat[,1]) + 1\n\nmatplot(t(dat), \n        type = \"l\", \n        las = 1, \n        lty = 1, \n        col = c(\"#FA00AA\", \"#1147AA\")[col_num])\nlegend(\"topleft\", legend = c(\"worse\", \"better\"), lty = 1, col = c(\"#FA00AA\", \"#1147AA\"), bty = \"n\")\n\n\n\npar(mfrow = c(1,1))\n\n\n\n\n\n\n\n\n\n\n2. Task\n\n\n\n2. For identifying an appropriate test for the effect of the water treatment filter, what are your first two choices in the decision tree?\n\n Two groups, unpaired observations Two groups, paired observations Three or more groups, unpaired observations Three or more groups, paired observations\n\n\n\nFeedback\n\nThe number of groups to compare is two, up versus down stream. The observations are paired because the water tested up and down stream of the filter is not independent from each other, i.e. the “same” water is measured twice!\n\n\n\n\n\n\n\n\n\n3. Task\n\n\n\n3. The next decision you have to make is whether you can use a parametric test or not. Apply the Shapiro-Wilk test to check if the data are normally distributed. Are the tests significant and what does that tell you?\n\n One test is significant. The downstream data significantly deviate from a normal distribution. The upstream data does not significantly deviate from a normal distribution. Both tests are significant. Both data significantly deviate from a normal distribution. One test is significant. The downstream data significantly deviate from a normal distribution. The upstream data does not significantly deviate from a normal distribution.\n\n\n\nClick here to see the solution\n\nThe Shapiro-Wilk test is significant (p &lt; 0.05) for down stream data, i.e. we reject H0 (the data is normally distributed). Thus, the data significantly deviate from a normal distribution. The test is not significant for upstream data; the data does not significantly deviate from a normal distribution.\n\nshapiro.test(dat$down)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  dat$down\n## W = 0.86604, p-value = 0.02367\nshapiro.test(dat$up)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  dat$up\n## W = 0.93609, p-value = 0.3038\n\n\n\n\n\n\n\n\n\n\n4. Task\n\n\n\n4. Which test is appropriate for evaluating the effect of the filter?\n\n Wilcoxon signed rank test Student t-test Welch t-test Mann-Whitney U test Paired t-test\n\n\n\nFeedback\n\nWe select a Wilcoxon signed rank test that is appropriate to compare not-normal, paired observations in two groups.\n\n\n\n\n\n\n\n\n\n5. Task\n\n\n\n5. Does the filter influence the water quality? (The warnings are related to identical values, i.e. ties, and zero differences; we ignore these here) Use to appropriate test to answer this question!\n\n The filter significantly influences water quality (Wilcoxon signed rank test, p = 0.00497). Equal or greater rank differences between the pairs would occur under H0 (no effect of the filter) only with a probability of 0.497 %. We could prove an effect of the water filter with &gt;95% certainty. The filter has no significant influence on the water quality (Wilcoxon signed rank test, p = 0.00497).\n\n\n\nClick here to see the solution\n\nH0 of the Wilcoxon signed rank test is that the location shift between the two groups equals zero, i.e. the difference between the pairs follows a symmetric distribution around zero. As p &lt; 0.05, we can reject H0. The filter significantly influences water quality. (In case of ties also see the function wilcox.test() in the package coin for exact, asymptotic and Monte Carlo conditional p-values)\n\nwilcox.test(dat$down, dat$up, paired = TRUE)\n## \n##  Wilcoxon signed rank test with continuity correction\n## \n## data:  dat$down and dat$up\n## V = 8, p-value = 0.004971\n## alternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\nH_0_ Hypothesis:\n\nShapiro: Data is normal distributed\nWilcoxon: No differences in their ranks"
  },
  {
    "objectID": "3A-Exercise.html#chicken",
    "href": "3A-Exercise.html#chicken",
    "title": "Exercise - NHST and statistical tests",
    "section": "Chicken",
    "text": "Chicken\nThe ‘chickwts’ experiment was carried out to measure and compare the effectiveness of various feed supplements on the growth rate of chickens. We are interested in two questions: Does the feed type influence the chickens weight at all? Which feed types result in significantly different chicken weights?\n\ndat = chickwts\n\n\n\n\n\n\n\n1. Task\n\n\n\nAnalyze the data and answer the following questions.\n1. Visualize the data. What is an appropriate plot for this kind of data?\n\n Histogram Mosaicplot Scatterplot Boxplot\n\n\n\nClick here to see the solution\n\nAn appropriate visualization for one numeric and one categorial variable is a boxplot. Using notch = T in the function boxplot(), adds confidence interval for the median (the warning here indicates that we are not very confident in the estimates of the medians as the number of observations is rather small, you can see at the notches that go beyond the boxes).\n\ndat = chickwts\nboxplot(weight ~ feed, data = dat)\n\n\n\nboxplot(weight ~ feed, data = dat, notch = T)\n## Warning in (function (z, notch = FALSE, width = NULL, varwidth = FALSE, : some\n## notches went outside hinges ('box'): maybe set notch=FALSE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. Task\n\n\n\n2. Can you apply an ANOVA to this data? What are the assumptions for an ANOVA? Remember: you have to test two things for the groups (for this exercise it is enough if you test the groups “casein” and “horsebean” only).\n\n We have no indication to assume that the data is not-normally distributed or that the variances are different. We should use a Kruskal-Wallis test. The Shapiro-Wilk test of normality and the test for equal variances are not signficant. We cannot use an ANOVA. We have no indication to assume that the data is not-normally distributed or that the variances are different. We can use an ANOVA. Boxplot\n\n\n\nClick here to see the solution\n\nThe two requirements for applying an ANOVA are 1) the data in each group are normally distributed, and 2) the variances of the different groups are equal. For 1) we again use a Shapiro-Wilk test. For 2) we can use the function var.test() or for all feed types the function bartlett.test(). All tests are not significant, and we thus have no indication to assume that the data is not-normally distributed or that the variances are different. We can use an ANOVA.\n\n# get data of each group\ncasein = dat$weight[dat$feed == \"casein\"]\nhorsebean = dat$weight[dat$feed == \"horsebean\"]\n\nshapiro.test(casein)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  casein\n## W = 0.91663, p-value = 0.2592\nshapiro.test(horsebean)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  horsebean\n## W = 0.93758, p-value = 0.5264\n# H0 normally distributed\n# not rejected, normality assumption is okay\n\nvar.test(casein, horsebean)\n## \n##  F test to compare two variances\n## \n## data:  casein and horsebean\n## F = 2.7827, num df = 11, denom df = 9, p-value = 0.1353\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 95 percent confidence interval:\n##  0.711320 9.984178\n## sample estimates:\n## ratio of variances \n##           2.782737\n# H0 ratio of variances is 1 = groups have the same variance\n# not rejected, same variances is okay\n\n\n### Extra: testing the assumptions for all groups:\n\n# Normality test using the dplyr package\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\ndat %&gt;% \n  group_by(feed) %&gt;% \n  summarise(p = shapiro.test(weight)$p)\n## # A tibble: 6 × 2\n##   feed          p\n##   &lt;fct&gt;     &lt;dbl&gt;\n## 1 casein    0.259\n## 2 horsebean 0.526\n## 3 linseed   0.903\n## 4 meatmeal  0.961\n## 5 soybean   0.506\n## 6 sunflower 0.360\n\n# Bartlett test for equal variances\nbartlett.test(weight ~ feed, dat)\n## \n##  Bartlett test of homogeneity of variances\n## \n## data:  weight by feed\n## Bartlett's K-squared = 3.2597, df = 5, p-value = 0.66\n\n\n\n\n\n\n\n\n\n\n3. Task\n\n\n\n3. Apply an ANOVA or the non-parametric test. How would you describe the result in a thesis or publication?\n\n We have proven that the feed type influences the chicken weight (ANOVA, p = 5.94e-10). The chicken weights differ significantly between the six feed types (ANOVA, p = 5.94e-10). The feed type significantly influences the chicken weight (ANOVA, p = 5.94e-10). Boxplot\n\n\n\nClick here to see the solution\n\nH0 of the ANOVA is that feed has no influence on the chicken weight. As p &lt; 0.05, we reject H0. In the result section, we would write something like: “The feed type significantly influenced the chicken weight (ANOVA, p = 5.94e-10).”\n\nfit = aov(weight ~ feed, data = dat)\nsummary(fit)\n##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n## feed         5 231129   46226   15.37 5.94e-10 ***\n## Residuals   65 195556    3009                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe answer “The chicken weights differ significantly between the six feed types (ANOVA, p = 5.94e-10).” is not precise enough - there are significant differences, but an ANOVA doesn’t test if this is true for all comparisons. ANOVA only tests globally.\n\n\n\n\n\n\n\n\n\n4. Task\n\n\n\n4. Also apply the alternative test and compare p-values. Which of the tests has a higher power?\n\n ANOVA The alternative test Both have the same power The p-value doesn't help us to identify the power.\n\n\n\nClick here to see the solution\n\nThe non-parametric alternative of an ANOVA is the Kruskal-Wallis test, which should be applied if the data is not normally distributed. In this example, the test comes to the same conclusion: H0 is rejected, the feed type has a significant effect on the chicken weight. The p-value, however, is not as small as in the ANOVA. The reason for this is that non-parametric tests have a lower power than parametric ones as they only use the ranks of the data. Therefore, the ANOVA is preferred over the non-parametric alternative in case its assumptions are fulfilled.\n\nkruskal.test(chickwts$weight, chickwts$feed)\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  chickwts$weight and chickwts$feed\n## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07\n\nThe p-value in the Kruskal-Wallis test is not as small as in the ANOVA. The reason for this is that non-parametric tests have a lower power than parametric ones as they only use the ranks of the data. Therefore, the ANOVA is preferred over the non-parametric alternative in case its assumptions are fulfilled.\n\n\n\n\n\n\n\n\n\n5. Task\n\n\n\n5. Use the result of the ANOVA to carry out a post-hoc test. How many of the pairwise comparisons indicate significant differences between the groups? \n\n\nClick here to see the solution\n\n\nTukeyHSD(fit)\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = weight ~ feed, data = dat)\n## \n## $feed\n##                            diff         lwr       upr     p adj\n## horsebean-casein    -163.383333 -232.346876 -94.41979 0.0000000\n## linseed-casein      -104.833333 -170.587491 -39.07918 0.0002100\n## meatmeal-casein      -46.674242 -113.906207  20.55772 0.3324584\n## soybean-casein       -77.154762 -140.517054 -13.79247 0.0083653\n## sunflower-casein       5.333333  -60.420825  71.08749 0.9998902\n## linseed-horsebean     58.550000  -10.413543 127.51354 0.1413329\n## meatmeal-horsebean   116.709091   46.335105 187.08308 0.0001062\n## soybean-horsebean     86.228571   19.541684 152.91546 0.0042167\n## sunflower-horsebean  168.716667   99.753124 237.68021 0.0000000\n## meatmeal-linseed      58.159091   -9.072873 125.39106 0.1276965\n## soybean-linseed       27.678571  -35.683721  91.04086 0.7932853\n## sunflower-linseed    110.166667   44.412509 175.92082 0.0000884\n## soybean-meatmeal     -30.480519  -95.375109  34.41407 0.7391356\n## sunflower-meatmeal    52.007576  -15.224388 119.23954 0.2206962\n## sunflower-soybean     82.488095   19.125803 145.85039 0.0038845\n\nYou can also summarize this more formally:\n\naov_post &lt;- TukeyHSD(fit)\nsum(aov_post$feed[,4] &lt; 0.05)\n## [1] 8\n\n\n\n\n\n\n\n\n\n\n6. Task\n\n\n\n6. Which conclusion about the feed types ‘meatmeal’ and ‘casein’ is correct?\n\n It is statistically proven that the feed types 'meatmeal' and 'casein' produce the same chicken weight. The Null-hypothesis, i.e. there is no weight difference between the feed types 'meatmeal' and 'casein', is accepted. The experiment did not reveal a significant weight difference between the feed types 'meatmeal' and 'casein'.\n\n\n\nFeedback\n\nThe experiment did not reveal a significant weight difference between the feed types ‘meatmeal’ and ‘casein’. Remember that we cannot prove or accept H0; we can only reject it.\nYou can also visualize the comparisons using the function glht() from the multcomp package.\n\nlibrary(multcomp) # install.packages(\"multcomp\")\n## Loading required package: mvtnorm\n## Loading required package: survival\n## Loading required package: TH.data\n## Loading required package: MASS\n## \n## Attaching package: 'MASS'\n## The following object is masked from 'package:dplyr':\n## \n##     select\n## \n## Attaching package: 'TH.data'\n## The following object is masked from 'package:MASS':\n## \n##     geyser\ntuk &lt;- glht(fit, linfct = mcp(feed = \"Tukey\"))\n\n# extract information\ntuk.cld &lt;- cld(tuk)\n\n# use sufficiently large upper margin\nold.par &lt;- par(mai=c(1,1,1.25,1), no.readonly = TRUE)\n\n# plot\nplot(tuk.cld)\n\n\n\npar(old.par)"
  },
  {
    "objectID": "3A-Exercise.html#titanic",
    "href": "3A-Exercise.html#titanic",
    "title": "Exercise - NHST and statistical tests",
    "section": "Titanic",
    "text": "Titanic\nThe dataset ‘titanic’ from the EcoData package (not to confuse with the dataset ‘Titanic’) provides information on individual passengers of the Titanic.\n\nlibrary(EcoData) #or: load(\"EcoData.Rdata\"), if you had problems with installing the package\ndat = titanic\n\nAnswer the following questions.\n\n\n\n\n\n\n1. Task\n\n\n\n\nWe are interested in first and second class differences only. Reduce the dataset to these classes only. How can you do this in R?\n\n\n\nClick here to see the solution\n\nThe dataset can be reduced in different ways. All three options result in a dataset with class 1 and 2 only.\n\nlibrary(EcoData)\ndat = titanic\n\ndat = dat[dat$pclass == 1 | dat$pclass == 2, ]\ndat = dat[dat$pclass %in% 1:2, ] # the same\ndat = dat[dat$pclass != 3, ] # the same\n\n\n\n\n\n\n\n\n\n\n2. Task\n\n\n\n\nDoes the survival rate between the first and second class differ? Hint: you can apply the test to a contigency table of passenger class versus survived, i.e. table(dat$pclass, dat$survived). TRUEFALSE\n\n\n\nClick here to see the solution\n\nWe use the test of equal proportions here. H0, proportions in the two groups are equal, is rejected. The survival probability in class 1 and class 2 is significantly different. Note that the estimated proportions are for mortality not for survival because 0=died is in the first column of the table. Thus it is considered the “success” in the prop.test().\n\ntable(dat$pclass, dat$survived)\n##    \n##       0   1\n##   1 123 200\n##   2 158 119\nprop.test(table(dat$pclass, dat$survived))\n## \n##  2-sample test for equality of proportions with continuity correction\n## \n## data:  table(dat$pclass, dat$survived)\n## X-squared = 20.772, df = 1, p-value = 5.173e-06\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  -0.2717017 -0.1074826\n## sample estimates:\n##    prop 1    prop 2 \n## 0.3808050 0.5703971\n\n\n\n\n\n\n\n\n\n\n3. Task\n\n\n\n\nIs the variable passenger age normally distributed?\n\n\n The data seems normally distributed. The distribution significantly differs from normal.\n\n\n\nClick here to see the solution\n\nThe distribution of passenger age significantly differs from normal.\n\nhist(dat$age, breaks = 20)\n\n\n\nshapiro.test(dat$age)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  dat$age\n## W = 0.9876, p-value = 0.00014\n\n\n\n\n\n\n\n\n\n\n4. Task\n\n\n\n\nIs the variable Body Identification Number (body) uniformly distributed?\n\nUniform distribution == all values within a range are equally distributed:\n\nhist(runif(10000))\n\n\n\n\nHint: ks.test() (check documentation, see examples)\n\n The distribution significantly differs from uniform. The distribution looks uniform.\n\n\n\nClick here to see the solution\n\nThe distribution of body significantly differs from uniform.\n\nhist(dat$body, breaks = 20)\n\n\n\nks.test(dat$body, \"punif\")\n## \n##  Exact one-sample Kolmogorov-Smirnov test\n## \n## data:  dat$body\n## D = 1, p-value = 2.22e-16\n## alternative hypothesis: two-sided\n\n\n\n\n\n\n\n\n\n\n5. Task\n\n\n\n\nIs the correlation between fare and age significant? TRUEFALSE\n\n\n\nClick here to see the solution\n\nThe correlation between fare and age is non-significant. You can also plot the data using the scatter.smooth function.\n\ncor.test(dat$fare, dat$age)\n## \n##  Pearson's product-moment correlation\n## \n## data:  dat$fare and dat$age\n## t = 1.9326, df = 543, p-value = 0.0538\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.001346105  0.165493055\n## sample estimates:\n##        cor \n## 0.08265257\nscatter.smooth(dat$fare, dat$age)"
  },
  {
    "objectID": "3A-Exercise.html#simulation-of-type-i-and-ii-error",
    "href": "3A-Exercise.html#simulation-of-type-i-and-ii-error",
    "title": "Exercise - NHST and statistical tests",
    "section": "Simulation of Type I and II error",
    "text": "Simulation of Type I and II error\nThis is an additional task for those who are fast! Please finish the other parts first before you continue here!\nAnalogously to the previous example of simulating the test statistic, we can also simulate error rates. Complete the code …\n\nPperGroup = 50\npC = 0.5\npT = 0.5\n\npvalues = rep(NA, 1000)\n\nfor(i in 1:1000){\n  control = rbinom(n = 1, size = PperGroup, prob = pC)\n  treat = rbinom(n = 1, size = PperGroup, prob = pT)\n  #XXXX\n}\n\n… and answer the following questions for the prop.test in R:\n\n\n\n\n\n\n1. Task\n\n\n\n\nHow does the distribution of p-values and the number of false positive (Type I error) look like if pC = pT\n\n\n\nClick here to see the solution\n\n\nPperGroup = 50\npC = 0.5\npT = 0.5\n\npvalues = rep(NA, 1000)\npositives = rep(NA, 1000)\n\nfor(i in 1:1000){\n  control = rbinom(1, PperGroup, prob = pC )\n  treatment = rbinom(1, PperGroup, prob = pT )\n  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value\n  positives[i] = pvalues[i] &lt;= 0.05\n}\nhist(pvalues)\n\n\n\ntable(positives)\n## positives\n## FALSE  TRUE \n##   966    34\nmean(positives) \n## [1] 0.034\n\n# type I error rate = false positives (if data simulation etc. is performed several times, this should be on average 0.05 (alpha))\n\n\n\n\n\n\n\n\n\n\n2. Task\n\n\n\n\nHow does the distribution of p-values and the number of true positive (Power) look like if pC != pT, e.g. 0.5, 0.6\n\n\n\nClick here to see the solution\n\npC != pT with difference 0.1\n\nPperGroup = 50\npC = 0.5\npT = 0.6\n\npvalues = rep(NA, 1000)\npositives = rep(NA, 1000)\n\nfor(i in 1:1000){\n  control = rbinom(1, PperGroup, prob = pC )\n  treatment = rbinom(1, PperGroup, prob = pT )\n  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value\n  positives[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value &lt; 0.05\n}\nhist(pvalues)\n\n\n\ntable(positives)\n## positives\n## FALSE  TRUE \n##   878   122\nmean(pvalues &lt; 0.05) # = power (rate at which effect is detected by the test)\n## [1] 0.122\n# power = 1- beta &gt; beta = 1-power = typeII error rate\n1-mean(pvalues &lt; 0.05)\n## [1] 0.878\n\n## Factors increasing power and reducing type II errors:\n# - increase sample size\n# - larger real effect size (but this is usually fixed by the system)\n\n\n\n\n\n\n\n\n\n\n3. Task\n\n\n\n\nHow does the distribution of p-values and the number of false positive (Type I error) look like if you modify the for loop in a way that you first look at the data, and then decide if you test for greater or less?\n\n\n\nClick here to see the solution\n\nYou first look at the data, and then decide if you test for greater or less:\n\n# ifelse(test,yes,no)\nPperGroup = 50\npC = 0.5\npT = 0.5\n\nfor(i in 1:1000){\n  control = rbinom(1, PperGroup, prob = pC )\n  treatment = rbinom(1, PperGroup, prob = pT )\n  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2), \n                        alternative= ifelse(mean(control)&gt;mean(treatment),\n                                            \"greater\",\"less\"))$p.value\n  positives[i] = prop.test(c(control, treatment), rep(PperGroup, 2),\n                     alternative= ifelse(mean(control)&gt;mean(treatment),\n                                         \"greater\",\"less\"))$p.value &lt; 0.05\n}\nhist(pvalues)\n\n\n\ntable(positives)\n## positives\n## FALSE  TRUE \n##   942    58\nmean(pvalues &lt; 0.05) \n## [1] 0.058\n# higher false discovery rate"
  },
  {
    "objectID": "2A-DescriptiveStatistics.html#plotting",
    "href": "2A-DescriptiveStatistics.html#plotting",
    "title": "3  Plotting and describing data",
    "section": "3.1 Plotting",
    "text": "3.1 Plotting\n\n3.1.1 One variable\n\n3.1.1.1 Numerical variable - Histogram and Boxplot\n\nThe histogram plots the frequency of the values of a numerical variable with bins (otherwise each unique value will appear only once, the range will be cut in n elements). The number of bins is automatically inferred by the function but can be also changed by the user\nThe boxplot plots the distribution of a numerical variable based on summary statistics (the quantiles). The boxplot is particular useful for comparing/contrasting a numerical with a categorical variable (see below)\n\n\nBase Rggplot2\n\n\n\npar(mfrow = c(1,2)) # number of plots, one row, two columns\nhist(iris$Sepal.Length, \n     main = \"Histogram\", # title\n     xlab = \"Sepal.Length\", \n     ylab = \"Frequency\",\n     las = 1) # rotation of x and y values (las = 1, all of them should be horizontal)\n\nboxplot(iris$Sepal.Length,      \n        main = \"Boxplot\", # title\n        ylab = \"Values\")\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nplt1 = \n  ggplot(iris, aes(x = Sepal.Length)) +\n    geom_histogram() +\n    ggtitle(\"Histogram\") +\n    xlab(\"Sepal.Length\") +\n    ylab(\"Frequency\") +\n    theme_bw() # scientific theme (white background)\n\nplt2 = \n  ggplot(iris, aes(y = Sepal.Length)) +\n    geom_boxplot() +\n      ggtitle(\"Boxplot\") +\n      ylab(\"Values\") +\n      theme_bw() \n\n\nggarrange(plt1, plt2, ncol = 2L, nrow = 1L)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n3.1.1.2 Categorical variable - Barplot\n\nstr(mtcars)\n## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\ncounts = table(mtcars$cyl)\ncounts\n## \n##  4  6  8 \n## 11  7 14\n\n\nBase Rggplot2\n\n\n\nbarplot(counts, \n        main = \"Barplot of Cyl\",\n        ylab = \"Number of occurrences\",\n        xlab = \"Cyl levels\",\n        col = \"#4488AA\")\n\n\n\n\n\n\n\nggplot(mtcars, aes(x = cyl)) +\n  geom_bar(fill = \"#4488AA\") +\n    ggtitle(\"Barplot of Cyl\") +\n    xlab(\"Number of occurrences\") +\n    ylab(\"Cyl levels\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n3.1.2 Two variables\nThe general idea of plotting is to look for correlations / associations between variables, i.e. is there a non-random pattern between the two variables.\n\n3.1.2.1 Numerical vs numerical variable - Scatterplot\n\nBase Rggplot2\n\n\n\n# Scatterplot\npar(mfrow = c(1,2))\nplot(airquality$Solar.R, airquality$Ozone)\n\n# plot(Ozone ~ Solar.R, data = airquality) #the same\n\n# different symbol for each month\nplot(Ozone ~ Solar.R, data = airquality, pch = Month)\n\n\n\n\nWe can also add other objects such as lines to our existing plot:\n\npar(mfrow = c(1,1))\nplot(Ozone ~ Solar.R, data = airquality)\nabline(h = 50)\n\n\n\n\n\n\n\nplt1 = ggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n        geom_point() +\n        theme_bw()\n\nplt2 = ggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n        geom_point(shape = airquality$Month) +\n        theme_bw()\n\nggarrange(plt1, plt2, ncol = 2L, nrow = 1L)\n## Warning: Removed 42 rows containing missing values (`geom_point()`).\n## Removed 42 rows containing missing values (`geom_point()`).\n\n\n\n\nWe can also add other objects such as lines to our existing plot:\n\nggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n  geom_point(shape = airquality$Month) +\n  geom_abline(intercept = 50, slope = 0) +\n    theme_bw()\n## Warning: Removed 42 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n3.1.2.2 Categorical vs numerical variable - Boxplot\nOften we have a numerical variable (e.g. weight/fitness) and a categorical vairable that tells us the group of the observation (e.g. control or treatment). To compare visually now the distributions of the numerical variable between the levels of the grouping variable, we can use a boxplot\n\nBase Rggplot2\n\n\n\nboxplot(mpg ~ cyl, mtcars, notch=TRUE) # formula notation\n## Warning in (function (z, notch = FALSE, width = NULL, varwidth = FALSE, : some\n## notches went outside hinges ('box'): maybe set notch=FALSE\n\n\n\n# boxplot(x = mtcars$cyl, y = mtcars$mpg) # the same\n\n\n\n\nggplot(mtcars, aes(y = mpg, group = cyl)) +\n  geom_boxplot(notch=TRUE) +\n    theme_bw()\n## Notch went outside hinges\n## ℹ Do you want `notch = FALSE`?\n## Notch went outside hinges\n## ℹ Do you want `notch = FALSE`?"
  },
  {
    "objectID": "2B-SummaryStatistics.html#correlation",
    "href": "2B-SummaryStatistics.html#correlation",
    "title": "4  Summary statistics",
    "section": "4.1 Correlation",
    "text": "4.1 Correlation\nIf two variables A and B are related, we say that they are correlated (correlation != causality!!). We can calculate the magnitude of this relatedness with correlation factors. Correlation factors are normalized covariances (two variables have two variances, how they vary independent of each other, and they share a covariance, how much they vary together) are in the range of \\([-1,1]\\). The Pearson’s correlation coefficient/factor can be calculated by:\n\\[\nr = cor(x_1, x_2) = \\frac{ \\frac{1}{N} \\sum_{i=1}^n (x_{1i} - \\overline{x_{1}}) (x_{2i} - \\overline{x_{2}}) }{ \\sqrt{\\frac{1}{N} \\sum_{i=1}^n (x_{1i} - \\overline{x_{1}})^2 (x_{2i} - \\overline{x_{2}})^2} }\n\\]\nThe Pearson correlation works well with linear relationship, but poorly with non-linear relationships and is sensitive to outliers. Also transforming one of the variables affects the correlation:\n\n\nCode\nA = runif(1000, -6, 6)\nB = plogis(A*3.5)\npar(mfrow = c(1, 2))\nplot(B, A)\ntext(x = 0.6, y = -4, paste0(\"Pearson: \", round(cor(A, B, method = \"pearson\"), 3)))\nplot(log(B), A)\ntext(x = -12, y = 0, paste0(\"Pearson: \", round(cor(A, log(B), method = \"pearson\"), 3)))\n\n\n\n\n\nThe correlation factor for the untransformed data is 0.9 but they have a perfect relationship.\nRank correlation factors\nIdea: Arranging the data following their order and using their rank 1…n for x und y, respectively. There two different rank correlation factors:\n\nSpearman: calculate Pearson based on ranks\nKendall: counts the number of data pairs that point in the same direction\n\n\ncor(A, B)\n## [1] 0.9096673\ncor(order(A), order(B)) # Spearman\n## [1] 1\ncor(A, B, method = \"spearman\") # Spearman\n## [1] 1\ncor(A, B, method = \"kendall\")\n## [1] 1\n\nExample:\n\ncor(airquality$Solar.R, airquality$Ozone, use = \"complete.obs\") # pearson = default\n## [1] 0.3483417\ncor(airquality$Solar.R, airquality$Ozone, use = \"complete.obs\", method = \"spearman\")\n## [1] 0.3481865\n\n# pairs plot: correlates all variables to each other\nplot(airquality)\npairs(airquality) #the same\n\n\n\n\n# you can change what is plotted in upper and lower panel:\n# copy function from help\npanel.cor &lt;- function(x, y, digits = 2, prefix = \"\", cex.cor, ...)\n{\n  usr &lt;- par(\"usr\"); on.exit(par(usr))\n  par(usr = c(0, 1, 0, 1))\n  r &lt;- abs(cor(x, y, use = \"complete.obs\")) ### complete.obs must be added manually\n  txt &lt;- format(c(r, 0.123456789), digits = digits)[1]\n  txt &lt;- paste0(prefix, txt)\n  if(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\n  text(0.5, 0.5, txt, cex = cex.cor * r)\n}\npairs(airquality, lower.panel = panel.smooth, upper.panel = panel.cor)\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDescriptive statistics characterize properties of the data without providing any test or statement of probability."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science with R",
    "section": "",
    "text": "Preface\nMaterial for a first 1-week course in Data Science, taught at the University of Regensburg.\nDay 1: Getting started, descriptive statistics\nDay 2: Hypothesis tests\nDay 3: Linear regression (simple, multivariate)\nDay 4: Generalised linear models (GLM), Multivariate statistics\nDay 5: Machine Learning, Data organisation\nLink to the UR GRIPS course here (requires UR account)"
  },
  {
    "objectID": "10-DataWrangling.html#exercise",
    "href": "10-DataWrangling.html#exercise",
    "title": "14  Exercise - Data handling",
    "section": "14.1 Exercise",
    "text": "14.1 Exercise\nIn this exercise you will practice common steps of the data preparation procedure using a dataset that was gathered in a forest in Switzerland.\nThe main steps are as follows (to solve the tasks, please carefully read the detailed instructions and hints below):\n\nRead the provided datasets treedata.csv and species.csv to R.\nHave a look at the dataset properties.\nFind values that prevent a column you expect to be numeric to do so.\nDoes the dataset contain NA-values in height? How many?\nHave a look at the data: Check for implausible values. For now, remove these values.\nAdd the species names to the dataset.\nCreate a new dataset containing only trees with both, height and dbh measurements.\nAre there any correlations within the new dataset?\nRemove all trees with rem = F4 from the dataset\nCalculate mean dbh by species.\n\n\nTo thoroughly check the dataset and perform the operations, you will need the following functions:\n\nread.csv(): Check the different options using ?read.csv\nstr(): Structure of an object\ntable(): Check the optional arguments!\nmerge(): Combine to data.frames\nas.character(): Change a vector’s class to character\nas.numeric(): Change a vector’s class to numeric.\n%in%\nis.na()\nmax()\nsummary()\ncomplete.cases()\ncor.test()\n%&gt;% and group_by() and summarize() from the dlyr package (check demonstration Part 2)\n\nRegarding the solutions, note we don’t expect you to come up with exactly this code - there are many ways to solve the problem. The solutions just give you an idea of a possible solution.\n\n14.1.1 1. Read data\nRead the provided datasets treedata.csv and species.csv to R. Use the option stringsAsFactors = FALSE in the function read.csv.\nRead the dataset treedata.csv and call it treedata. It has the following columns:\n\nspecies_code: tree species code\ndbh: diameter at breast height (1.3 m) [cm]\nheight: total height of the tree [m]. Measured only on a subset of trees.\nrem: coded values, containing remarks\n\nRead the dataset species.csv and call it species. The dataset consists of the following columns:\n\nspecies_code: tree species code (matching the one used in treedata.csv)\nspecies_scientific: Scientific species name\nspecies_english: English species name\n\n\n\n\nClick here to see the solution\n\n\n14.1.1.1 Solution\nFirst, you read in the file using read.csv. You have to specify the correct separator for your dataset, here this is “;”.\n\ntreedata &lt;- read.csv('Where/ever/you/put/it/treedata.csv', sep = \";\")\nspecies &lt;- read.csv('Hopefully/in/the/same/folder/species.csv', sep = \";\")\n\n\n\n\n\n14.1.2 2. Dataset properties\nHave a look at the properties of the dataset:\n\nWhich classes do the columns have?\nDid you expect these classes?\n\n\n\n\nClick here to see the solution\n\n\n14.1.2.1 Solution\nThe data.frame dat contains 4 columns: species, dbh (diameter at breast height [cm]), height [m] and rem, a remark. We expect the following formats:\n\n\n\ncolumn\nformat\n\n\n\n\nspecies\ncharacter\n\n\ndbh\nnumeric\n\n\nheight\nnumeric\n\n\nrem\nfactor\n\n\n\nUsing str we get an overview of the structure of a dataset:\n\nstr(treedata)\n## 'data.frame':    287 obs. of  4 variables:\n##  $ species_code: int  121 121 411 411 411 431 411 411 411 121 ...\n##  $ dbh         : chr  \"19.3\" \"21.3\" \"43\" \"25.8\" ...\n##  $ height      : num  NA NA 37.7 NA 34.4 44.4 NA NA NA NA ...\n##  $ rem         : chr  \"\" \"\" \"P7\" \"F2\" ...\n\nColumn dbh is a character, although we would have expected this one to be class numeric. This indicates, that a letter or special characters are in that column (we do not want these to be in there at all!).\n\n\n\n\n14.1.3 3. Turn character to numeric\nOne column, which we expect to be numeric, is of class character. Find the value causing this column to be character, set it to NA and turn the column into class numeric.\nNote that using ‘is.numeric()’ is not enough, if the column is a factor. This may be the case if you have used the option stringAsFactor = T in read.csv or an older version of R. Use a combination of ‘as.character()’ and ‘as.numeric()’ in that case.\n\n\n\nClick here to see the solution\n\n\n14.1.3.1 Solution\nWe suspect dbh to contain a character and we want to remove this. With the function ‘table()’, we can check all existing values in the column. There seems to be an ‘X’ in the data.\n\ntable(treedata$dbh)\n## \n## 10.1 10.2 10.4 10.6 10.7 10.9 11.2 11.4 11.5 11.9 12.3 12.5   13 13.2 13.3 13.4 \n##    1    2    2    1    1    1    2    1    1    1    1    2    1    1    1    1 \n## 13.8 13.9   14 14.4 14.8 14.9 15.2 15.4 15.5 15.8   16 16.2 16.4 16.6 16.8 17.1 \n##    3    1    1    6    1    1    3    1    1    1    1    1    1    1    4    1 \n## 17.2 17.4 17.5 17.6 17.8 17.9 18.1 18.2 18.3 18.7 18.8   19 19.1 19.2 19.3 19.4 \n##    2    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n## 19.5 19.8   20 20.2 20.4 20.7 20.8 20.9 2030 21.3 21.6 21.8 21.9   22 22.2 22.3 \n##    3    3    1    2    1    1    2    2    1    1    4    1    1    1    2    1 \n## 22.6 22.7 22.9 23.2 23.3 23.6 23.7 23.8   24 24.2 24.3 24.4 24.9   25 25.2 25.3 \n##    2    1    2    1    1    2    1    1    2    2    2    1    2    1    1    1 \n## 25.4 25.8   26 26.2 26.3 26.4 26.6 27.4 27.5 27.6 27.8 28.2 28.4 28.6 28.8 29.2 \n##    4    3    1    2    1    1    1    2    2    1    1    1    2    1    1    1 \n## 29.4 29.6 29.7 29.8 30.1 30.2 30.4 30.8 31.2 31.3 31.4 31.9   32 32.2 32.3 32.4 \n##    2    1    1    2    1    1    1    2    1    1    2    1    1    3    1    1 \n## 32.9   33 33.2 33.3 33.4 33.8 33.9 34.2 34.4 34.6 34.7 34.8   35 35.8 36.5 36.6 \n##    1    1    1    1    1    3    1    5    1    1    1    1    1    1    1    1 \n## 36.8 37.1 37.2 37.4 37.8 38.1 38.2 38.3 38.5 39.2 39.4 39.7 39.8    4  4.2  4.6 \n##    2    1    1    1    2    1    1    1    1    1    1    1    1    3    3    1 \n##  4.7   40 40.6 40.8 41.3 41.4 41.5 42.3 42.4   43 43.2 43.4 43.6 44.2 44.5 44.8 \n##    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 45.2 45.7 47.8   48 48.7 48.8   49 49.7 49.8  5.1  5.2  5.4  5.5  5.6  5.7  5.8 \n##    1    1    1    1    1    1    1    1    1    1    3    3    1    1    1    2 \n##  5.9 51.2  6.2  6.3  6.4  6.5  6.6  6.8  6.9    7  7.2  7.3  7.4  7.6  7.8  8.1 \n##    2    1    4    2    1    1    2    1    2    1    1    1    2    1    2    1 \n##  8.2  8.3  8.5  8.8  8.9    9  9.4  9.5  9.6  9.9    X \n##    3    2    1    1    2    1    1    2    2    1    1\n\nTo automatically search for characters, we can check if dbh contains a character that is part of LETTERS (capital letters) or letters:\n\ntreedata[treedata$dbh %in% LETTERS | treedata$dbh %in% letters,]\n##     species_code dbh height rem\n## 159          411   X   27.2  F4\n\nA more advanced option would be to use grepl. If we are using the solution above, we will only find the value if it is exactly one character. Things get a bit more complicated, if we have special characters, e.g, a *.\n\n\nx &lt;- rep(c(1, 3, 5, '*', 'AA', ',', 9), 2)\nx[grepl(\"^[A-Za-z]+$\", x, perl = T)]\n## [1] \"AA\" \"AA\"\nx[!grepl('[^[:punct:]]', x, perl =T)]\n## [1] \"*\" \",\" \"*\" \",\"\n\nWe want to set the X in dbh to NA (probably, this is a transcription error, so one could also have a look at the field forms…).\n\n\ntreedata$dbh[treedata$dbh == 'X'] &lt;- NA\nstr(treedata)\n## 'data.frame':    287 obs. of  4 variables:\n##  $ species_code: int  121 121 411 411 411 431 411 411 411 121 ...\n##  $ dbh         : chr  \"19.3\" \"21.3\" \"43\" \"25.8\" ...\n##  $ height      : num  NA NA 37.7 NA 34.4 44.4 NA NA NA NA ...\n##  $ rem         : chr  \"\" \"\" \"P7\" \"F2\" ...\n\nJust removing the ‘X’ does not turn a character to numeric! R provides the function as.numeric, which might be of use in this case.\n\ntreedata$dbh &lt;- as.numeric(treedata$dbh)\nhead(treedata$dbh)\n## [1] 19.3 21.3 43.0 25.8 38.5 36.8\n\n\n\n\n\n14.1.4 4. NA- values in height\nCheck for NA’s in the column height:\n\nHow many NA’s do appear in this column?\nDid you expect this column to contain NA’s? Why?\n\n\n\n\nClick here to see the solution\n\n\n14.1.4.1 Solution\n\nsummary(treedata)\n##   species_code        dbh              height           rem           \n##  Min.   :101.0   Min.   :   4.00   Min.   :  4.60   Length:287        \n##  1st Qu.:121.0   1st Qu.:  11.60   1st Qu.: 21.75   Class :character  \n##  Median :411.0   Median :  21.60   Median : 29.10   Mode  :character  \n##  Mean   :329.8   Mean   :  29.22   Mean   : 26.30                     \n##  3rd Qu.:411.0   3rd Qu.:  31.40   3rd Qu.: 32.62                     \n##  Max.   :920.0   Max.   :2030.00   Max.   :110.88                     \n##                  NA's   :1         NA's   :221\n\nsum(is.na(treedata$height))\n## [1] 221\ntable(is.na(treedata$height))\n## \n## FALSE  TRUE \n##    66   221\n\nnrow(treedata[is.na(treedata$height),])\n## [1] 221\ntable(treedata$height, useNA = 'ifany')\n## \n##    4.6    4.7    4.8    5.7    5.8    6.1    6.2    6.8    7.3    7.9   10.8 \n##      1      2      1      1      1      1      1      1      1      1      1 \n##   11.1   11.3   11.5   14.2   21.7   21.9     22   22.4   23.3   23.8   24.5 \n##      1      1      1      1      1      1      1      2      1      2      1 \n##   24.8   26.1   27.2   27.5   27.8   28.6     29   29.2   29.4   29.8   29.9 \n##      1      1      1      2      1      1      1      1      1      1      1 \n##   30.2   30.3   30.4   31.3   31.5   31.9     32   32.1   32.3   32.4   32.7 \n##      1      2      1      1      1      1      2      1      1      1      1 \n##   32.8   33.2   33.5   33.8   34.2   34.4   35.4   35.8   37.7   38.2   38.6 \n##      1      1      1      1      1      1      1      1      2      1      1 \n##     39   40.8   44.4 110.88   &lt;NA&gt; \n##      1      1      1      1    221\n\n\nThe dataset contains 221 NA.\nSince height has been measured only on a subset of the trees, we expect this column to contain NA-values.\n\n\n\n\n\n14.1.5 5. Implausible values\nThe dataset contains some implausible values (completely out of range!). Find and replace these values with NA.\n\n\n\nClick here to see the solution\n\n\n14.1.5.1 Solution\nWhat values are implausible? The dataset contains different species. A value which is plausible for species A might be implausible for species B. However, for now, we will not go into the details here.\nDo a visual check of the dataset\n\nboxplot(treedata$height, main = 'Height')\n\n\n\nboxplot(treedata$dbh, main ='DBH')\n\n\n\n\nmax(treedata$dbh, na.rm = T)\n## [1] 2030\nmax(treedata$height, na.rm = T)\n## [1] 110.88\n\ntreedata[treedata$dbh &gt; 500 & !is.na(treedata$dbh),]\n##     species_code  dbh height rem\n## 114          411 2030     NA  F2\ntreedata[treedata$height &gt; 50 & !is.na(treedata$height),]\n##     species_code  dbh height rem\n## 234          411 36.6 110.88\n\nThere seems to be one outlier in both datasets which can be seen as implausible: No tree is more than 100 m of height and no tree has a diameter &gt; 20 m (These values can be considered implausible for trees in Switzerland).\nWe will now remove these values from our dataset by setting them to NA - this might not always be the best option, there are also statistical models that can account for such errors!\n\ntreedata$dbh[treedata$dbh &gt; 500] &lt;- NA\ntreedata$height[treedata$height &gt; 50] &lt;- NA\n\nboxplot(treedata$dbh, main = 'DBH')\n\n\n\nboxplot(treedata$height, main = 'Height')\n\n\n\n\n\n\n\n\n14.1.6 6. Add species names\nAdd the species names from the species dataset to the treedata dataset.\nHint: ?merge\n\n\n\nClick here to see the solution\n\n\n14.1.6.1 Solution\nUse merge to add species names to the dataset treedata. For adding only one column, match is a helpful function.\n\ntreedata &lt;- merge(treedata, species, by = \"species_code\")\nhead(treedata)\n##   species_code  dbh height rem         species_scientific species_english\n## 1          101  8.1    5.7     Picea abies (L.) H. Karst.   Norway Spruce\n## 2          101 10.4     NA     Picea abies (L.) H. Karst.   Norway Spruce\n## 3          101  5.2     NA  S0 Picea abies (L.) H. Karst.   Norway Spruce\n## 4          101  9.6     NA     Picea abies (L.) H. Karst.   Norway Spruce\n## 5          101  8.2    6.2     Picea abies (L.) H. Karst.   Norway Spruce\n## 6          101  7.4    6.1     Picea abies (L.) H. Karst.   Norway Spruce\n\n# Alternative using match():\n# treedata$species_english &lt;- species$species_english[match(treedata$species_code, species$species_code)]\n\n\n\n\n\n14.1.7 7. Remove F4\nThe remark F4 indicates, that the dbh-measurements might be flawed. Remove these trees from treedata.\n\n\n\nClick here to see the solution\n\n\n14.1.7.1 Solution\n\ntreedata &lt;- treedata[!treedata$rem %in% 'F4',]\n\n\n\n\n\n14.1.8 8. Select trees that contain both dbh and height measurements\nSubset treedata to trees where both height and dbh measurements were carried out. Call the result dbh_height.\n\n\n\nClick here to see the solution\n\n\n14.1.8.1 Solution\nThis can be done using complete.cases using\n\ndbh_height &lt;- treedata[complete.cases(treedata$dbh, treedata$height),]\n\n\n\n\n\n14.1.9 9. Correlations?\nCheck for correlations in the dataset dbh_height. Use a test and plot height vs dbh.\n\n\n\nClick here to see the solution\n\n\n14.1.9.1 Solution\n\ncor.test(dbh_height$dbh\n         , dbh_height$height)\n## \n##  Pearson's product-moment correlation\n## \n## data:  dbh_height$dbh and dbh_height$height\n## t = 13.147, df = 47, p-value &lt; 2.2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.8066074 0.9348068\n## sample estimates:\n##       cor \n## 0.8866893\n\nplot(dbh_height$dbh\n     , dbh_height$height\n     , ylab = 'Height'\n     , xlab = 'DBH')\n\n\n\n\n\n\n\n\n14.1.10 10. Export a plot\nWhen generating a plot within Rstudio, it is by default shown in the Plots window. However, sometimes we want to export a plot and store it in a file for use elsewhere. There are graphics devices that support different formats, such as PDF (vector-based) or PNG (pixel-based). The basic workflow is to start a graphics device and tell it where to store the output file, then create the plot, and finally close the device again. Only if you close the device properly will a valid file be generated. For example, to create a PNG file:\n\n# Open graphics device\npng(filename = \"/path/to/store/plot.png\",  # where to store the output file\n    width = 620,  # width of the plot in pixels\n    height = 480, # height of the plot in pixels\n    bg = \"white\") # background color\n\n# Plot something\nplot(1:5,rnorm(5), type ='b', col=1:5, pch = 19 )\n\n# Close the device to finish file writing\ndev.off()\n\nNow, export one of the plots that we generated above to a PNG file, using your favorite background color. (You can also try and use another device, e.g. pdf(), just make sure to use the matching file extension in the file name.)\n\n\nClick here to see the solution\n\n\n14.1.10.1 Solution\n\n# Open device\npng(filename = \"plot.png\", # \"/path/to/store/plot.png\",  # where to store the output file\n    width = 620,  # width of the plot in pixels\n    height = 480, # height of the plot in pixels\n    bg = \"lightskyblue2\") # background color\n\n# Scatterplot from section 9:\nplot(dbh_height$dbh\n     , dbh_height$height\n     , ylab = 'Height'\n     , xlab = 'DBH'\n     , main = 'Trees')\n\n# Close the device to finish file writing\ndev.off()\n## quartz_off_screen \n##                 2\n\n\n\n\n\n14.1.11 11. Calculate mean dbh per species.\nFor calculating summary statistics, the dplyr package is really helpful. It is part of the tidyverse environment, which was designed for data science. If you work with large and complex datasets or if you have to derive many new variables, I really recommend that you have a look at this. Also, the syntax for dplyr is quite intuitive.\nFor this exercise, use the dplyr package to calculate mean dbh per species. If you need help on this, check the demonstration of Part 2 where we calculated summary statistics for groups using the dplyr package!\n\n\nClick here to see the solution\n\n\n14.1.11.1 Solution\n\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\ntreedata %&gt;% \n  group_by(species_english) %&gt;% \n  summarize(N = n(),\n            meanDBH = mean(dbh, na.rm = T), \n            sdDBH = sd(dbh, na.rm = T))\n## # A tibble: 10 × 4\n##    species_english         N meanDBH  sdDBH\n##    &lt;chr&gt;               &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n##  1 Beech                 131   29.6  10.1  \n##  2 Commom Yew              2   18.8   0.212\n##  3 European Ash           12   30.6   4.14 \n##  4 European Silver Fir    75   11.8   5.55 \n##  5 Ivy                     6    5.13  0.927\n##  6 Little Leaf Linden      4   13.3   9.88 \n##  7 Norway Maple            9   28.2  11.1  \n##  8 Norway Spruce          15    7.77  1.89 \n##  9 Scotch Elm              1   17.9  NA    \n## 10 Sycamore               12   24.0   3.91"
  },
  {
    "objectID": "3B-HypothesisTests.html#comparison-of-mean-of-two-or-more-groups",
    "href": "3B-HypothesisTests.html#comparison-of-mean-of-two-or-more-groups",
    "title": "6  Statistical tests",
    "section": "6.1 Comparison of mean of two or more groups",
    "text": "6.1 Comparison of mean of two or more groups\nMany tests aim at showing that variables are significantly different between groups, i.e. have different means/medians. In all these tests, H0 is that there is no difference between the groups. The following decision tree helps to select the appropriate test.\nDecision tree for statistical tests\nRemark 1: Tests for 2 groups also work for one group only. Then they test whether the mean is equal to 0.\nRemark 2: Paired / unpaired: this means that observations in the groups are linked to each other. An example for unpaired data is a typical experiment with 10 observations in the control group and 10 observations in the treatment group. An example for paired data is when the same individuals were exposed to the treatment and to the control. The observations of each individual would belong together (pairs).\nRemark 3: Parametric: assumption of normal distribution. Non-parametric = no assumption for the distribution.\nRemark 4: Blue text: If a test for more than two groups is significant, post-hoc tests are carried out in a second step. These check all possible comparisons of groups for significant differences by adjusting p-values for multiple testing.\n\n6.1.1 Tests for 2 groups\n\n6.1.1.1 t-Test\nThe t-test can draw conclusions about the mean(s) of 1 or 2 normally-distributed groups.\n\n## Classical example: Student's sleep data\nplot(extra ~ group, data = sleep)\n\n\n\n\nBe aware: The line in the box plot does not show the mean but the median.\n\n## Formula interface\nt.test(extra ~ group, data = sleep)\n## \n##  Welch Two Sample t-test\n## \n## data:  extra by group\n## t = -1.8608, df = 17.776, p-value = 0.07939\n## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n## 95 percent confidence interval:\n##  -3.3654832  0.2054832\n## sample estimates:\n## mean in group 1 mean in group 2 \n##            0.75            2.33\n\nThis output tells us, that the difference in means between the 2 groups is not significant(p-value ≥ 0.05, specifically: p-value = 0.07939), provided that our significance level is 0.05.\nThe underlying Null-hypothesis is that the true difference in means is equal to 0. In the last two lines of the output you can see the means of the respective groups. Even though the means seem to be quite different, the difference is not significant, this could be due to the small sample size of only 10 students per group.\nLet’s look at different settings of the t-test:\n\n\n6.1.1.2 t-test, H0: one group, mean = 0\nThe Null-hypothesis here is that the mean of the observed group is equal to 0.\n\nx = rnorm(20, mean = 2)\nt.test(x)\n## \n##  One Sample t-test\n## \n## data:  x\n## t = 7.4679, df = 19, p-value = 4.587e-07\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  1.577591 2.806249\n## sample estimates:\n## mean of x \n##   2.19192\n\np-value &lt; 0.05 means we can reject the Null-hypothesis, i.e. the mean of the observed group is significantly different from 0.\n\n\n6.1.1.3 t-test, H0: two groups, equal means, equal variances\nThe Null-hypothesis here is that the two observed groups have the same mean and the same variance (specified by the argument var.equal = T).\n\nx1 = rnorm(20, mean = 2)\nx2 = rnorm(20, mean = 3)\nt.test(x1,x2, var.equal = T)\n## \n##  Two Sample t-test\n## \n## data:  x1 and x2\n## t = -3.7626, df = 38, p-value = 0.0005672\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -1.9534247 -0.5867374\n## sample estimates:\n## mean of x mean of y \n##  1.729008  2.999089\n\n\n\n6.1.1.4 t-test, H0: two groups, equal means, variable variance\nThe Null-hypothesis here is that the two observed groups have the same mean and variable variances (the default setting of the argument var.equal = F).\n\nx1 = rnorm(20, mean = 2, sd = 1)\nx2 = rnorm(20, mean = 3, sd = 2)\nt.test(x1,x2)\n## \n##  Welch Two Sample t-test\n## \n## data:  x1 and x2\n## t = -2.2345, df = 26.177, p-value = 0.0342\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -1.92593937 -0.08068265\n## sample estimates:\n## mean of x mean of y \n##  2.160601  3.163912\n\n\n\n6.1.1.5 t-test, H0: two groups, equal means, variance can be different (can also set to equal)\nThe Null-hypothesis here is that the two groups are paired observations (e.g. group 1 before treatment and group 2 after treatment) have the same mean and variable variance (specified by the argument var.equal = F, which is also the default setting).\n\nx1 = rnorm(20, mean = 2)\nx2 = rnorm(20, mean = 3)\nt.test(x1,x2, paired = T, var.equal = F)\n## \n##  Paired t-test\n## \n## data:  x1 and x2\n## t = -2.0252, df = 19, p-value = 0.05713\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  -1.40778309  0.02318272\n## sample estimates:\n## mean difference \n##      -0.6923002\n\n\n\n\n6.1.2 Wilcoxon Rank Sum and Mann-Whitney U Test\nIn R, there is only one function for both tests together: wilcox.test(). The Wilcoxon rank sum test with (paired = F) is classically called Mann-Whitney U test.\n\n6.1.2.1 Mann-Whitney U Test\n\nx1 = rnorm(20, mean = 2)\nx2 = rlnorm(20, mean = 3)\n\nwilcox.test(x1, x2)\n## \n##  Wilcoxon rank sum exact test\n## \n## data:  x1 and x2\n## W = 0, p-value = 1.451e-11\n## alternative hypothesis: true location shift is not equal to 0\n\n\n\n6.1.2.2 Wilcoxon signed rank test\n\nx1 = rnorm(20, mean = 2)\nx2 = rlnorm(20, mean = 3)\n\nwilcox.test(x1, x2, paired = T)\n## \n##  Wilcoxon signed rank exact test\n## \n## data:  x1 and x2\n## V = 0, p-value = 1.907e-06\n## alternative hypothesis: true location shift is not equal to 0\n\n\n\n\n6.1.3 Tests for &gt; 2 groups\n\n6.1.3.1 Anova, unpaired\nH0 &gt;2 groups, normal distribution, equal variance, equal means, unpaired\n\nx = aov(weight ~ group, data = PlantGrowth)\nsummary(x)\n##             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n## group        2  3.766  1.8832   4.846 0.0159 *\n## Residuals   27 10.492  0.3886                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAn ANOVA only tests, if there is a difference, but not between which groups. To perform pairwise comparisons, you can use post-hoc tests. Common for ANOVA results is\n\nTukeyHSD(x)\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = weight ~ group, data = PlantGrowth)\n## \n## $group\n##             diff        lwr       upr     p adj\n## trt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\n## trt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\n## trt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\nAlternatively, you can also perform several tests each comparing two groups and then correct for multiple testing. This is what we did before.\nPairwise comparisons are often visualized using different letters to significantly different groups:\n\n# install.packages(\"multcomp\")\nlibrary(multcomp)\ntuk = glht(x, linfct = mcp(group = \"Tukey\")) #performs Tukey pairwise comparisons\ntuc.cld = cld(tuk) # assigns different letters to significantly different groups\n\nold.par = par(mai = c(1, 1, 1.25, 1), no.readonly = T)\nplot(tuc.cld) # draws boxplot + letters from cld function\n\n\n\npar(old.par)\n\n\n\n6.1.3.2 Anova, paired\naov is not good in doing repeated = paired ANOVA. For this task, you should use so-called mixed models!\n\n\n6.1.3.3 Kruskal-Wallis\nNon-parametric test for differences in the mean of &gt;2 groups, unpaired\n\nboxplot(Ozone ~ Month, data = airquality)\n\n\n\nkruskal.test(Ozone ~ Month, data = airquality)\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  Ozone by Month\n## Kruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06\n\n\n\n6.1.3.4 Friedmann Test\nNon-parametric test for differences in the mean of &gt;2 groups, paired.\n\nwb &lt;- aggregate(warpbreaks$breaks,\n                by = list(w = warpbreaks$wool,\n                          t = warpbreaks$tension),\n                FUN = mean)\n#wb\nfriedman.test(wb$x, wb$w, wb$t)\n## \n##  Friedman rank sum test\n## \n## data:  wb$x, wb$w and wb$t\n## Friedman chi-squared = 0.33333, df = 1, p-value = 0.5637\n# Alternative: friedman.test(x ~ w | t, data = wb)\n# Note that x is the response, w is the group, and t are the blocks that are paired"
  },
  {
    "objectID": "3B-HypothesisTests.html#comparison-of-variances",
    "href": "3B-HypothesisTests.html#comparison-of-variances",
    "title": "6  Statistical tests",
    "section": "6.2 Comparison of variances",
    "text": "6.2 Comparison of variances\nH0 in variance tests is always that the variances are equal.\n\n6.2.1 F-Test for two normally-distributed samples\n\nx &lt;- rnorm(50, mean = 0, sd = 2)\ny &lt;- rnorm(30, mean = 1, sd = 1)\nvar.test(x, y)                  # Do x and y have the same variance? - Significantly different\n## \n##  F test to compare two variances\n## \n## data:  x and y\n## F = 6.9377, num df = 49, denom df = 29, p-value = 3.998e-07\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 95 percent confidence interval:\n##   3.485685 13.052795\n## sample estimates:\n## ratio of variances \n##           6.937749\n\n\n\n6.2.2 Bartlett test for more than two normally-distributed samples\n\nx &lt;- rnorm(50, mean = 0, sd = 1)\ny &lt;- rnorm(30, mean = 1, sd = 1)\nz &lt;- rnorm(30, mean = 1, sd = 1)\nbartlett.test(list(x, y, z))                # Do x, y and z have the same variance? - Not sigificantly different\n## \n##  Bartlett test of homogeneity of variances\n## \n## data:  list(x, y, z)\n## Bartlett's K-squared = 1.6542, df = 2, p-value = 0.4373"
  },
  {
    "objectID": "3B-HypothesisTests.html#comparison-of-probabilities",
    "href": "3B-HypothesisTests.html#comparison-of-probabilities",
    "title": "6  Statistical tests",
    "section": "6.3 Comparison of probabilities",
    "text": "6.3 Comparison of probabilities\nProportions are typically analyzed assuming the binomial model (k/n with probability p)\n\n6.3.1 Exact Binomial Test\nH0 is that the data are binomially distributed with a fixed probability p.\n\n## Conover (1971), p. 97f.\n## Under (the assumption of) simple Mendelian inheritance, a cross\n##  between plants of two particular genotypes produces progeny 1/4 of\n##  which are \"dwarf\" and 3/4 of which are \"giant\", respectively.\n##  In an experiment to determine if this assumption is reasonable, a\n##  cross results in progeny having 243 dwarf and 682 giant plants.\n##  If \"giant\" is taken as success, the null hypothesis is that p =\n##  3/4 and the alternative that p != 3/4.\nbinom.test(c(682, 243), p = 3/4)\n## \n##  Exact binomial test\n## \n## data:  c(682, 243)\n## number of successes = 682, number of trials = 925, p-value = 0.3825\n## alternative hypothesis: true probability of success is not equal to 0.75\n## 95 percent confidence interval:\n##  0.7076683 0.7654066\n## sample estimates:\n## probability of success \n##              0.7372973\nbinom.test(682, 682 + 243, p = 3/4)   # The same.\n## \n##  Exact binomial test\n## \n## data:  682 and 682 + 243\n## number of successes = 682, number of trials = 925, p-value = 0.3825\n## alternative hypothesis: true probability of success is not equal to 0.75\n## 95 percent confidence interval:\n##  0.7076683 0.7654066\n## sample estimates:\n## probability of success \n##              0.7372973\n## =&gt; Data are in agreement with H0\n\n\n\n6.3.2 Test of Equal or Given Proportions\nbased on Chi-squared-test, H0 is that the data in two groups are binomially distributed with the same probability p.\n\n## Data from Fleiss (1981), p. 139.\n## H0: The null hypothesis is that the four populations from which\n##     the patients were drawn have the same true proportion of smokers.\n## A:  The alternative is that this proportion is different in at\n##     least one of the populations.\nsmokers  &lt;- c( 83, 90, 129, 70 )\npatients &lt;- c( 86, 93, 136, 82 )\nprop.test(smokers, patients)\n## \n##  4-sample test for equality of proportions without continuity correction\n## \n## data:  smokers out of patients\n## X-squared = 12.6, df = 3, p-value = 0.005585\n## alternative hypothesis: two.sided\n## sample estimates:\n##    prop 1    prop 2    prop 3    prop 4 \n## 0.9651163 0.9677419 0.9485294 0.8536585\n##  =&gt; Data are not in agreement with H0\n\n\n\n6.3.3 Contingency tables\nChi-squared-test for count data, H0 is that the joint distribution of the cell counts in a 2-dimensional contingency table is the product of the row and column marginals\n\n## From Agresti(2007) p.39\nM &lt;- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))\ndimnames(M) &lt;- list(gender = c(\"F\", \"M\"),\n                    party = c(\"Democrat\",\"Independent\", \"Republican\"))\nchisq.test(M)\n## \n##  Pearson's Chi-squared test\n## \n## data:  M\n## X-squared = 30.07, df = 2, p-value = 2.954e-07"
  },
  {
    "objectID": "3B-HypothesisTests.html#distribution-tests",
    "href": "3B-HypothesisTests.html#distribution-tests",
    "title": "6  Statistical tests",
    "section": "6.4 Distribution tests",
    "text": "6.4 Distribution tests\nOften we are interested in the distribution of a variable. This can be tested with distribution tests. All these tests are defined as follows: H0 is that the data follow a specific distribution. So in case H0 is rejected, the data significantly deviates from the specified distribution.\nOften, we want to know whether a variable is normally distributed because this is an important assumption for parametric hypothesis tests. But data can follow many other distributions:\n\n\n6.4.1 Shapiro-Wilk Normality Test\nBecause many tests require normal distribution, this is the test needed most often.\n\nshapiro.test(rnorm(100, mean = 5, sd = 3))\n## \n##  Shapiro-Wilk normality test\n## \n## data:  rnorm(100, mean = 5, sd = 3)\n## W = 0.9874, p-value = 0.4649\n\n\n\n6.4.2 Kolmogorov-Smirnov Test\nFor everything else, the KS test can be used. It compares two different distributions, or a distribution against a reference.\n\nx &lt;- rnorm(50)\ny &lt;- runif(30)\n# Do x and y come from the same distribution?\nks.test(x, y)\n## \n##  Exact two-sample Kolmogorov-Smirnov test\n## \n## data:  x and y\n## D = 0.54, p-value = 1.598e-05\n## alternative hypothesis: two-sided\n\n# Does x come from a shifted gamma distribution with shape 3 and rate 2?\nks.test(x+2, \"pgamma\", 3, 2) # two-sided, exact\n## \n##  Exact one-sample Kolmogorov-Smirnov test\n## \n## data:  x + 2\n## D = 0.35969, p-value = 2.674e-06\n## alternative hypothesis: two-sided\nks.test(x+2, \"pgamma\", 3, 2, exact = FALSE)\n## \n##  Asymptotic one-sample Kolmogorov-Smirnov test\n## \n## data:  x + 2\n## D = 0.35969, p-value = 4.81e-06\n## alternative hypothesis: two-sided\nks.test(x+2, \"pgamma\", 3, 2, alternative = \"gr\")\n## \n##  Exact one-sample Kolmogorov-Smirnov test\n## \n## data:  x + 2\n## D^+ = 0.068834, p-value = 0.5957\n## alternative hypothesis: the CDF of x lies above the null hypothesis\n\nFor an overview on distribution see here: http://www.stat.umn.edu/geyer/old/5101/rlook.html"
  },
  {
    "objectID": "3B-HypothesisTests.html#other-tests",
    "href": "3B-HypothesisTests.html#other-tests",
    "title": "6  Statistical tests",
    "section": "6.5 Other tests",
    "text": "6.5 Other tests\n\n6.5.1 Correlation\nA test for the significance of a correlation:\n\ncor.test(airquality$Ozone, airquality$Wind)\n## \n##  Pearson's product-moment correlation\n## \n## data:  airquality$Ozone and airquality$Wind\n## t = -8.0401, df = 114, p-value = 9.272e-13\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.7063918 -0.4708713\n## sample estimates:\n##        cor \n## -0.6015465\n\nInterpretation: Ozone and Wind are significantly negatively correlated with a p-value &lt; 0.05 and a correlation coefficient of -0.6015465.\n\n\n6.5.2 Mantel test\nThe Mantel test compares two distance matrices\n\nlibrary(vegan)\n## Is vegetation related to environment?\ndata(varespec)\ndata(varechem)\nveg.dist &lt;- vegdist(varespec) # Bray-Curtis\nenv.dist &lt;- vegdist(scale(varechem), \"euclid\")\nmantel(veg.dist, env.dist)\n## \n## Mantel statistic based on Pearson's product-moment correlation \n## \n## Call:\n## mantel(xdis = veg.dist, ydis = env.dist) \n## \n## Mantel statistic r: 0.3047 \n##       Significance: 0.002 \n## \n## Upper quantiles of permutations (null model):\n##   90%   95% 97.5%   99% \n## 0.118 0.152 0.178 0.201 \n## Permutation: free\n## Number of permutations: 999\nmantel(veg.dist, env.dist, method=\"spear\")\n## \n## Mantel statistic based on Spearman's rank correlation rho \n## \n## Call:\n## mantel(xdis = veg.dist, ydis = env.dist, method = \"spear\") \n## \n## Mantel statistic r: 0.2838 \n##       Significance: 0.001 \n## \n## Upper quantiles of permutations (null model):\n##   90%   95% 97.5%   99% \n## 0.119 0.156 0.183 0.206 \n## Permutation: free\n## Number of permutations: 999"
  },
  {
    "objectID": "5-MultipleRegression.html#confounder",
    "href": "5-MultipleRegression.html#confounder",
    "title": "8  Multiple regression",
    "section": "8.1 Confounder",
    "text": "8.1 Confounder\nConfounders have effects on the response and another predictor.\n\nClimate = runif(100)\nTemp = Climate + rnorm(100, sd = 0.2)\nGrowth = 0.5*Temp - 1.0*Climate + rnorm(100, sd = 0.2)\n\nsummary(lm(Growth~Temp))\n## \n## Call:\n## lm(formula = Growth ~ Temp)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.55719 -0.18748 -0.01354  0.18858  0.59337 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.16604    0.04228  -3.927  0.00016 ***\n## Temp        -0.19311    0.06602  -2.925  0.00428 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2472 on 98 degrees of freedom\n## Multiple R-squared:  0.0803, Adjusted R-squared:  0.07091 \n## F-statistic: 8.556 on 1 and 98 DF,  p-value: 0.004279\nsummary(lm(Growth~Temp+Climate)) # correct effects!!\n## \n## Call:\n## lm(formula = Growth ~ Temp + Climate)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.41912 -0.13228 -0.00661  0.12988  0.41630 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.009234   0.038203   0.242     0.81    \n## Temp         0.568083   0.102652   5.534 2.66e-07 ***\n## Climate     -1.088041   0.127964  -8.503 2.27e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1881 on 97 degrees of freedom\n## Multiple R-squared:  0.473,  Adjusted R-squared:  0.4622 \n## F-statistic: 43.54 on 2 and 97 DF,  p-value: 3.205e-14\n\nIdentifying confounders is the most important challenge in observational studies: For example, several studies showed that overweight adults have lower mortality. However, another study shows that these earlier results might have come up due to confounding: smoking!\n\nsmokers: higher mortality and lower BMI -&gt; people with lower BMI have higher mortality rates\nWhen we correct for the confounder smoking, the correlation between BMI and mortality goes in the other direction, i.e. obese people have higher mortality!\n\nConfounders can even lead to observed correlations where in reality there is no such correlation. This is called spurious correlation.\n\n\n\n\n\n\nWarning\n\n\n\nConclusion: Confounders can cause correlations where no causal relationship exists."
  },
  {
    "objectID": "5-MultipleRegression.html#multiple-lm",
    "href": "5-MultipleRegression.html#multiple-lm",
    "title": "8  Multiple regression",
    "section": "8.2 Multiple LM",
    "text": "8.2 Multiple LM\nThe multiple linear regression can deal with confounders:\n\nUnivariate (simple) linear regression describes how y depends on x using a polynomial of x1 e.g.: \\[\ny = a_0 + a_1*x_1 + a_2*x_1^2\n\\]\nMultiple linear regression expands simple linear regression to a polynomial of several explanatory variables x1, x2… e.g.: \\[\ny = a_0 + a_1*x_1 + a_2*x_2 + a_3*x_3\n\\]\nIdea: if we jointly consider “all” variables in the model formula, the influence of confounding variables is incorporated\n\n\n## first remove observations with NA values\nnewAirquality = airquality[complete.cases(airquality),]\nsummary(newAirquality)\n##      Ozone          Solar.R           Wind            Temp      \n##  Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n##  1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n##  Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n##  Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n##  3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n##  Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n##      Month            Day       \n##  Min.   :5.000   Min.   : 1.00  \n##  1st Qu.:6.000   1st Qu.: 9.00  \n##  Median :7.000   Median :16.00  \n##  Mean   :7.216   Mean   :15.95  \n##  3rd Qu.:9.000   3rd Qu.:22.50  \n##  Max.   :9.000   Max.   :31.00\n\n# simple regression\nm0 = lm(Ozone ~ Temp , data = newAirquality)\nsummary(m0)\n## \n## Call:\n## lm(formula = Ozone ~ Temp, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -40.922 -17.459  -0.874  10.444 118.078 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -147.6461    18.7553  -7.872 2.76e-12 ***\n## Temp           2.4391     0.2393  10.192  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 23.92 on 109 degrees of freedom\n## Multiple R-squared:  0.488,  Adjusted R-squared:  0.4833 \n## F-statistic: 103.9 on 1 and 109 DF,  p-value: &lt; 2.2e-16\nplot(m0)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(Ozone ~ Temp , data = newAirquality)\nabline(m0, col = \"blue\", lwd = 3)\n\n\n\n\n# Today: multiple linear regression\nm1 = lm(Ozone ~ Temp + Wind , data = newAirquality)\n# have a look at the residuals:\nop &lt;- par(mfrow = c(2,2))\nplot(m1)\n\n\n\npar(op)\n\nsummary(m1)\n## \n## Call:\n## lm(formula = Ozone ~ Temp + Wind, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -42.156 -13.216  -3.123  10.598  98.492 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -67.3220    23.6210  -2.850  0.00524 ** \n## Temp          1.8276     0.2506   7.294 5.29e-11 ***\n## Wind         -3.2948     0.6711  -4.909 3.26e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 21.73 on 108 degrees of freedom\n## Multiple R-squared:  0.5814, Adjusted R-squared:  0.5736 \n## F-statistic: 74.99 on 2 and 108 DF,  p-value: &lt; 2.2e-16\n\n# plotting multiple regression outputs\nlibrary(effects)\n## Loading required package: carData\n## lattice theme set by effectsTheme()\n## See ?effectsTheme for details.\nplot(allEffects(m1))\n\n\n\n\n\n## Omitted variable bias\nboth = lm(Ozone ~ Wind + Temp, newAirquality)\nwind = lm(Ozone ~ Wind , newAirquality)\ntemp = lm(Ozone ~ Temp, newAirquality)\nsummary(both)\n## \n## Call:\n## lm(formula = Ozone ~ Wind + Temp, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -42.156 -13.216  -3.123  10.598  98.492 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -67.3220    23.6210  -2.850  0.00524 ** \n## Wind         -3.2948     0.6711  -4.909 3.26e-06 ***\n## Temp          1.8276     0.2506   7.294 5.29e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 21.73 on 108 degrees of freedom\n## Multiple R-squared:  0.5814, Adjusted R-squared:  0.5736 \n## F-statistic: 74.99 on 2 and 108 DF,  p-value: &lt; 2.2e-16\nsummary(wind)\n## \n## Call:\n## lm(formula = Ozone ~ Wind, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -43.513 -18.597  -5.035  15.814  88.437 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  99.0413     7.4724   13.25  &lt; 2e-16 ***\n## Wind         -5.7288     0.7082   -8.09 9.09e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 26.42 on 109 degrees of freedom\n## Multiple R-squared:  0.3752, Adjusted R-squared:  0.3694 \n## F-statistic: 65.44 on 1 and 109 DF,  p-value: 9.089e-13\n\nslopes &lt;- data.frame(\n  predictor = c(\"Wind\", \"Temp\"),\n  both.pred = round(coef(both)[2:3], digits = 2),\n  only.wind = c(round(coef(wind)[2], digits = 2), \"NA\"),\n  only.temp = c(\"NA\", round(coef(temp)[2], digits = 2))\n)\nslopes\n##      predictor both.pred only.wind only.temp\n## Wind      Wind     -3.29     -5.73        NA\n## Temp      Temp      1.83        NA      2.44\n\nOmitting Wind makes the effect of Temperature larger.\nProblem: Multiple regression can separate the effect of collinear explanatory variables, but only, if collinearity is not too strong.\nSolution: If the correlation is really strong, we can omit one variable and interpret the remaining collinear variable as representing both."
  },
  {
    "objectID": "5-MultipleRegression.html#interactions-between-variables",
    "href": "5-MultipleRegression.html#interactions-between-variables",
    "title": "8  Multiple regression",
    "section": "8.3 Interactions between variables",
    "text": "8.3 Interactions between variables\nIf one predictor influences the effect of the other predictor, we can include an interaction term into our model:\n\\[\ny \\sim a + b + a:b\n\\]\nor:\n\\[\ny \\sim a*b\n\\]\n\n# Include interaction\nm2 = lm(Ozone ~  scale(Wind)* scale(Temp) , data = newAirquality)\n# if including interactions, always scale your predictor variables!\n# scale: subtracts the mean and divides by standard deviation\nsummary(m2)\n## \n## Call:\n## lm(formula = Ozone ~ scale(Wind) * scale(Temp), data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -40.930 -11.193  -3.034   8.193  97.456 \n## \n## Coefficients:\n##                         Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)               38.469      2.137  18.002  &lt; 2e-16 ***\n## scale(Wind)              -11.758      2.238  -5.253 7.68e-07 ***\n## scale(Temp)               17.544      2.239   7.837 3.62e-12 ***\n## scale(Wind):scale(Temp)   -7.367      1.848  -3.987 0.000123 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 20.37 on 107 degrees of freedom\n## Multiple R-squared:  0.6355, Adjusted R-squared:  0.6253 \n## F-statistic: 62.19 on 3 and 107 DF,  p-value: &lt; 2.2e-16\nop &lt;- par(mfrow = c(2,2))\nplot(m2)\n\n\n\npar(op)\n\nThe influence of temperature on growth depends on the amount of precipitation, or: If there’s not enough water, also higher temperatures cannot increase growth.\nExample:\n\n# How does everything change, if we have factorial predictors?\nnewAirquality$MonthFactor = as.factor(newAirquality$Month)\n\nm4 = lm(sqrt(Ozone) ~ MonthFactor + scale(Wind) * scale(Temp) * scale(Solar.R) , \n        data = newAirquality)\nsummary(m4)\n## \n## Call:\n## lm(formula = sqrt(Ozone) ~ MonthFactor + scale(Wind) * scale(Temp) * \n##     scale(Solar.R), data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.6096 -0.8869 -0.2067  0.7647  4.3191 \n## \n## Coefficients:\n##                                        Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                             6.12172    0.37148  16.479  &lt; 2e-16 ***\n## MonthFactor6                           -0.54487    0.60633  -0.899 0.371025    \n## MonthFactor7                           -0.37571    0.51347  -0.732 0.466072    \n## MonthFactor8                           -0.03770    0.52839  -0.071 0.943262    \n## MonthFactor9                           -0.74343    0.43308  -1.717 0.089179 .  \n## scale(Wind)                            -0.76983    0.16456  -4.678 9.18e-06 ***\n## scale(Temp)                             1.35350    0.20937   6.465 3.86e-09 ***\n## scale(Solar.R)                          0.65689    0.16212   4.052 0.000101 ***\n## scale(Wind):scale(Temp)                -0.30440    0.14655  -2.077 0.040379 *  \n## scale(Wind):scale(Solar.R)             -0.07695    0.17222  -0.447 0.655999    \n## scale(Temp):scale(Solar.R)              0.22985    0.15451   1.488 0.140040    \n## scale(Wind):scale(Temp):scale(Solar.R)  0.03202    0.15179   0.211 0.833366    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.328 on 99 degrees of freedom\n## Multiple R-squared:  0.7335, Adjusted R-squared:  0.7039 \n## F-statistic: 24.78 on 11 and 99 DF,  p-value: &lt; 2.2e-16\n\nm5 = lm(sqrt(Ozone) ~ MonthFactor + scale(Wind) + scale(Temp) + scale(Solar.R) \n                      + scale(Wind):scale(Temp)\n                      + scale(Wind):scale(Solar.R)\n                      + scale(Temp):scale(Solar.R), \n        data = newAirquality)\nsummary(m5)\n## \n## Call:\n## lm(formula = sqrt(Ozone) ~ MonthFactor + scale(Wind) + scale(Temp) + \n##     scale(Solar.R) + scale(Wind):scale(Temp) + scale(Wind):scale(Solar.R) + \n##     scale(Temp):scale(Solar.R), data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.6023 -0.9182 -0.2180  0.7713  4.3209 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                 6.12350    0.36960  16.568  &lt; 2e-16 ***\n## MonthFactor6               -0.54871    0.60315  -0.910   0.3652    \n## MonthFactor7               -0.39194    0.50524  -0.776   0.4397    \n## MonthFactor8               -0.04701    0.52402  -0.090   0.9287    \n## MonthFactor9               -0.74873    0.43028  -1.740   0.0849 .  \n## scale(Wind)                -0.75588    0.14997  -5.040 2.07e-06 ***\n## scale(Temp)                 1.35192    0.20823   6.492 3.29e-09 ***\n## scale(Solar.R)              0.65178    0.15953   4.086 8.88e-05 ***\n## scale(Wind):scale(Temp)    -0.31305    0.14002  -2.236   0.0276 *  \n## scale(Wind):scale(Solar.R) -0.09259    0.15469  -0.599   0.5508    \n## scale(Temp):scale(Solar.R)  0.23573    0.15126   1.558   0.1223    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.321 on 100 degrees of freedom\n## Multiple R-squared:  0.7334, Adjusted R-squared:  0.7068 \n## F-statistic: 27.51 on 10 and 100 DF,  p-value: &lt; 2.2e-16\n\n# short form for including only two-way interactions:\n\nm5 = lm(sqrt(Ozone) ~ MonthFactor + (scale(Wind) + scale(Temp) + scale(Solar.R))^2,\n        data = newAirquality)\nsummary(m5)\n## \n## Call:\n## lm(formula = sqrt(Ozone) ~ MonthFactor + (scale(Wind) + scale(Temp) + \n##     scale(Solar.R))^2, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.6023 -0.9182 -0.2180  0.7713  4.3209 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                 6.12350    0.36960  16.568  &lt; 2e-16 ***\n## MonthFactor6               -0.54871    0.60315  -0.910   0.3652    \n## MonthFactor7               -0.39194    0.50524  -0.776   0.4397    \n## MonthFactor8               -0.04701    0.52402  -0.090   0.9287    \n## MonthFactor9               -0.74873    0.43028  -1.740   0.0849 .  \n## scale(Wind)                -0.75588    0.14997  -5.040 2.07e-06 ***\n## scale(Temp)                 1.35192    0.20823   6.492 3.29e-09 ***\n## scale(Solar.R)              0.65178    0.15953   4.086 8.88e-05 ***\n## scale(Wind):scale(Temp)    -0.31305    0.14002  -2.236   0.0276 *  \n## scale(Wind):scale(Solar.R) -0.09259    0.15469  -0.599   0.5508    \n## scale(Temp):scale(Solar.R)  0.23573    0.15126   1.558   0.1223    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.321 on 100 degrees of freedom\n## Multiple R-squared:  0.7334, Adjusted R-squared:  0.7068 \n## F-statistic: 27.51 on 10 and 100 DF,  p-value: &lt; 2.2e-16\n# get overall effect of Month:\nanova(m5)\n## Analysis of Variance Table\n## \n## Response: sqrt(Ozone)\n##                             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \n## MonthFactor                  4 158.726  39.681 22.7249 2.261e-13 ***\n## scale(Wind)                  1 149.523 149.523 85.6296 4.282e-15 ***\n## scale(Temp)                  1 126.124 126.124 72.2290 1.899e-13 ***\n## scale(Solar.R)               1  19.376  19.376 11.0961 0.0012129 ** \n## scale(Wind):scale(Temp)      1  20.639  20.639 11.8198 0.0008556 ***\n## scale(Wind):scale(Solar.R)   1   1.803   1.803  1.0328 0.3119518    \n## scale(Temp):scale(Solar.R)   1   4.241   4.241  2.4288 0.1222856    \n## Residuals                  100 174.616   1.746                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# this is doing a type I ANOVA = sequential\n# order in which you include the predictors changes the estimates and p-values\n\n# If you want to do a type II ANOVA, use ANova() from the car package\nlibrary(car)\nAnova(m5) # Anova with capital A\n## Anova Table (Type II tests)\n## \n## Response: sqrt(Ozone)\n##                             Sum Sq  Df F value    Pr(&gt;F)    \n## MonthFactor                  9.557   4  1.3683 0.2503349    \n## scale(Wind)                 41.993   1 24.0488 3.641e-06 ***\n## scale(Temp)                 78.938   1 45.2067 1.112e-09 ***\n## scale(Solar.R)              23.189   1 13.2797 0.0004276 ***\n## scale(Wind):scale(Temp)      8.728   1  4.9983 0.0275955 *  \n## scale(Wind):scale(Solar.R)   0.626   1  0.3582 0.5508395    \n## scale(Temp):scale(Solar.R)   4.241   1  2.4288 0.1222856    \n## Residuals                  174.616 100                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#type II ANOVA: all other predictors have already been taken into account\n# Does an additional predictor explain some of the variance on top of that?"
  },
  {
    "objectID": "5-MultipleRegression.html#model-selection",
    "href": "5-MultipleRegression.html#model-selection",
    "title": "8  Multiple regression",
    "section": "8.4 Model selection",
    "text": "8.4 Model selection\nWe’ve learned that we should include variables in the model that are collinear, that is they correlate with other predictors, but how many and which factors should we include?\nFamous example: Female hurricanes are deadlier than male hurricanes (Jung et al., 2014)\nThey have analyzed the number of fatalities of hurricane and claimed that there is an effect of femininity of the name on the number of deads (while correcting for confounders). They recommend to give hurricans only male names because it would considerably reduce the number of deads.\n\nlibrary(DHARMa)\n## This is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\nlibrary(effects)\n?hurricanes\nstr(hurricanes)\n## Classes 'tbl_df', 'tbl' and 'data.frame':    92 obs. of  14 variables:\n##  $ Year                    : num  1950 1950 1952 1953 1953 ...\n##  $ Name                    : chr  \"Easy\" \"King\" \"Able\" \"Barbara\" ...\n##  $ MasFem                  : num  6.78 1.39 3.83 9.83 8.33 ...\n##  $ MinPressure_before      : num  958 955 985 987 985 960 954 938 962 987 ...\n##  $ Minpressure_Updated_2014: num  960 955 985 987 985 960 954 938 962 987 ...\n##  $ Gender_MF               : num  1 0 0 1 1 1 1 1 1 1 ...\n##  $ Category                : num  3 3 1 1 1 3 3 4 3 1 ...\n##  $ alldeaths               : num  2 4 3 1 0 60 20 20 0 200 ...\n##  $ NDAM                    : num  1590 5350 150 58 15 ...\n##  $ Elapsed_Yrs             : num  63 63 61 60 60 59 59 59 58 58 ...\n##  $ Source                  : chr  \"MWR\" \"MWR\" \"MWR\" \"MWR\" ...\n##  $ ZMasFem                 : num  -0.000935 -1.670758 -0.913313 0.945871 0.481075 ...\n##  $ ZMinPressure_A          : num  -0.356 -0.511 1.038 1.141 1.038 ...\n##  $ ZNDAM                   : num  -0.439 -0.148 -0.55 -0.558 -0.561 ...\n\nlibrary(glmmTMB)\n## Warning in checkMatrixPackageVersion(): Package version inconsistency detected.\n## TMB was built with Matrix version 1.5.4\n## Current Matrix version is 1.5.4.1\n## Please re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n## Warning in checkDepPackageVersion(dep_pkg = \"TMB\"): Package version inconsistency detected.\n## glmmTMB was built with TMB version 1.9.6\n## Current TMB version is 1.9.4\n## Please re-install glmmTMB from source or restore original 'TMB' package (see '?reinstalling' for more information)\n\nm1 = glmmTMB(alldeaths ~ MasFem*\n                             (Minpressure_Updated_2014 + scale(NDAM)),\n                           data = hurricanes, family = nbinom2)\nsummary(m1)\n##  Family: nbinom2  ( log )\n## Formula:          alldeaths ~ MasFem * (Minpressure_Updated_2014 + scale(NDAM))\n## Data: hurricanes\n## \n##      AIC      BIC   logLik deviance df.resid \n##    660.7    678.4   -323.4    646.7       85 \n## \n## \n## Dispersion parameter for nbinom2 family (): 0.787 \n## \n## Conditional model:\n##                                  Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)                     69.661590  23.425598   2.974 0.002942 ** \n## MasFem                          -5.855078   2.716589  -2.155 0.031138 *  \n## Minpressure_Updated_2014        -0.069870   0.024251  -2.881 0.003964 ** \n## scale(NDAM)                     -0.494094   0.455968  -1.084 0.278536    \n## MasFem:Minpressure_Updated_2014  0.006108   0.002813   2.171 0.029901 *  \n## MasFem:scale(NDAM)               0.205418   0.061956   3.316 0.000915 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nInteractions -&gt; we need to scale variables:\n\nm2 = glmmTMB(alldeaths ~ scale(MasFem)*\n                             (scale(Minpressure_Updated_2014) + scale(NDAM)+scale(sqrt(NDAM))),\n                           data = hurricanes, family = nbinom2)\nsummary(m2)\n##  Family: nbinom2  ( log )\n## Formula:          \n## alldeaths ~ scale(MasFem) * (scale(Minpressure_Updated_2014) +  \n##     scale(NDAM) + scale(sqrt(NDAM)))\n## Data: hurricanes\n## \n##      AIC      BIC   logLik deviance df.resid \n##    634.9    657.6   -308.4    616.9       83 \n## \n## \n## Dispersion parameter for nbinom2 family (): 1.12 \n## \n## Conditional model:\n##                                               Estimate Std. Error z value\n## (Intercept)                                    2.28082    0.10850  21.022\n## scale(MasFem)                                  0.05608    0.10672   0.525\n## scale(Minpressure_Updated_2014)               -0.14267    0.17804  -0.801\n## scale(NDAM)                                   -1.11104    0.28030  -3.964\n## scale(sqrt(NDAM))                              2.10764    0.36487   5.776\n## scale(MasFem):scale(Minpressure_Updated_2014)  0.07371    0.19618   0.376\n## scale(MasFem):scale(NDAM)                     -0.10159    0.27080  -0.375\n## scale(MasFem):scale(sqrt(NDAM))                0.32960    0.36594   0.901\n##                                               Pr(&gt;|z|)    \n## (Intercept)                                    &lt; 2e-16 ***\n## scale(MasFem)                                    0.599    \n## scale(Minpressure_Updated_2014)                  0.423    \n## scale(NDAM)                                   7.38e-05 ***\n## scale(sqrt(NDAM))                             7.63e-09 ***\n## scale(MasFem):scale(Minpressure_Updated_2014)    0.707    \n## scale(MasFem):scale(NDAM)                        0.708    \n## scale(MasFem):scale(sqrt(NDAM))                  0.368    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe effect of femininity is gone! Already with the scaled variables, but also with the transformation with the NDAM variable. The question was raised which of both is more reasonable, whether the relationship between damage and mortality isn’t a straight line or that the gender of the hurricane names affect deaths (Bob O’Hara and GrrlScientist). They argue that the model with the transformed variable fits the data better which brings us to the topic of this section, how to choose between different models? Answering this question if the goal of model selection.\nWhy not include all the variables we can measure in our model? Problem with the full model:\n\nIf you have more parameters than data points, the model cannot be fitted at all\nEven with n (samples) ~ k (number of parameters), model properties become very unfavorable (high p-values and uncertainties/standard errors) –&gt; Overfitting\n\nA “good model” depends on the goal of the analysis, do we want to optimize:\n\nPredictive ability – how well can we predict with the model?\nInferential ability – do we identify the true values for the parameters (true effects), are the p-values correct, can we correctly say that a variable has an effect?\n\nThe more complex a model gets, the better it fits to the data, but there’s a downside, the bias-variance tradeoff.\nExplanation bias-variance tradeoff\nExplanation LRT and AIC\nProblem of p-hacking\nExample:\n\n# Compare different competing models:\n# let's compare models m3 and m5 to decide which one explains our data better:\n# 1. LRT\nanova(m3, m5)\n# RSS = residual sum of squares = variance not explained by the model\n# smaller RSS = better model\n# p-value\n\n#2. AIC\nAIC(m3)\nAIC(m5)\n# also here, model m5 is better\n\n\n#### Demonstration: Why interpretation of effect sizes and p-values \n### after extensive model selection is not a good idea:\nlibrary(MASS)\nset.seed(1)\n#make up predictors:\ndat = data.frame(matrix(runif(20000), ncol = 100))\n# create a response variable\ndat$y = rnorm(200)\nfullModel = lm(y ~ ., data = dat)\nsum &lt;- summary(fullModel)\nmean(sum$coefficients[,4] &lt; 0.05)\n# 0.019: less than 2 % false positives = type I error rate\n\nselection = stepAIC(fullModel)\nsum.sel &lt;- summary(selection)\nmean(sum.sel$coefficients[,4] &lt; 0.05)\n# 0.48: Now almost 50 % of our results are false positives!!!"
  },
  {
    "objectID": "5-MultipleRegression.html#formula-syntax",
    "href": "5-MultipleRegression.html#formula-syntax",
    "title": "8  Multiple regression",
    "section": "8.5 Formula syntax",
    "text": "8.5 Formula syntax\n\nFormula syntax\n\n\n\n\n\n\n\nFormula\nMeaning\nDetails\n\n\n\n\ny~x_1\n\\(y=a_0 +a_1*x_1\\)\nSlope+Intercept\n\n\ny~x_1 - 1\n\\(y=a_1*x_1\\)\nSlope, no intercept\n\n\ny~I(x_1^2)\n\\(y=a_0 + a_1*(x_1^2)\\)\nQuadratic effect\n\n\ny~x_1+x_2\n\\(y=a_0+a_1*x_1+a_2*x_2\\)\nMultiple linear regression (two variables)\n\n\ny~x_1:x_2\n\\(y=a_0+a_1*(x_1*x_2)\\)\nInteraction between x1 and x2\n\n\ny~x_1*x_2\n\\(y=a_0+a_1*(x_1*x_2)+a_2*x_1+a_3*x_2\\)\nInteraction and main effects"
  }
]