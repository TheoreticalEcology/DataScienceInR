[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science with R",
    "section": "",
    "text": "Preface\nMaterial for a first 1-week course in Data Science, taught at the University of Regensburg.\nDay 1: Getting started, descriptive statistics\nDay2: Hypothesis tests\nDay3: Linear regression (simple, multivariate)\nDay4: Generalised linear models (GLM), Multivariate statistics\nDay5: Machine Learning, Data organisation"
  },
  {
    "objectID": "1-GettingStarted.html#your-r-system",
    "href": "1-GettingStarted.html#your-r-system",
    "title": "1  Getting Started with R",
    "section": "1.1 Your R System",
    "text": "1.1 Your R System\nIn this course, we work with the combination of R + RStudio.\n\nR is the calculation engine that performs the computations.\nRStudio is the editor that helps you sending inputs to R and collect outputs.\n\nMake sure you have a recent version of R + RStudio installed on your computer. If you have never used RStudio, here is a good video introducing the basic system and how R and RStudio interact."
  },
  {
    "objectID": "1-GettingStarted.html#libraries-that-you-will-need",
    "href": "1-GettingStarted.html#libraries-that-you-will-need",
    "title": "1  Getting Started with R",
    "section": "1.2 Libraries that you will need",
    "text": "1.2 Libraries that you will need\nThe R engine comes with a number of base functions, but one of the great things about R is that you can extend these base functions by libraries that can be programmed by anyone. In principle, you can install libraries from any website or file. In practice, however, most commonly used libraries are distributed via two major repositories. For statistical methods, this is CRAN, and for bioinformatics, this is Bioconductor.\n\n\n\n\n\n\nClick to see more on installing libraries in R\n\n\n\n\n\nTo install a package from a library, use the command\n\ninstall.packages(LIBRARY)\n\nExchange “LIBRARY” with the name of the library you want to install. The default is to search the package in CRAN, but you can specify other repositories or file locations in the function. For Windows / Mac, R should work out of the box. For other UNIX based systems, may also need to install\nbuild-essential\ngfortran\nlibmagick++-dev\nr-base-dev\ncmake\nIf you are new to installing packages on Debian / Ubuntu, etc., type the following:\nsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev cmake\n\n\n\nIn this book, we will often use data sets from the EcoData package, which is not on CRAN, but on a GitHub page. To install the package, if you don’t have the devtools package installed already, first install devtools from CRAN by running\n\ninstall.packages(\"devtools\")\n\nThen install the EcoData package via\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\",\n                         dependencies = T, build_vignettes = T)\n\nFor your convenience, the EcoData installation also forces the installation of most of the packages needed in this book, so this may take a while. If you want to load only the EcoData package, or if you encounter problems during the install, set dependencies = F, build_vignettes = F."
  },
  {
    "objectID": "1-GettingStarted.html#data",
    "href": "1-GettingStarted.html#data",
    "title": "1  Getting Started with R",
    "section": "1.3 Data",
    "text": "1.3 Data\nData typically consist of a table with several observations with one or more variables:\n\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\n\nOne row per observation is the standard format for most programs. There are different types of variables (with respect to the analysis):\n\nDependent / response variable variable of interest we want to know what it is influenced by. Typically, there is one variable of particular interest.\nExplanatory or independent variables, aka. predictors / covariates / treatments variables that potentially influence the response variable\n\n\n\n\n\n\n\nTip\n\n\n\nExample: We measure plant growth and nitrogen in the soil. Growth is the dependent variable and nitrogen is the explanatory variable\n\n\nVariables differ in range and scale:\n\nScale of measure: nominal (unordered), ordinal (ordered), metric (differences can be interpreted)\nNumeric or metric variables Examples: Body size, conductivity\nNon-numeric = categorial variables\n\nunordered / nominal (red, green, blue) ordered /\nordinal (tiny, small, large) Attention: categories small = 1m, medium = 1.5m, large = 2m would be interpreted as numeric, because their relative differences are defined\nSpecial case: binary variable (0,1), technically it is also categorial, but with two levels only we call in binary\n\n\n\n\n\n\n\n\nRemarks on data handling\n\n\n\n\n\nTypically, data will be recorded electronically with a measurement device, or you have to enter it manually using a spreadsheet program, e.g. MS Excel. The best format for data storage is csv (comma separated values) because it is long-term compatibility with all kinds of programs / systems (Excel can export to csv).\nAfter raw data is entered, it should never be manipulated by hand! If you modify data by hand, make a copy and document all changes (additional text file). Better: Make changes using a script\nData handling in R:\n\ncreate R script “dataprep.R” or similar and import dataset\npossibly combine different datasets\nclean data (remove NAs, impossible values etc.)\nsave as Rdata (derived data)"
  },
  {
    "objectID": "1-GettingStarted.html#data-manipulation-in-r",
    "href": "1-GettingStarted.html#data-manipulation-in-r",
    "title": "1  Getting Started with R",
    "section": "1.4 Data manipulation in R",
    "text": "1.4 Data manipulation in R\nR works like a calculator:\n\n2+2\n## [1] 4\n5*4\n## [1] 20\n2^2\n## [1] 4\n\nWe can also use functions that perform specific calculations:\n\nsqrt(4)\n## [1] 2\nsum(c(2,2))\n## [1] 4\n\nWe can assign values/data to variables:\n\nobject.name <- 1\n\nNote that both operators ‘<-’ or “=” work. Functions in R (e.g. sum(), mean(), etc.) have arguments that control/change their behavior and are also used to pass the data to the function:\n\nmean(x = c(2, 2))\n## [1] 2\n\n\n\n\n\n\n\nHelp\n\n\n\nA list and description of all arguments can be found in the help of a function (which can be accessed via ?mean or if you place the cursor on the function and press F1)\n\n\n\n1.4.1 Data types and data structures\nThere are four important data types in R (there are more but we focus on these 5):\n\nNumeric: 1, 2, 3, 4\nLogical: TRUE or FALSE\nCharacters: “A”, “B”,…\nFactors which are characters but we have to tell R explicitly that they are factors\nNot a number: NA, NaN (empty value)\n\nBased on the data types we can build data structures which contain either only specific data types or a mixture of data types:\n\nVector: Several values of one data type, can be created with the c function:\n\nc(5, 3, 5, 6) # numeric vector\nc(TRUE, TRUE, FALSE, TRUE) # logical vector\nc(\"A\", \"B\", \"C\") # character vector\nas.factor(c(\"A\", \"B\", \"C\")) # factor vector\n\nMatrix: two-dimensional data structure of one data type, can be created with the matrix function (we can pass a vector to the matrix function and tell it via arguments how the matrix should be constructed):\n\nmatrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2)\n##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n\nData.frame: Often our data has variables of different types which makes a matrix unsuitable data structure. Data.frames can handle different data types and is organized in columns (one column = one variables) and can be created with the data.frame function:\n\ndata.frame(A = c(1, 2, 3), B = c(\"A\", \"B\", \"C\"), C = c(TRUE, FALSE, FALSE))\n##   A B     C\n## 1 1 A  TRUE\n## 2 2 B FALSE\n## 3 3 C FALSE\n\n\n\n\n1.4.2 Data manipulation\nA vector is a one dimensional data structure and we can access the values by using [ ]:\n\nvec = c(1, 2, 3, 4, 5)\nvec[1] # access first element\n## [1] 1\nvec[5] # access last element\n## [1] 5\n\nA data.frame is a two dimensional data structure. Let’s define a data.frame from two vectors:\n\ndf = data.frame(\n  x = c(2,2,2,3,2,2,1), #add column named x with 2 elements\n  y = c(4,5,5,4,5,3,5) #add a second column named y\n)\n#Let's see how this looks like:\ndf\n##   x y\n## 1 2 4\n## 2 2 5\n## 3 2 5\n## 4 3 4\n## 5 2 5\n## 6 2 3\n## 7 1 5\n\nAccess parts of the data.frame:\n\ndf[1,2] #get element in row 1, column 1\n## [1] 4\ndf[7,1] #get element in row 7, column 1\n## [1] 1\ndf[2,] #get row 2\n##   x y\n## 2 2 5\ndf[,2] #get column 2\n## [1] 4 5 5 4 5 3 5\n#or use the $ sign to access columns:\ndf$y\n## [1] 4 5 5 4 5 3 5\ndf[2:4,1:2] #get rows 2 to 4 and only columns 1 and 2\n##   x y\n## 2 2 5\n## 3 2 5\n## 4 3 4\n\nWe can also set filters:\n\ndf[df$x > 2,] # show only data where x is larger than 2\n##   x y\n## 4 3 4\ndf[df$y == 5,] #show only data where y equals 5\n##   x y\n## 2 2 5\n## 3 2 5\n## 5 2 5\n## 7 1 5\ndf[df$y == 5 & df$x == 1,] #show only data where y equals 5 AND x equals 1\n##   x y\n## 7 1 5\ndf[df$y == 5 | df$x == 3,] #show data where y equals 5 OR x equals 3\n##   x y\n## 2 2 5\n## 3 2 5\n## 4 3 4\n## 5 2 5\n## 7 1 5\n\n\n\n\n\n\n\nLogical operators\n\n\n\n\n\n\nLogical operators in R\n\n\nOperators\nMeaning\n\n\n\n\n<\nLess than\n\n\n<=\nLess than or equal to\n\n\n>\nMore than\n\n\n>=\nMore than or equal to\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n!a\nNot a\n\n\na|b\na or b\n\n\na & b\na and b\n\n\nisTRUE(a)\nTest if a is true\n\n\n\n\n\n\nAdd an additional column with NA values:\n\ndf$NAs = NA #fills up a new column named NAs with all NA values\ndf\n##   x y NAs\n## 1 2 4  NA\n## 2 2 5  NA\n## 3 2 5  NA\n## 4 3 4  NA\n## 5 2 5  NA\n## 6 2 3  NA\n## 7 1 5  NA\n\n\n\n1.4.3 Data analysis workflow\nThis is a simple version of what you’re going to learn during this course:\n\nLet’s say we measured the size of individuals in two different treatment groups\n\ngroup1 = c(2,2,2,3,2,2,1.1)\ngroup2 = c(4,5,5,4,5,3,5.1) \n\nclass(group2)\n## [1] \"numeric\"\n\nDescriptive statistics and visualization\n\nmean(group1)\n## [1] 2.014286\nmean(group2)\n## [1] 4.442857\n\nboxplot(group1, group2)\n\n\n\n\nTesting for differences. Question: Is there a difference between group1 and group2?\n\nt.test(group1, group2)\n## \n##  Welch Two Sample t-test\n## \n## data:  group1 and group2\n## t = -6.6239, df = 10.628, p-value = 4.413e-05\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.238992 -1.618151\n## sample estimates:\n## mean of x mean of y \n##  2.014286  4.442857\n\nInterpretation of the results. Individuals in Group 2 were larger than those in group 1 (t test, t = -6.62, p < 0.0001)\n\nIn the course we will work a lot with datasets implemented in R or in R packages which can be accessed via their name:\n\ndat = airquality\nhead(dat)\n##   Ozone Solar.R Wind Temp Month Day\n## 1    41     190  7.4   67     5   1\n## 2    36     118  8.0   72     5   2\n## 3    12     149 12.6   74     5   3\n## 4    18     313 11.5   62     5   4\n## 5    NA      NA 14.3   56     5   5\n## 6    28      NA 14.9   66     5   6"
  },
  {
    "objectID": "1-GettingStarted.html#exercises",
    "href": "1-GettingStarted.html#exercises",
    "title": "1  Getting Started with R",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\nIn this exercise you will practice:\n\nto set up your working environment (project) in RStudio\nto write R scripts and execute code\nto access data in dataframes (the most important data class in R)\nto query (filter) dataframes\nto spot typical mistakes in R code\n\nPlease carefully follow the instructions for setting up your working environment and ask other participants or use the forum if you face any problems.\n\n1.5.1 Setting up the working environment in RStudio\nYour first task is to open RStudio and create a new project for the course.\n\nClick the ‘File’ button in the menu, then ‘New Project’ (or the second icon in the bar below the menu “Create a project”).\nClick “New Directory”.\nClick “New Project”.\nType in the name of the directory to store your project, e.g. “IntroStatsR”.\n“Browse” to the folder on your computer where you want to have your project created.\nClick the “Create Project” button.\n\n\n\n\n\n\nFor all exercises during this week, use this project! You can open it via the file system as follows (please try this out now):\n\n(Exit RStudio).\nNavigate to the directory where you created your project.\nDouble click on the “IntroStatsR.Rproj” file in that directory.\n\nYou should now be back to RStudio in your project.\nIn the directory of the R project, generate a folder “scripts” and a folder “data”. You can do this either in the file directory or in RStudio. For the latter:\n\nGo to the “Files” panel in R Studio (bottom right panel).\nClick the icon “New Folder” in the upper left corner.\nEnter the folder name.\nThe new folder is now visible in your project directory.\n\nThe idea is that you will create an R script for each exercise and save all these files in the scripts folder. You can do this as follows:\n\nClick the “File” button in the menu, then “New File” and “R Script” (or the first icon in the bar below the menu and then “R Script” in the dropdown menu).\nClick the “File” button in the menu, then “Save” (or the “Save” icon in the menu).\nNavigate to your scripts folder.\nEnter the file name, e.g. “Exercise_01.R”.\nSave the file.\n\n\n\n1.5.2 A few hints before you can start\nRemember the different ways of running code:\n\nclick the “Run” button in the top right corner of the top left panel (code editor) OR\nhit “Ctrl”+“Enter” (MAC: “Cmd”+“Return”)\n\nRStudio will then run\n\nthe code that is currently marked OR\nthe line of code where the text cursor currently is (simply click into that line)\n\nIf you face any problems with executing the code, check the following:\n\nall brackets closed?\ncapital letters instead of small letters?\ncomma is missing?\nif RStudio shows red attention signs (next to the code line number), take it seriously\ndo you see a “+” (instead of a “>”) in the console? stop executions with “esc” key and then try again.\n\nHave a look at the shortcuts by clicking “Tools” and than “Keybord Shortcuts Help”!!\n\n\n1.5.3 Getting an overview of a dataset\nWe work with the airquality dataset:\n\ndat = airquality\n\nCopy the code into your code editor and execute it.\nBefore working with a dataset, you should always get an overview of it. Helpful functions for this are:\n\nstr()\nView()\nhead() and tail()\n\nTry out these functions and provide answers to the following questions on elearning-extern (“01_Test for Exercise in R”):\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat is the most common atomic class in the airquality dataset?\nHow many rows does the dataset have?\nWhat is the last value in the column “Temp”?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWhat is the most common atomic class in the airquality dataset?\n\ninteger\nfunction str() helps to find this out\n\nHow many rows does the dataset have?\n\n153\nthis is easiest to see when using the function str(dat)\ndim(dat) or nrow(dat) give the same information\n\nWhat is the last value in the column “Temp”?\n\n68\ntail(dat) helps to find this out very fast\n\nTo see all this, run\n\ndat = airquality\nView(dat)\nstr(dat)\nhead(dat)\ntail(dat)\n\n\n\n\n\n\n1.5.4 Accessing rows and columns of a data frame\nYou have seen how you can use squared brackets [ ] and the dollar sign $ to extract parts of your data. Some people find this confusing, so let’s repeat the basic concepts:\n\nsquared brackets are used as follows: data[rowNumber, columnNumber]\nthe dollar sign helps to extract colums with their name (good for readability): data$columnName\nthis syntax can also be used to assign new columns, simply use a new column name and the assign operator: data$newColName <-)\n\nThe following lines of code assess parts of the data frame. Try out what they do and sort the code lines and their meaning on elearning-extern.\nHint: Some of the code lines actually do the same; chose the preferred way in these cases.\n\n\n\n\n\n\nQuestion\n\n\n\nWhich of the following commands\n\ndat[2, ]\ndat[, 2]\ndat[, 1]\ndat$Ozone\nnew = dat[, 3] + dat[, 4]\ndat$new = dat[, 3] + dat[, 4]\ndat$NAs = NA\nNA -> dat$NAs \n\nwill get you\n\nget the second row\nget column Ozone\ngenerate a new column with NA’s\ncalculate the sum of columns 3 and 4 and assign to a new column\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nget second row\n\ndat[2, ] is correct\ndat[, 2] gives the second column\n\nget column Ozone\n\ndat$Ozone is the best option\ndat[, 1] gives the same result, but is much harder to understand later on\n\ngenerate a new column with NA’s\n\ndat$NAs = NA is the best option\nNA -> dat$NAs does the same, but the preferred syntax in R is having the new variable on the left hand side (the arrow should face to the left not right)\n\ncalculate the sum of columns 3 and 4 and assign to a new column\n\ndat$new = dat[, 3] + dat[, 4] is correct\nnew = dat[, 3] + dat[, 4] creates a new object but not a new column in the existing data frame\n\n\n\n\n\n\n1.5.5 Filtering data\nTo use the data, you must also be able to filter it. For example, we may be interested in hot days in July and August only. Hot days are typically defined as days with a temperature equal or > 30°C (or 86°F as in the dataset here). Imagine, your colleague tried to query the data accordingly. She/he also found a mistake in each of the first 4 rows and wants to exclude these, but she/he is very new to R and made a few common errors in the following code:\n\n# Return only rows where the temperature is exactly is 86\ndat[dat$Temp = 86, ]\n\n# Return only rows where the temperature is equal or larger than 86\ndat[dat$Temp >= 86]\n\n# Exclude rows 1 through 4\ndat[-1:4, ]\n\n# Return only rows for the months 7 or 8\ndat[dat$Month == 7 | 8, ]\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you fix his/her mistakes? These hints may help you:\n\nrows or columns can be excluded, if the numbers are given as negative numbers\n== means “equals”\n& means “AND”\n“|” means “OR” (press “AltGr”+“<” to produce |, or “option”+“7” on MacOS)\nexecuting the erroneous code may help you to spot the problem\nrun parts of the code if you don’t understand what the code does\nthe last question is a bit trickier, no problem if you don’t find a solution\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the corrected code:\n\n# Return only rows where the temperature is exactly is 86\ndat[dat$Temp == 86, ]\n\n# Return only rows where the temperature is equal or larger than 86\ndat[dat$Temp >= 86, ]\n\n# Exclude rows 1 through 4\ndat[-(1:4), ]\n\n# Return only rows for the months 7 or 8\ndat[dat$Month == 7 | dat$Month == 8, ]\ndat[dat$Month %in% 7:8, ] # alternative expression\n\n\n\n\n\n\n1.5.6 Last task - check if the EcoData package works for you\nDuring the course, we will use some datasets that we compiled in the \\(EcoData\\) package. You should have installed this package already(see instructions above). To check if this works, run\n\nlibrary(EcoData)\nsoep\n\nIf you see some data being displayed in your console, everything is fine."
  },
  {
    "objectID": "2-DescriptiveStatistics.html#exercises",
    "href": "2-DescriptiveStatistics.html#exercises",
    "title": "2  Plotting and describing data",
    "section": "2.1 Exercises",
    "text": "2.1 Exercises\nIn this exercise you will practice:\n\ncreating and plotting contingency tables for categorical variables (mosaicplot)\nplotting a numerical variable (histogram)\nplotting two numerical variables against each other (scatterplot)\ncalculating correlations\n\nTo perform this exercise use the help of the according functions. Also have a look at the examples at the bottom of the help page. You can obtain help for a specific function in several ways:\n\nselect the function in your script (e.g. double click it or simply place the cursor in the word) and press F1\n“?” + function name (e.g. ?hist) and execute\ngo to the “help” panel (below the environment) and search for the function\n\nLet’s get started!\n\n2.1.1 Contingency tables\nHere, we will have a look at the data set “arthritis” from the “EcoData” package. The study was conducted as a double-blind clinical trial investigating a new treatment against arthritis. The improvement was measured as a categorical variable with the possible outcomes “Improved”, “Some” or “None”.\nInstall and then load the package “EcoData” and have a look at the data using the View() function. This will open a new tab next to your R script. To return to your script, close the new tab or click on your script.\n\nlibrary(EcoData)\n# View(arthritis)\n\n# Get the data\ndat <- arthritis\n\n# Coerce columns ’Improved' and 'Treatment' to (ordered) factors\n# (When the factor is ordered, other functions like table() and barplot() will use this order.\n# Otherwise, the levels will be ordered alphabetically.)\ndat$Improved <- factor(dat$Improved, levels = c(\"None\",\"Some\",\"Marked\"), ordered = TRUE)\ndat$Treatment <- as.factor(dat$Treatment)\nstr(dat)\n## 'data.frame':    84 obs. of  5 variables:\n##  $ ID       : int  57 46 77 17 36 23 75 39 33 55 ...\n##  $ Treatment: Factor w/ 2 levels \"Placebo\",\"Treated\": 2 2 2 2 2 2 2 2 2 2 ...\n##  $ Sex      : chr  \"Male\" \"Male\" \"Male\" \"Male\" ...\n##  $ Age      : int  27 29 30 32 46 58 59 59 63 63 ...\n##  $ Improved : Ord.factor w/ 3 levels \"None\"<\"Some\"<..: 2 1 1 3 3 3 1 3 1 1 ...\n\nAn important function for categorical data is table(). It produces a contingency table counting the occurrences of the different categories of one variable or of each combination of the categories of two (or more) variables.\nWe are curious how many patients actually improved in the study and how this is influenced by the treatment. To show this graphically, we produced two plots (see below) using the following functions:\n\ntable()\nplot() and barplot()\n(str())\n(summary())\n\nYour task is now to reconstruct the two plots shown below by using these functions.\n\n\n\n\n\nHints for plot 1\nWhat kind of plot is shown here? How many variables are shown in the plot? Approach: First, create a new object consisting of the table of the variable of interest. Then use this object for plotting. Changing plot elements: Have a look at the help of the plotting-function to find out how to change the y- and x-axis labels. What do you notice on the y-axis? You can change the limits of the y-axis using “ylim = c(from, to)” as an argument in the plotting function.\n\n\n\n\n\nHints for plot 2\nWhat kind of plot can you see here? How many variables does it show? To plot this you need to create a contingency table with the variables of interest. Changing plot elements: You can name the variables in your contingency table (e.g. name = variable, othername = othervariable). The name you assign to your table will be used as the title in the plot.\nWhat do you think of the study now? Could you already draw conclusions from this plot? Provide your answer to the last question on elearning-extern (“02_Test for Exercise in R”).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPlot 1\n\ncounts <- table(dat$Improved)  # create a table which gives you counts of the three categories in the \"Improved\" variable\n\nbarplot(counts,            # create a barplot of your table\n        ylim = c(0,50),    # change the limits of your y axis: starting from zero to 50\n        xlab = \"Improved\",  # add a label to your x axis\n        ylab = \"Frequency\")  # add a label to your y axis\n\n\n\n\nPlot 2\n\nRatios <- table(Improved = dat$Improved, Treatment = dat$Treatment)  # create a table with the assigned name \"ratios\", give the name \"Improved\" to the first variable which is the variable Improved from the data set Arthritis, give the name \"Treatment\" to the second variable which is the variable Treatment from the data set Arthritis\nplot(Ratios)  # create a plot of the table \"ratios\"\n\n\n\n\nTo view the tables with the names “counts” and “Ratios” you can simply execute their names:\n\ncounts\n## \n##   None   Some Marked \n##     42     14     28\nRatios\n##         Treatment\n## Improved Placebo Treated\n##   None        29      13\n##   Some         7       7\n##   Marked       7      21\n\nCould you already draw conclusions from this plot? No, because this is only a descriptive plot. You can say that a large proportion of the patients that got a placebo treatment did not improve while a large proportion of the patients that got the new treatment improved markedly. However, this could also be the result of random variation and sampling. We need inferential statistics to make conclusions about the effectiveness of the treatment.\n\n\n\n\n\n2.1.2 Histogram\nNow let’s have a look at the “birdabundance” dataset, which is in the “EcoData” package. This is not stored at CRAN (the official platform for R packages, but at github where we host our own code collections). If you haven’t done this yet, use the code below to install the package (note that you also need the “devtools” package to do this. Again, to load it and make the data set accessible, execute the function library(EcoData). To view the data set you can use the View() function again.\nYou can also get more explanations on the data set via the help.\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", dependencies = T, build_vignettes = T)\nlibrary(EcoData)\nView(birdabundance)\n\nThe dataset has been assessed by scientists in Australia who counted birds in forest fragments and looked for drivers that influence these bird abundances, e.g. the size of a forest fragment and the distance to the next fragment. We want to see how these distances vary. A histogram is the standard figure to depict one numerical variable, such as for example distance measurements.\nYour task is now to reconstruct the following histogram including the labels, colors and lines using the functions:\n\nhist() to create a histogram\nabline() to add (vertical) lines to your histogram\n\nThink about what the histogram tells you about the distances between each forest fragment and the next forest fragment. What do the red and purple lines represent? Which site was the furthest away from forest fragments? Give your answer on elearning-extern.\n\n\n\n\n\nHints for plotting\nChange the color (look at the help to see how). You can also try other colors, e.g. your favorite color (you can find all available colors with the function colors()). Change the bar widths (breaks). Play around to see how they change. When changing the bar widths, what do you notice on the y-axis? You can change the y-axis limits using “ylim” (see examples for hist(), second last line). Change the title and the x-axis name of your histogram using the arguments “main” and “xlab”.\nTo add the lines, try the abline() function with the argument “v = 90” and look what happens. To remove the line, you have to execute the code producing your histogram again. Remember that abline() is a low level plotting function, which means it only adds an element to an existing plot! Instead of calculating the depicted values with a function and then pasting the values into your abline function, you can also directly use the function in abline().\nSee an example here:\n\nhist(airquality$Temp)\nabline(v = 90, col = \"blue\")\nabline(v = median(airquality$Temp), col = \"red\")\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can plot the histogram like this:\n\nhist(birdabundance$DIST,  # plot a histogram of the varriable distance from the data set birdabundance\n     breaks = 20,  # change bar widths, here we make them smaller\n     col = \"blue\",  # change color to blue\n     ylim = c(0,20),  # change the range of the y axis to from 0 to 20\n     main = \"Distance distribution\",  # change title of the plot\n     xlab = \"Distance to the next forest fragment\")  # change x axis name\nabline(v = mean(birdabundance$DIST), col = \"red\")  # add a vertical line with an x value equal to the mean of the variable distance\nabline(v = median(birdabundance$DIST), col = \"purple\")  # add a vertical line with an x value equal to the median of the variable distance\n\n\n\n\nWhat do the red and purple lines represent?\nThe red line represents the mean distance between forest fragments, while the purple line represents the median.\nWhich site was the furthest away from forest fragments?\n\n# Extract the line in which the variable DIST takes its maximum\nbirdabundance[which.max(birdabundance$DIST),]\n##    Site ABUND AREA DIST LDIST YR.ISOL GRAZE ALT\n## 48   48  39.6   49 1427  1557    1972     1 180\n\nThe site number 48 was the furthest away.\n\n\n\n\n\n2.1.3 Scatterplot\nAs you’ve learned by now, plot() can create different types of plots depending on the type of the input data. It creates a scatterplot when plotting two numerical variables. Now we are interested to see how the abundance of birds is affected by the distance to the next forest fragment, and if there is another variable that is important for this relationship (visualized here by the color of the points).\nAgain, your task is to reconstruct the following plot using the following functions:\n\nplot()\n(str())\n(summary())\n\nWhat do you notice about the distribution of the colors along bird abundance?\nWhat is the mean bird abundance per color?\n\n\n\n\n\nHints for plotting:\nWhat is plotted on the x-axis, what on the y-axis?\nThere are two different ways to write the plot function. One is to stick with the “Usage” in the help of the plot function (giving coordinates along the x-axis as the first attribute and those along the y-axis as the second attribute). The other way is to write the relationship between x and y as a formula that is: “y~x, data = dataset” Use google to find out how you can change the point shapes in your plot.\nLook at the dataset to find out which variable is indicated by the color of the points in the plot. Hint: It is a variable indicating 5 intensity levels. To change the color, include the attribute “col” in your plot function and set it equal to the variable.\nTo get a color gradient you can create a function with the following code. Apply it before producing your plot and use the created function rbPal() as the color in the plot.\n\n# Palettes can be created with a function for the grDevices package\ninstall.packages(\"grDevices\")\nlibrary(grDevices)\n\n# Create a function to generate a continuous color palette from red to blue\nrbPal <- colorRampPalette(c('red','blue'))\n\n# Example for color in a plot\nplot(Ozone ~ Solar.R, data = airquality, \n     col = rbPal(12)[Month]) # you can use any number, here it's 12 because we have 12 months\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can either create a scatterplot of two numerical variables like this:\n\n#Create a function to generate a continuous color palette from red to blue\nrbPal <- colorRampPalette(c('red','blue')) # rpPal for red to blue palette\n\nplot(birdabundance$DIST, birdabundance$ABUND,  # create a plot of the variables DIST against ABUND from the data set birdabundance\n     ylab = \"Distance (m)\",  # add the label \"Distance\" to the y axis\n     xlab = \"Bird abundance\",  # add the label \"Bird abundance\" to the x axis\n     col = rbPal(5)[birdabundance$GRAZE],  # color the data points according to their category in the variable GRAZE from the data set birdabundance\n     pch = 17) # change the point shape\n\n\n\n\nOr like this:\n\nplot(ABUND ~ DIST, data = birdabundance,  # create a plot of the variables DIST against ABUND from the data set birdabundance\n     xlab = \"Distance (m)\",  # add the label \"Distance\" to the x axis\n     ylab = \"Bird abundance\",  # add the label \"Bird abundance\" to the y axis\n     col = rbPal(5)[GRAZE],  # color the data points according to their category in the variable GRAZE as a gradient\n     pch = 17) # change the point shape\n\n\n\n\nThe advantage of the second version is that it uses the structure of “response variable (y) explained by (~) explanatory variable (x)”. Also, you tell the plot function which data set to use once and it will automatically draw the variables from there, while in the first version you name the data set and the respective variable each time (even for the color).\nWhat do you notice about the distribution of the colors along bird abundance?\nYou can see that the blue data points are only at the low abundances, whereas the red data points are rather at the higher abundances. Purple data points are throughout all abundances. There thus seems to be a correlation between the grazing classes and bird abundances.\nWhat is the mean bird abundance per color / level of grazing intensity?\n\n# Option 1: Using tidyverse / dplyr as shown at the end of section 2.0.1 Summary statistics:\nlibrary(dplyr)\nbird_grouped <- birdabundance %>% # define dataset to be summarized\n  group_by(GRAZE) %>% # define grouping factor\n  summarise(mean.abund = mean(ABUND)) # summarize by taking the mean of abundance\n\n# Option 2: Using base R and formula notation:\nbird_grouped <- aggregate(ABUND~GRAZE, data = birdabundance, FUN = mean)\n\nbird_grouped\n##   GRAZE     ABUND\n## 1     1 28.623077\n## 2     2 21.950000\n## 3     3 21.286667\n## 4     4 20.571429\n## 5     5  6.292308\n\n\n\n\n\n\n2.1.4 Correlation\nIn the previous plot on bird abundance you’ve seen three variables. Now we want to know, how they are correlated with each other. Remember that we can use the function cor() to calculate correlations. Which of the following correlation coefficients (Pearson) belongs to which variable pair? Can you see these correlations in your previous plot?\n\n## [1] 0.2361125\n## [1] -0.6825114\n## [1] -0.2558418\n\nGive your answer to the second last question on elearning-extern. Think about the meaning of the correlation values (positive/negative, strength). Is it what you would have expected by looking at the plot?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncor(birdabundance$ABUND, birdabundance$DIST)\n## [1] 0.2361125\ncor(birdabundance$ABUND, birdabundance$GRAZE)\n## [1] -0.6825114\ncor(birdabundance$GRAZE, birdabundance$DIST)\n## [1] -0.2558418\n\nThe first correlation (abundance to distance) tells us that there is a small positive correlation between the two variables, but it does not tell us whether it is significant or not. We will properly test such relationships later in the course. In the scatter plot we have seen this weak positive correlation already. The second correlation (abundance to grazing) tells us that there is a stronger negative correlation between abundance and grazing. We have already seen a pattern of the color of the data points along bird abundances (red towards higher, blue towards lower abundances). The third correlation (grazing to distance) tells us that there is a small negative correlation between the two variables. However, the color pattern along distance is not as obvious as for abundance."
  },
  {
    "objectID": "3-HypothesisTesting.html#a-recipe-for-hypothesis-testing",
    "href": "3-HypothesisTesting.html#a-recipe-for-hypothesis-testing",
    "title": "3  Hypothesis Tests",
    "section": "3.1 A recipe for hypothesis testing",
    "text": "3.1 A recipe for hypothesis testing\nAim: We want to know if there is a difference between the control and the treatment\n\nWe introduce a Null hypothesis H0 (e.g. no effect, no difference between control and treatment)\nWe invent a test statistic\nWe calculate the expected distribution of our test statistic given H0 (this is from our data-generating model)\nWe calculate the p-value = probability of getting observed test statistic or more extreme given that H0 is true (there is no effect): \\(p-value = P(d\\ge D_{obs} | H_0)\\)\n\n\n\n\n\n\n\nInterpretation of the p-value\n\n\n\n\n\np-values make a statement on the probability of the data or more extreme values given H0 (no effect exists), but not on the probability of H0 (no effect) and not on the size of the effect or on the probability of an effect!\nIf you want to read more about null hypothesis testing and the p-value, take a look at Daniel Lakens Book\n\n\n\nExample:\nImagine we do an experiment with two groups, one treatment and one control group. Test outcomes are binary, e.g. whether individuals are cured.\n\nWe need a test statistic. For example: number cured of total patients: treat/(treat+control)\nWe need the distribution of this test statistic under the null.\n\nLet’s create a true world without effect:\n\nset.seed(123)\n# Let's create a true world without effect:\nPperGroup = 50 # number of replicates (e.g., persons per treatment group)\npC = 0.5 #probability of being cured in control group\npT = 0.5 #probability of being cured in treatment group; = same because we want to use these to get the distribution of the test statistic we define below when H0 is true (no effect)\n\n# Let's draw a sample from this world without effect\ncontrol = rbinom(n = 1, size = PperGroup, prob = pC)\ntreat = rbinom(n = 1, size = PperGroup, prob = pT)\n#calculate the test statistic: \ntreat/(treat+control)\n## [1] 0.5490196\n#and plot\nbarplot(c(control, treat), ylim = c(0, 50))\n\n\n\n\n\nNow, let’s do this very often to get the distribution under H0\n\n\ntestStatistic = rep(NA, 100000) \nfor (i in 1:100000) {\n  control = rbinom(n = 1, size = PperGroup, prob = pC)\n  treat = rbinom(n = 1, size = PperGroup, prob = pT)\n  testStatistic[i] = control/(treat+control) # test statistic \n}\nhist(testStatistic, breaks = 50)\n\n\n\n\n\nWe now have our test statistic + the frequency distribution of our statistic if the H0 = true. Now we make an experiment: Assume that we observed the following data: C = 30, T = 23.\n\n\nhist(testStatistic, breaks = 50)\ntestStatisticData = 30/(30+23)\nabline(v = testStatisticData, col = \"red\")\n\n\n\n\nmean(testStatistic > testStatisticData)\n## [1] 0.09317\n# compare each value in our testStatistic distribution with\n# the observed value and calculate proportion of TRUE values \n# (where testStatistic > testStatisticData)\n\nBut we know actually that the test statistic follows a Chi2 distribution. So to get correct p-values we can use the prop.test for this test statistic:\n\nprop.test(c(30, 23), c(PperGroup, PperGroup))\n## \n##  2-sample test for equality of proportions with continuity correction\n## \n## data:  c(30, 23) out of c(PperGroup, PperGroup)\n## X-squared = 1.4452, df = 1, p-value = 0.2293\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  -0.0737095  0.3537095\n## sample estimates:\n## prop 1 prop 2 \n##   0.60   0.46\n# other test statistic with known distribution\n# Pearson's chi-squared test statistic\n# no need to simulate\n\nWe pass the data to the function which first calculates the test statistic and then calculates the p-value using the Chi2 distribution."
  },
  {
    "objectID": "3-HypothesisTesting.html#t-test",
    "href": "3-HypothesisTesting.html#t-test",
    "title": "3  Hypothesis Tests",
    "section": "3.2 T-test",
    "text": "3.2 T-test\nOriginally developed by Wiliam Sealy Gosset (1876-1937) who has worked in the Guinness brewery. He wanted to measure which ingredients result in a better beer. The aim was to compare two beer recipes and decide whether one of the recipes was better (e.g. to test if it results in more alcohol). He published under the pseudonym ‘Student’ because the company considered his statistical methods as a commercial secret.\n\n\n\n\n\n\nT-test assumptions\n\n\n\n\nData in both groups is normally distributed\nH0 : the means of both groups are equal\n\n\n\nThe idea is that we have two normal distributions (e.g. alcohol distributions):\n\n\nCode\nset.seed(1)\nA = rnorm(100, mean = -.3)\nB = rnorm(100, mean = .3)\nplot(density(A), col = \"red\", xlim = c(-2, 2), ylim = c(0, 0.6))\nlines(density(B))\nabline(v = mean(A), col = \"red\")\nabline(v = mean(B))\n\n\n\n\n\nAnd our goals is now to test if the difference between the two means of the variables is statistically significant or not.\nProcedure:\n\nCalculate variances and means of both variables\n\nA_m = mean(A)\nB_m = mean(B)\nA_v = var(A)\nB_v = var(B)\n\nCalculate t-statistic (difference between means / (Standard deviation/sample size)\n\nt_statistic = (A_m - B_m) / sqrt( A_v  / length(A) + B_v / length(B))\nt_statistic\n## [1] -3.452108\n\nCompare observed t with t distribution under H0 (which we can do by using the CDF function of the t-distribution:\n\npt( t_statistic,  # test statistic\n   df = length(A)+length(B)-2, # degrees of freedom, roughly = n_obs - n_parameters\n   lower.tail = TRUE\n  )*2\n## [1] 0.0006799933\n\n\n\n\n\n\n\n\nOne-sided or two-sided\n\n\n\nIf we do NOT know if the dataset from one group is larger or smaller than the other, we must use two-sided tests (that’s why we multiply the p-values with 2). Only if we are sure that the effect MUST be positive / negative, we can test for greater / less. Decide BEFORE you look at the data!\n\n\nLet’s compare it to the output of the t.test function which does everything for us, we only need to pass the data to the function:\n\nt.test(A, B, var.equal = TRUE)\n## \n##  Two Sample t-test\n## \n## data:  A and B\n## t = -3.4521, df = 198, p-value = 0.00068\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.7122549 -0.1943542\n## sample estimates:\n##  mean of x  mean of y \n## -0.1911126  0.2621919\n\nUsually we also have to test for normality of our data, which we can do with another test.\nExample airquality\n\n# with real data\nhead(PlantGrowth)\n##   weight group\n## 1   4.17  ctrl\n## 2   5.58  ctrl\n## 3   5.18  ctrl\n## 4   6.11  ctrl\n## 5   4.50  ctrl\n## 6   4.61  ctrl\nboxplot(weight ~ group, data = PlantGrowth)\n\n\n\n\nctrl = PlantGrowth$weight[PlantGrowth$group == \"ctrl\"]\ntrt1 = PlantGrowth$weight[PlantGrowth$group == \"trt1\"]\n\n# attention: t test assumes normal dirstribution of measurements in both groups!\n# test normality before doing the t test:\nshapiro.test(ctrl)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  ctrl\n## W = 0.95668, p-value = 0.7475\nshapiro.test(trt1)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  trt1\n## W = 0.93041, p-value = 0.4519\n\n\nt.test(ctrl, trt1)\n## \n##  Welch Two Sample t-test\n## \n## data:  ctrl and trt1\n## t = 1.1913, df = 16.524, p-value = 0.2504\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.2875162  1.0295162\n## sample estimates:\n## mean of x mean of y \n##     5.032     4.661\n# note that this is a \"Welch\" t-test\n# we will have a look at the differences among t-tests in the next large exercise\n\n# What is H0? equal means\n# What is the result? test is not significant, H0 is not rejected\n# Explain the different values in the output!\n\n\n\n\n\n\n\nShapiro - Test for normality\n\n\n\nIf you have a small sample size, the shapiro.test will always be non-significant (i.e. not significantly different from a normal distribution)! This is because small sample size leads to low power for rejecting H0 of normal distribution"
  },
  {
    "objectID": "3-HypothesisTesting.html#type-i-error-rate",
    "href": "3-HypothesisTesting.html#type-i-error-rate",
    "title": "3  Hypothesis Tests",
    "section": "3.3 Type I error rate",
    "text": "3.3 Type I error rate\nLet’s start with a small simulation example:\n\nresults = replicate(1000, {\n  A = rnorm(100, mean = 0.0)\n  B = rnorm(100, mean = 0.0)\n  t.test(A, B)$p.value\n})\nhist(results)\n\n\n\n\nWhat’s happening here? We have no effect in our simulation but there are many p-values lower than \\(\\alpha = 0.05\\):\n\nmean(results < 0.05)\n## [1] 0.043\n\nSo in 0.043 of our experiments we would reject H0 even when there is no effect at all! This is called the type I error rate. Those are false positives.\n\n\n\n\n\n\nType I error rate and multiple testing\n\n\n\n\n\nIf there is no effect, the probability of having a positive test result is equal to the significance level \\(\\alpha\\). If you test 20 things that don’t have an effect, you will have one significant result on average when using a significance level of 0.05. If multiple tests are done, a correction for multiple testing should be used.\nThis problem is called multiple testing\n\ne.g.: if you try 20 different analyses (Null hypotheses), on average one of them will be significant.\ne.g.: if you test 1000 different genes for their association with cancer, and in reality, none of them is related to cancer, 50 out of the tests will still be significant.\nIf multiple tests are done, a correction for multiple testing should be used\nincreases the p-values for each test in a way that the overall alpha level is 0.05\n\n\n# conduct a t-test for each of the treatment combinations\n# save each test as a new object (test 1 to 3)\ncontrol = PlantGrowth$weight[PlantGrowth$group == \"ctrl\"]\ntrt1 = PlantGrowth$weight[PlantGrowth$group == \"trt1\"]\ntrt2 = PlantGrowth$weight[PlantGrowth$group == \"trt2\"]\n\ntest1 = t.test(control, trt1)\ntest2 = t.test(control, trt2)\ntest3 = t.test(trt1, trt2)\n\nc(test1$p.value, test2$p.value, test3$p.value)\n## [1] 0.250382509 0.047899256 0.009298405\n\n# now adjust these values\np.adjust(c(test1$p.value, test2$p.value, test3$p.value)) # standard is holm, average conservative\n## [1] 0.25038251 0.09579851 0.02789521\np.adjust(c(test1$p.value, test2$p.value, test3$p.value), method = \"bonferroni\") # conservative\n## [1] 0.75114753 0.14369777 0.02789521\np.adjust(c(test1$p.value, test2$p.value, test3$p.value), method = \"BH\") # least conservative\n## [1] 0.25038251 0.07184888 0.02789521\n# for details on the methods see help\n\n\n\n\nIf multiple testing is a problem and if we want to avoid false positives (type I errors), why don’t we use a smaller alpha level? Because if would increase the type II error rate"
  },
  {
    "objectID": "3-HypothesisTesting.html#type-ii-error-rate",
    "href": "3-HypothesisTesting.html#type-ii-error-rate",
    "title": "3  Hypothesis Tests",
    "section": "3.4 Type II error rate",
    "text": "3.4 Type II error rate\nIt can also happen the other way around:\n\nresults = replicate(1000, {\n  A = rnorm(100, mean = 0.0)\n  B = rnorm(100, mean = 0.2) # effect is there\n  t.test(A, B)$p.value\n})\nhist(results)\n\n\n\n\n\nmean(results < 0.05)\n## [1] 0.292\n\nNo we wouldn’t reject the H0 in 0.708% of our experiments. This is the type II error rate (false negatives).\nThe type II error rate (\\(\\beta\\)) is affected by\n\nsample size \\(\\uparrow\\) , decreases \\(\\beta\\)\ntrue effect size \\(\\uparrow\\), decreases \\(\\beta\\)\n\\(\\alpha\\) \\(\\uparrow\\), decreases \\(\\beta\\)\nvariability (variance) \\(\\uparrow\\), increases \\(\\beta\\)\n\nAfter the experiment, the only parameter we could change would be the significance level \\(\\alpha\\), but increasing it would result in too high Type I error rates."
  },
  {
    "objectID": "3-HypothesisTesting.html#statistical-power",
    "href": "3-HypothesisTesting.html#statistical-power",
    "title": "3  Hypothesis Tests",
    "section": "3.5 Statistical power",
    "text": "3.5 Statistical power\nWe can reduce \\(\\alpha\\) and we will get fewer type I errors (false positives), but type II errors (false negatives) will increase. So what can we do with this in practice?\n1- \\(\\beta\\) is the so called statistical power which is the rate at which a test is significant if the effect truly exists. Power increases with stronger effect, smaller variability, (larger \\(\\alpha\\) ), and more data (sample size). So, collect more data? How much data do we need?\nBefore the experiment, you can estimate the effect size and the variability. Together with alpha (known), you can calculate the power depending on the sample size:\n\n\nCode\nresults = \n  sapply(seq(10, 500, by = 20), function(n) {\n    results = replicate(100, {\n      A = rnorm(n, mean = 0.0)\n      B = rnorm(n, mean = 0.2) # effect is there\n      t.test(A, B)$p.value\n    })\n    power = 1 - mean(results > 0.05)\n    return(power)\n  })\nplot(seq(10, 500, by = 20), results, xlab = \"Sample size\", ylab = \"Power\", main = \"\")\n\n\n\n\n\nWe call that a power analysis and there’s a function in R to do that:\n\npower.t.test(n = 10, delta = 1, sd = 1, type = \"one.sample\")\n## \n##      One-sample t test power calculation \n## \n##               n = 10\n##           delta = 1\n##              sd = 1\n##       sig.level = 0.05\n##           power = 0.8030962\n##     alternative = two.sided\n\n# Power increases with sample size (effect size constant, sd constant):\npow <- function(n) power.t.test(n, delta = 1, sd = 1, type = \"one.sample\")$power\nplot(1:20, sapply(1:20, pow), xlab = \"Sample size\", ylab = \"Power\", pch = 20)\n\n\n\n\n# Power increases with effect size\npow <- function(d) power.t.test(n = 20, delta = d, sd = 1, \n                                type = \"one.sample\")$power\nplot(seq(0,1,0.05), sapply(seq(0,1,0.05), pow), xlab = \"Effect size\", \n     ylab = \"Power\", pch = 20)\n\n\n\n\n# Power decreases with increasing standard deviation (or variance):\npow <- function(s) power.t.test(n = 20, delta = 1, sd = s, \n                                type = \"one.sample\")$power\nplot(seq(0.5,1.5,0.05), sapply(seq(0.5,1.5,0.05), pow), \n     xlab = \"Standard deviation\", ylab = \"Power\", pch = 20)"
  },
  {
    "objectID": "3-HypothesisTesting.html#false-discovery-rate",
    "href": "3-HypothesisTesting.html#false-discovery-rate",
    "title": "3  Hypothesis Tests",
    "section": "3.6 False discovery rate",
    "text": "3.6 False discovery rate\nYou may have realized that if we do an experiment with a (weak) effect, we can get a significant result because of the effect but also significant results because of the Type I error rate. How to distinguish between those two? How can we decide whether a significant result is a false positive? This error rate is called the false discovery rate and to lower it we need to increase the power:\n\\[\nFDR = \\frac{p(H_0)\\cdot\\alpha}{p(H_0)\\cdot\\alpha + p(!H_0)\\cdot(1-\\beta)}\n\\]\n\\(p(H_0)\\) = probability of H0 (no effect); \\(p(!H_0)\\) = probability of not H0 (effect exists). Both are unknown and the only parameters we can influence are \\(\\alpha\\) and \\(\\beta\\). But decreasing \\(\\alpha\\) leads to too high false negatives, so \\(\\beta\\) is left."
  },
  {
    "objectID": "3-HypothesisTesting.html#statistical-tests",
    "href": "3-HypothesisTesting.html#statistical-tests",
    "title": "3  Hypothesis Tests",
    "section": "3.7 Statistical tests",
    "text": "3.7 Statistical tests\nThe following three sections provide an overview over the most important hypothesis tests, a guideline to select an appropriate test (see decision tree) and the necessary code to apply these tests in R. It is not necessary to read and understand every detail. These explanations are also meant as an advice if you want to select an appropriate hypothesis test after the course. Note that some of the examples use simulated data instead of real observation (all functions that start with r=random, e.g. rnorm() or runif()). Simulated data is useful, because then we know the true distribution and its true mean.\nIn the last section at the end of this file, you find exercises that you should solve using the explanations above. It may be helpful to use the table of content and/or the search option to find the respective example in the explanations."
  },
  {
    "objectID": "3-HypothesisTesting.html#comparison-of-mean-of-two-or-more-groups",
    "href": "3-HypothesisTesting.html#comparison-of-mean-of-two-or-more-groups",
    "title": "3  Hypothesis Tests",
    "section": "3.8 Comparison of mean of two or more groups",
    "text": "3.8 Comparison of mean of two or more groups\nMany tests aim at showing that variables are significantly different between groups, i.e. have different means/medians. In all these tests, H0 is that there is no difference between the groups. The following decision tree helps to select the appropriate test.\n\nRemark 1: Tests for 2 groups also work for one group only. Then they test whether the mean is equal to 0.\nRemark 2: Paired / unpaired: this means that observations in the groups are linked to each other. An example for unpaired data is a typical experiment with 10 observations in the control group and 10 observations in the treatment group. An example for paired data is when the same individuals were exposed to the treatment and to the control. The observations of each individual would belong together (pairs).\nRemark 3: Parametric: assumption of normal distribution. Non-parametric = no assumption for the distribution.\nRemark 4: Blue text: If a test for more than two groups is significant, post-hoc tests are carried out in a second step. These check all possible comparisons of groups for significant differences by adjusting p-values for multiple testing.\n\n3.8.1 Tests for 2 groups\n\n3.8.1.1 t-Test\nThe t-test can draw conclusions about the mean(s) of 1 or 2 normally-distributed groups.\n\n## Classical example: Student's sleep data\nplot(extra ~ group, data = sleep)\n\n\n\n\nBe aware: The line in the box plot does not show the mean but the median.\n\n## Formula interface\nt.test(extra ~ group, data = sleep)\n## \n##  Welch Two Sample t-test\n## \n## data:  extra by group\n## t = -1.8608, df = 17.776, p-value = 0.07939\n## alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n## 95 percent confidence interval:\n##  -3.3654832  0.2054832\n## sample estimates:\n## mean in group 1 mean in group 2 \n##            0.75            2.33\n\nThis output tells us, that the difference in means between the 2 groups is not significant(p-value ≥ 0.05, specifically: p-value = 0.07939), provided that our significance level is 0.05.\nThe underlying Null-hypothesis is that the true difference in means is equal to 0. In the last two lines of the output you can see the means of the respective groups. Even though the means seem to be quite different, the difference is not significant, this could be due to the small sample size of only 10 students per group.\nLet’s look at different settings of the t-test:\n\n3.8.1.1.1 t-test, H0: one group, mean = 0\nThe Null-hypothesis here is that the mean of the observed group is equal to 0.\n\nx = rnorm(20, mean = 2)\nt.test(x)\n## \n##  One Sample t-test\n## \n## data:  x\n## t = 11.224, df = 19, p-value = 7.944e-10\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  1.786080 2.604892\n## sample estimates:\n## mean of x \n##  2.195486\n\np-value < 0.05 means we can reject the Null-hypothesis, i.e. the mean of the observed group is significantly different from 0.\n\n\n3.8.1.1.2 t-test, H0: two groups, equal means, equal variances\nThe Null-hypothesis here is that the two observed groups have the same mean and the same variance (specified by the argument var.equal = T).\n\nx1 = rnorm(20, mean = 2)\nx2 = rnorm(20, mean = 3)\nt.test(x1,x2, var.equal = T)\n## \n##  Two Sample t-test\n## \n## data:  x1 and x2\n## t = -3.1484, df = 38, p-value = 0.00319\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -1.6632472 -0.3614177\n## sample estimates:\n## mean of x mean of y \n##  2.074797  3.087130\n\n\n\n3.8.1.1.3 t-test, H0: two groups, equal means, variable variance\nThe Null-hypothesis here is that the two observed groups have the same mean and variable variances (the default setting of the argument var.equal = F).\n\nx1 = rnorm(20, mean = 2, sd = 1)\nx2 = rnorm(20, mean = 3, sd = 2)\nt.test(x1,x2, var.equal = FALSE)\n## \n##  Welch Two Sample t-test\n## \n## data:  x1 and x2\n## t = -2.1475, df = 30.4, p-value = 0.03984\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -1.9210146 -0.0487896\n## sample estimates:\n## mean of x mean of y \n##  1.930744  2.915646\n\nWhich is actually a Welch t-test and which is the default in R! (The smaller the samples, the more likely it is that the variances differ! So it is an conservative assumption to assume that the variances are unequal)\n\n\n3.8.1.1.4 t-test, H0: two groups, paired, equal means, variance can be different\nThe Null-hypothesis here is that the two groups are paired observations (e.g. group 1 before treatment and group 2 after treatment) have the same mean. Variances doesn’t matter here (but the variables must be still normally distributed). We can ignore the var.equal`argument here:\n\nx1 = rnorm(20, mean = 2)\nx2 = rnorm(20, mean = 3)\nt.test(x1,x2, paired = TRUE)\n## \n##  Paired t-test\n## \n## data:  x1 and x2\n## t = -5.5876, df = 19, p-value = 2.177e-05\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  -2.222268 -1.011105\n## sample estimates:\n## mean difference \n##       -1.616686\n\nIf the variables are not normally distributed we have to use non-parametric tests which don’t assume a certain assumption regarding the distribution of the variables (=nonparametric). But the test statistic still follows a certain distribution!\n\n\n\n3.8.1.2 Wilcoxon Rank Sum and Mann-Whitney U Test\nIn R, there is only one function for both tests together: wilcox.test(). The Wilcoxon rank sum test with (paired = F) is classically called Mann-Whitney U test.\n\n3.8.1.2.1 Unpaired: Mann-Whitney U Test\n\nx1 = rnorm(20, mean = 2)\nx2 = rlnorm(20, mean = 3)\n\nwilcox.test(x1, x2)\n## \n##  Wilcoxon rank sum exact test\n## \n## data:  x1 and x2\n## W = 18, p-value = 2.317e-08\n## alternative hypothesis: true location shift is not equal to 0\n\n\n\n3.8.1.2.2 Paired: Wilcoxon signed rank test\n\nx1 = rnorm(20, mean = 2)\nx2 = rlnorm(20, mean = 3)\n\nwilcox.test(x1, x2, paired = T)\n## \n##  Wilcoxon signed rank exact test\n## \n## data:  x1 and x2\n## V = 0, p-value = 1.907e-06\n## alternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n3.8.2 Tests for > 2 groups\n\n3.8.2.1 Anova, unpaired\nH0 >2 groups, normal distribution, equal variance, equal means, unpaired\n\nx = aov(weight ~ group, data = PlantGrowth)\nsummary(x)\n##             Df Sum Sq Mean Sq F value Pr(>F)  \n## group        2  3.766  1.8832   4.846 0.0159 *\n## Residuals   27 10.492  0.3886                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAn ANOVA only tests, if there is a difference, but not between which groups. To perform pairwise comparisons, you can use post-hoc tests. Common for ANOVA results is\n\nTukeyHSD(x)\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = weight ~ group, data = PlantGrowth)\n## \n## $group\n##             diff        lwr       upr     p adj\n## trt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\n## trt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\n## trt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\nAlternatively, you can also perform several tests each comparing two groups and then correct for multiple testing. This is what we did before.\nPairwise comparisons are often visualized using different letters to significantly different groups:\n\n# install.packages(\"multcomp\")\nlibrary(multcomp)\ntuk = glht(x, linfct = mcp(group = \"Tukey\")) #performs Tukey pairwise comparisons\ntuc.cld = cld(tuk) # assigns different letters to significantly different groups\n\nold.par = par(mai = c(1, 1, 1.25, 1), no.readonly = T)\nplot(tuc.cld) # draws boxplot + letters from cld function\n\n\n\npar(old.par)\n\n\n\n3.8.2.2 Anova, paired\naov is not good in doing repeated = paired ANOVA. In simple cases, you can just subtract the paired groups. In general, you should use so-called mixed models!\n\n\n3.8.2.3 Kruskal-Wallis, unpaired non-parametric\nNon-parametric test for differences in the mean of >2 groups, unpaired\n\nboxplot(Ozone ~ Month, data = airquality)\n\n\n\nkruskal.test(Ozone ~ Month, data = airquality)\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  Ozone by Month\n## Kruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06\n\n\n\n3.8.2.4 Friedmann Test, paired non-parametric\nNon-parametric test for differences in the mean of >2 groups, paired.\n\nwb <- aggregate(warpbreaks$breaks,\n                by = list(w = warpbreaks$wool,\n                          t = warpbreaks$tension),\n                FUN = mean)\n#wb\nfriedman.test(wb$x, wb$w, wb$t)\n## \n##  Friedman rank sum test\n## \n## data:  wb$x, wb$w and wb$t\n## Friedman chi-squared = 0.33333, df = 1, p-value = 0.5637\n# Alternative: friedman.test(x ~ w | t, data = wb)\n# Note that x is the response, w is the group, and t are the blocks that are paired"
  },
  {
    "objectID": "3-HypothesisTesting.html#comparison-of-variances",
    "href": "3-HypothesisTesting.html#comparison-of-variances",
    "title": "3  Hypothesis Tests",
    "section": "3.9 Comparison of variances",
    "text": "3.9 Comparison of variances\nH0 in variance tests is always that the variances are equal.\n\n3.9.1 F-Test for two normally-distributed samples\n\nx <- rnorm(50, mean = 0, sd = 2)\ny <- rnorm(30, mean = 1, sd = 1)\nvar.test(x, y)                  # Do x and y have the same variance? - Significantly different\n## \n##  F test to compare two variances\n## \n## data:  x and y\n## F = 6.1781, num df = 49, denom df = 29, p-value = 1.518e-06\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 95 percent confidence interval:\n##   3.10401 11.62354\n## sample estimates:\n## ratio of variances \n##           6.178079\n\n\n\n3.9.2 Bartlett test for more than two normally-distributed samples\n\nx <- rnorm(50, mean = 0, sd = 1)\ny <- rnorm(30, mean = 1, sd = 1)\nz <- rnorm(30, mean = 1, sd = 1)\nbartlett.test(list(x, y, z))                # Do x, y and z have the same variance? - Not sigificantly different\n## \n##  Bartlett test of homogeneity of variances\n## \n## data:  list(x, y, z)\n## Bartlett's K-squared = 11.622, df = 2, p-value = 0.002994"
  },
  {
    "objectID": "3-HypothesisTesting.html#comparison-of-discrete-proportions",
    "href": "3-HypothesisTesting.html#comparison-of-discrete-proportions",
    "title": "3  Hypothesis Tests",
    "section": "3.10 Comparison of discrete proportions",
    "text": "3.10 Comparison of discrete proportions\nDiscrete proportions are typically analyzed assuming the binomial model (k/n with probability p)\n\n3.10.1 Exact Binomial Test\nH0 is that the data are binomially distributed with a fixed probability p.\n\n## Conover (1971), p. 97f.\n## Under (the assumption of) simple Mendelian inheritance, a cross\n##  between plants of two particular genotypes produces progeny 1/4 of\n##  which are \"dwarf\" and 3/4 of which are \"giant\", respectively.\n##  In an experiment to determine if this assumption is reasonable, a\n##  cross results in progeny having 243 dwarf and 682 giant plants.\n##  If \"giant\" is taken as success, the null hypothesis is that p =\n##  3/4 and the alternative that p != 3/4.\nbinom.test(c(682, 243), p = 3/4)\n## \n##  Exact binomial test\n## \n## data:  c(682, 243)\n## number of successes = 682, number of trials = 925, p-value = 0.3825\n## alternative hypothesis: true probability of success is not equal to 0.75\n## 95 percent confidence interval:\n##  0.7076683 0.7654066\n## sample estimates:\n## probability of success \n##              0.7372973\nbinom.test(682, 682 + 243, p = 3/4)   # The same.\n## \n##  Exact binomial test\n## \n## data:  682 and 682 + 243\n## number of successes = 682, number of trials = 925, p-value = 0.3825\n## alternative hypothesis: true probability of success is not equal to 0.75\n## 95 percent confidence interval:\n##  0.7076683 0.7654066\n## sample estimates:\n## probability of success \n##              0.7372973\n## => Data are in agreement with H0\n\n\n\n3.10.2 Test of Equal or Given Proportions\nbased on Chi-squared-test, H0 is that the data in two groups are binomially distributed with the same probability p.\n\n## Data from Fleiss (1981), p. 139.\n## H0: The null hypothesis is that the four populations from which\n##     the patients were drawn have the same true proportion of smokers.\n## A:  The alternative is that this proportion is different in at\n##     least one of the populations.\nsmokers  <- c( 83, 90, 129, 70 )\npatients <- c( 86, 93, 136, 82 )\nprop.test(smokers, patients)\n## \n##  4-sample test for equality of proportions without continuity correction\n## \n## data:  smokers out of patients\n## X-squared = 12.6, df = 3, p-value = 0.005585\n## alternative hypothesis: two.sided\n## sample estimates:\n##    prop 1    prop 2    prop 3    prop 4 \n## 0.9651163 0.9677419 0.9485294 0.8536585\n##  => Data are not in agreement with H0\n\n\n\n3.10.3 Contingency tables\nChi-squared-test for count data, H~0~ is that the joint distribution of the cell counts in a 2-dimensional contingency table is the product of the row and column marginals\n\n## From Agresti(2007) p.39\nM <- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))\ndimnames(M) <- list(gender = c(\"F\", \"M\"),\n                    party = c(\"Democrat\",\"Independent\", \"Republican\"))\nchisq.test(M)\n## \n##  Pearson's Chi-squared test\n## \n## data:  M\n## X-squared = 30.07, df = 2, p-value = 2.954e-07"
  },
  {
    "objectID": "3-HypothesisTesting.html#distribution-tests",
    "href": "3-HypothesisTesting.html#distribution-tests",
    "title": "3  Hypothesis Tests",
    "section": "3.11 Distribution tests",
    "text": "3.11 Distribution tests\nOften we are interested in the distribution of a variable. This can be tested with distribution tests. All these tests are defined as follows: H0 is that the data follow a specific distribution. So in case H0 is rejected, the data significantly deviates from the specified distribution.\nOften, we want to know whether a variable is normally distributed because this is an important assumption for parametric hypothesis tests. But data can follow many other distributions:\n\n\n3.11.1 Shapiro-Wilk Normality Test\nBecause many tests require normal distribution, this is the test needed most often.\n\nshapiro.test(rnorm(100, mean = 5, sd = 3))\n## \n##  Shapiro-Wilk normality test\n## \n## data:  rnorm(100, mean = 5, sd = 3)\n## W = 0.97832, p-value = 0.09833\n\n\n\n3.11.2 Kolmogorov-Smirnov Test\nFor everything else, the KS test can be used. It compares two different distributions, or a distribution against a reference.\n\nx <- rnorm(50)\ny <- runif(30)\n# Do x and y come from the same distribution?\nks.test(x, y)\n## \n##  Exact two-sample Kolmogorov-Smirnov test\n## \n## data:  x and y\n## D = 0.54, p-value = 1.598e-05\n## alternative hypothesis: two-sided\n\n# Does x come from a shifted gamma distribution with shape 3 and rate 2?\nks.test(x+2, \"pgamma\", 3, 2) # two-sided, exact\n## \n##  Exact one-sample Kolmogorov-Smirnov test\n## \n## data:  x + 2\n## D = 0.26631, p-value = 0.001277\n## alternative hypothesis: two-sided\nks.test(x+2, \"pgamma\", 3, 2, exact = FALSE)\n## \n##  Asymptotic one-sample Kolmogorov-Smirnov test\n## \n## data:  x + 2\n## D = 0.26631, p-value = 0.001663\n## alternative hypothesis: two-sided\nks.test(x+2, \"pgamma\", 3, 2, alternative = \"gr\")\n## \n##  Exact one-sample Kolmogorov-Smirnov test\n## \n## data:  x + 2\n## D^+ = 0.059654, p-value = 0.6742\n## alternative hypothesis: the CDF of x lies above the null hypothesis\n\nFor an overview on distribution see here: http://www.stat.umn.edu/geyer/old/5101/rlook.html"
  },
  {
    "objectID": "3-HypothesisTesting.html#other-tests",
    "href": "3-HypothesisTesting.html#other-tests",
    "title": "3  Hypothesis Tests",
    "section": "3.12 Other tests",
    "text": "3.12 Other tests\n\n3.12.1 Correlation\nA test for the significance of a correlation:\n\ncor.test(airquality$Ozone, airquality$Wind)\n## \n##  Pearson's product-moment correlation\n## \n## data:  airquality$Ozone and airquality$Wind\n## t = -8.0401, df = 114, p-value = 9.272e-13\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.7063918 -0.4708713\n## sample estimates:\n##        cor \n## -0.6015465\n\nInterpretation: Ozone and Wind are significantly negatively correlated with a p-value < 0.05 and a correlation coefficient of -0.6015465.\n\n\n3.12.2 Mantel test\nThe Mantel test compares two distance matrices\n\nlibrary(vegan)\n## Is vegetation related to environment?\ndata(varespec)\ndata(varechem)\nveg.dist <- vegdist(varespec) # Bray-Curtis\nenv.dist <- vegdist(scale(varechem), \"euclid\")\nmantel(veg.dist, env.dist)\n## \n## Mantel statistic based on Pearson's product-moment correlation \n## \n## Call:\n## mantel(xdis = veg.dist, ydis = env.dist) \n## \n## Mantel statistic r: 0.3047 \n##       Significance: 0.001 \n## \n## Upper quantiles of permutations (null model):\n##   90%   95% 97.5%   99% \n## 0.109 0.136 0.165 0.198 \n## Permutation: free\n## Number of permutations: 999\nmantel(veg.dist, env.dist, method=\"spear\")\n## \n## Mantel statistic based on Spearman's rank correlation rho \n## \n## Call:\n## mantel(xdis = veg.dist, ydis = env.dist, method = \"spear\") \n## \n## Mantel statistic r: 0.2838 \n##       Significance: 0.001 \n## \n## Upper quantiles of permutations (null model):\n##   90%   95% 97.5%   99% \n## 0.118 0.149 0.174 0.199 \n## Permutation: free\n## Number of permutations: 999"
  },
  {
    "objectID": "3-HypothesisTesting.html#exercises",
    "href": "3-HypothesisTesting.html#exercises",
    "title": "3  Hypothesis Tests",
    "section": "3.13 Exercises",
    "text": "3.13 Exercises\n\n3.13.1 Streams\nThe dataset ‘streams’ contains water measurements taken at different locations along 16 rivers: ‘up’ and ‘down’ are water quality measurements of the same river taken before and after a water treatment filter, respectively. We want to find out if this water filter is effective. Use the decision tree to identify the appropriate test for this situation.\n\ndat = read.table(\"https://raw.githubusercontent.com/biometry/APES/master/Data/Simone/streams.txt\", header = T)\n\nVisualize and analyze the data and answer the following questions at elearning-extern (“03_Test for Exercise in R”).\n\nFor identifying an appropriate test for the effect of the water treatment filter, what are your first two choices in the decision tree?\nThe next decision you have to make is whether you can use a parametric test or not. Apply the Shapiro-Wilk test to check if the data are normally distributed. Are the tests significant and what does that tell you?\nWhich test is appropriate for evaluating the effect of the filter?\nDoes the filter influence the water quality? (The warnings are related to identical values, i.e. ties, and zero differences; we ignore these here)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYou can visualize the data as follows:\n\ndat = read.table(\"https://raw.githubusercontent.com/biometry/APES/master/Data/Simone/streams.txt\", header = T)\npar(mfrow = c(1, 2))\nboxplot(dat)\nmatplot(t(dat), type = \"l\") # each line is one river, left is down and right is upstream\n\n\n\npar(mfrow = c(1, 1))\n\n\nThe number of groups to compare is two, up versus down stream. The observations are paired because the water tested up and down stream of the filter is not independent from each other, i.e. the “same” water is measured twice!\nThe Shapiro-Wilk test is significant (p < 0.05) for down stream data, i.e. we reject H0 (the data is normally distributed). Thus, the data significantly deviate from a normal distribution. The test is not significant for upstream data; the data does not significantly deviate from a normal distribution.\n\n\nshapiro.test(dat$down)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  dat$down\n## W = 0.86604, p-value = 0.02367\nshapiro.test(dat$up)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  dat$up\n## W = 0.93609, p-value = 0.3038\n\n\nWe select a Wilcoxon signed rank test that is appropriate to compare not-normal, paired observations in two groups.\nH0 of the Wilcoxon signed rank test is that the location shift between the two groups equals zero, i.e. the difference between the pairs follows a symmetric distribution around zero. As p < 0.05, we can reject H0. The filter significantly influences water quality. (In case of ties also see the function wilcox.test() in the package coin for exact, asymptotic and Monte Carlo conditional p-values)\n\n\nwilcox.test(dat$down, dat$up, paired = T)\n## Warning in wilcox.test.default(dat$down, dat$up, paired = T): cannot compute\n## exact p-value with ties\n## Warning in wilcox.test.default(dat$down, dat$up, paired = T): cannot compute\n## exact p-value with zeroes\n## \n##  Wilcoxon signed rank test with continuity correction\n## \n## data:  dat$down and dat$up\n## V = 8, p-value = 0.004971\n## alternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n\n3.13.2 Chicken\nThe ‘chickwts’ experiment was carried out to measure and compare the effectiveness of various feed supplements on the growth rate of chickens. We are interested in two questions: Does the feed type influence the chickens weight at all? Which feed types result in significantly different chicken weights?\n\ndat = chickwts\n\nAnalyze the data and answer the following questions at elearning-extern.\n\nVisualize the data. What is an appropriate plot for this kind of data?\nCan you apply an ANOVA to this data? What are the assumptions for an ANOVA? Remember: you have to test two things for the groups (for this exercise it is enough if you test the groups “casein” and “horsebean” only).\nApply an ANOVA or the non-parametric test. How would you describe the result in a thesis or publication?\nAlso apply the alternative test and compare p-values. Which of the tests has a higher power?\nUse the result of the ANOVA to carry out a post-hoc test. How many of the pairwise comparisons indicate significant differences between the groups?\nWhich conclusion about the feed types ‘meatmeal’ and ‘casein’ is correct?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAn appropriate visualization for one numeric and one categorical variable is a boxplot. Using ‘notch = T’ in the function boxplot(), adds confidence interval for the median (the warning here indicates that we are not very confident in the estimates of the medians as the number of observations is rather small, you can see at the notches that go beyond the boxes).\n\n\ndat = chickwts\nboxplot(weight ~ feed, data = dat)\n\n\n\nboxplot(weight ~ feed, data = dat, notch = T)\n## Warning in (function (z, notch = FALSE, width = NULL, varwidth = FALSE, : some\n## notches went outside hinges ('box'): maybe set notch=FALSE\n\n\n\n\n\nThe two requirements for applying an ANOVA are 1) the data in each group are normally distributed, and 2) the variances of the different groups are equal. For 1) we again use a Shapiro-Wilk test. For 2) we can use the function var.test() or for all feed types the function bartlett.test(). All tests are not significant, and we thus have no indication to assume that the data is not-normally distributed or that the variances are different. We can use an ANOVA.\n\n\n# get data of each group\ncasein = dat$weight[dat$feed == \"casein\"]\nhorsebean = dat$weight[dat$feed == \"horsebean\"]\n\nshapiro.test(casein)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  casein\n## W = 0.91663, p-value = 0.2592\nshapiro.test(horsebean)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  horsebean\n## W = 0.93758, p-value = 0.5264\n# H0 normally distributed\n# not rejected, normality assumption is okay\n\nvar.test(casein, horsebean)\n## \n##  F test to compare two variances\n## \n## data:  casein and horsebean\n## F = 2.7827, num df = 11, denom df = 9, p-value = 0.1353\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 95 percent confidence interval:\n##  0.711320 9.984178\n## sample estimates:\n## ratio of variances \n##           2.782737\n# H0 ratio of variances is 1 = groups have the same variance\n# not rejected, same variances is okay\n\n\n### Extra: testing the assumptions for all groups:\n\n# Normality test for all groups\n\n# Option 1: using the dplyr package\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following object is masked from 'package:MASS':\n## \n##     select\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nnorm_test <- dat %>%\n  group_by(feed) %>% \n  summarise(p = shapiro.test(weight)$p)\n\n# Option 2: using base R\nnorm_test <- aggregate(weight~feed, data = dat, FUN = function(x){shapiro.test(x)$p})\n\nnorm_test # all groups seem normally distributed\n##        feed    weight\n## 1    casein 0.2591841\n## 2 horsebean 0.5264499\n## 3   linseed 0.9034734\n## 4  meatmeal 0.9611795\n## 5   soybean 0.5063768\n## 6 sunflower 0.3602904\n\n# Bartlett test for equal variances\nbartlett.test(weight ~ feed, dat)\n## \n##  Bartlett test of homogeneity of variances\n## \n## data:  weight by feed\n## Bartlett's K-squared = 3.2597, df = 5, p-value = 0.66\n\n\nH0 of the ANOVA is that feed has no influence on the chicken weight. As p < 0.05, we reject H0. In the result section, we would write something like: “The feed type significantly influenced the chicken weight (ANOVA, p = 5.94e-10).”\n\n\nfit = aov(weight ~ feed, data = dat)\nsummary(fit)\n##             Df Sum Sq Mean Sq F value   Pr(>F)    \n## feed         5 231129   46226   15.37 5.94e-10 ***\n## Residuals   65 195556    3009                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe non-parametric alternative of an ANOVA is the Kruskal-Wallis test, which should be applied if the data is not normally distributed. In this example, the test comes to the same conclusion: H0 is rejected, the feed type has a significant effect on the chicken weight. The p-value, however, is not as small as in the ANOVA. The reason for this is that non-parametric tests have a lower power than parametric ones as they only use the ranks of the data. Therefore, the ANOVA is preferred over the non-parametric alternative in case its assumptions are fulfilled.\n\n\nkruskal.test(chickwts$weight, chickwts$feed)\n## \n##  Kruskal-Wallis rank sum test\n## \n## data:  chickwts$weight and chickwts$feed\n## Kruskal-Wallis chi-squared = 37.343, df = 5, p-value = 5.113e-07\n\n\nFrom the 15 comparisons among feed types, 8 are significantly different.\n\n\nTukeyHSD(fit)\n##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = weight ~ feed, data = dat)\n## \n## $feed\n##                            diff         lwr       upr     p adj\n## horsebean-casein    -163.383333 -232.346876 -94.41979 0.0000000\n## linseed-casein      -104.833333 -170.587491 -39.07918 0.0002100\n## meatmeal-casein      -46.674242 -113.906207  20.55772 0.3324584\n## soybean-casein       -77.154762 -140.517054 -13.79247 0.0083653\n## sunflower-casein       5.333333  -60.420825  71.08749 0.9998902\n## linseed-horsebean     58.550000  -10.413543 127.51354 0.1413329\n## meatmeal-horsebean   116.709091   46.335105 187.08308 0.0001062\n## soybean-horsebean     86.228571   19.541684 152.91546 0.0042167\n## sunflower-horsebean  168.716667   99.753124 237.68021 0.0000000\n## meatmeal-linseed      58.159091   -9.072873 125.39106 0.1276965\n## soybean-linseed       27.678571  -35.683721  91.04086 0.7932853\n## sunflower-linseed    110.166667   44.412509 175.92082 0.0000884\n## soybean-meatmeal     -30.480519  -95.375109  34.41407 0.7391356\n## sunflower-meatmeal    52.007576  -15.224388 119.23954 0.2206962\n## sunflower-soybean     82.488095   19.125803 145.85039 0.0038845\n\n\nThe experiment did not reveal a significant weight difference between the feed types ‘meatmeal’ and ‘casein’. Remember that we cannot prove or accept H0; we can only reject it.\n\n\n\n\n\n\n3.13.3 Titanic\nThe dataset ‘titanic’ from the EcoData package (not to confuse with the dataset ‘Titanic’) provides information on individual passengers of the Titanic.\n\nlibrary(EcoData) #or: load(\"EcoData.Rdata\"), if you had problems with installing the package\ndat = titanic\n\nAnswer the following questions at elearning-extern.\n\nWe are interested in first and second class differences only. Reduce the dataset to these classes only. How can you do this in R?\nDoes the survival rate between the first and second class differ? Hint: you can apply the test to a contingency table of passenger class versus survived, i.e. table(dat$pclass, dat$survived).\nIs the variable passenger age normally distributed?\nIs the variable Body Identification Number (body) uniformly distributed?\nIs the linear correlation (i.e. Pearson) between fare and age significant?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe dataset can be reduced in different ways. All three options result in a dataset with class 1 and 2 only.\n\n\nlibrary(EcoData)\ndat = titanic\n\ndat = dat[dat$pclass == 1 | dat$pclass == 2, ]\ndat = dat[dat$pclass %in% 1:2, ] # the same\ndat = dat[dat$pclass != 3, ] # the same\n\n\nWe use the test of equal proportions here. H0, proportions in the two groups are equal, is rejected. The survival probability in class 1 and class 2 is significantly different. Note that the estimated proportions are for mortality not for survival because 0=died is in the first column of the table. Thus it is considered the “success” in the prop.test().\n\n\ntable(dat$pclass, dat$survived)\n##    \n##       0   1\n##   1 123 200\n##   2 158 119\nprop.test(table(dat$pclass, dat$survived))\n## \n##  2-sample test for equality of proportions with continuity correction\n## \n## data:  table(dat$pclass, dat$survived)\n## X-squared = 20.772, df = 1, p-value = 5.173e-06\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  -0.2717017 -0.1074826\n## sample estimates:\n##    prop 1    prop 2 \n## 0.3808050 0.5703971\n\n\nThe distribution of passenger age significantly differs from normal.\n\n\nhist(dat$age, breaks = 20)\n\n\n\nshapiro.test(dat$age)\n## \n##  Shapiro-Wilk normality test\n## \n## data:  dat$age\n## W = 0.9876, p-value = 0.00014\n\n\nThe distribution of body significantly differs from uniform.\n\n\nhist(dat$body, breaks = 20)\n\n\n\nks.test(dat$body, \"punif\")\n## \n##  Exact one-sample Kolmogorov-Smirnov test\n## \n## data:  dat$body\n## D = 1, p-value = 2.22e-16\n## alternative hypothesis: two-sided\n\n\nThe correlation between fare and age is non-significant. You can also plot the data using the scatter.smooth function.\n\n\ncor.test(dat$fare, dat$age)\n## \n##  Pearson's product-moment correlation\n## \n## data:  dat$fare and dat$age\n## t = 1.9326, df = 543, p-value = 0.0538\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.001346105  0.165493055\n## sample estimates:\n##        cor \n## 0.08265257\nscatter.smooth(dat$fare, dat$age)\n\n\n\n\n\n\n\n\n\n3.13.4 Simulation of Type I and II error\nThis is an additional task for those who are fast! Please finish the other parts first and submit your solution in elearning-extern before you continue here!\nAnalogously to the previous example of simulating the test statistic, we can also simulate error rates. Complete the code …\n\nPperGroup = 50\npC = 0.5\npT = 0.5\n\npvalues = rep(NA, 1000)\n\nfor(i in 1:1000){\n  control = rbinom(n = 1, size = PperGroup, prob = pC)\n  treat = rbinom(n = 1, size = PperGroup, prob = pT)\n  #XXXX\n}\n\n… and answer the following questions for the prop.test in R:\n\nHow does the distribution of p-values and the number of false positive (Type I error) look like if pC = pT\nHow does the distribution of p-values and the number of true positive (Power) look like if pC != pT, e.g. 0.5, 0.6\nHow does the distribution of p-values and the number of false positive (Type I error) look like if you modify the for loop in a way that you first look at the data, and then decide if you test for greater or less?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAnalogously to our previous example with simulating the test statistic, we can also simulate the error rates. This is the completed code the different examples:\n\npC = pT\n\n\nPperGroup = 50\npC = 0.5\npT = 0.5\n\npvalues = rep(NA, 1000)\npositives = rep(NA, 1000)\n\nfor(i in 1:1000){\n  control = rbinom(1, PperGroup, prob = pC )\n  treatment = rbinom(1, PperGroup, prob = pT )\n  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value\n  positives[i] = pvalues[i] <= 0.05\n}\nhist(pvalues)\n\n\n\ntable(positives)\n## positives\n## FALSE  TRUE \n##   952    48\nmean(positives) \n## [1] 0.048\n\n# type I error rate = false positives (if data simulation etc. is performed several times, this should be on average 0.05 (alpha))\n\n\npC != pT with difference 0.1\n\n\nPperGroup = 50\npC = 0.5\npT = 0.6\n\npvalues = rep(NA, 1000)\npositives = rep(NA, 1000)\n\nfor(i in 1:1000){\n  control = rbinom(1, PperGroup, prob = pC )\n  treatment = rbinom(1, PperGroup, prob = pT )\n  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value\n  positives[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value < 0.05\n}\nhist(pvalues)\n\n\n\ntable(positives)\n## positives\n## FALSE  TRUE \n##   857   143\nmean(pvalues < 0.05) # = power (rate at which effect is detected by the test)\n## [1] 0.143\n# power = 1- beta > beta = 1-power = typeII error rate\n1-mean(pvalues < 0.05)\n## [1] 0.857\n\n## Factors increasing power and reducing type II errors:\n# - increase sample size\n# - larger real effect size (but this is usually fixed by the system)\n\n\nYou first look at the data, and then decide if you test for greater or less:\n\n\n# ifelse(test,yes,no)\nPperGroup = 50\npC = 0.5\npT = 0.5\n\nfor(i in 1:1000){\n  control = rbinom(1, PperGroup, prob = pC )\n  treatment = rbinom(1, PperGroup, prob = pT )\n  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2), \n                        alternative= ifelse(mean(control)>mean(treatment),\n                                            \"greater\",\"less\"))$p.value\n  positives[i] = prop.test(c(control, treatment), rep(PperGroup, 2),\n                     alternative= ifelse(mean(control)>mean(treatment),\n                                         \"greater\",\"less\"))$p.value < 0.05\n}\nhist(pvalues)\n\n\n\ntable(positives)\n## positives\n## FALSE  TRUE \n##   953    47\nmean(pvalues < 0.05) \n## [1] 0.047\n# higher false discovery rate"
  },
  {
    "objectID": "4-SimpleRegression.html#maximum-likelihood-estimator",
    "href": "4-SimpleRegression.html#maximum-likelihood-estimator",
    "title": "4  Simple linear regression",
    "section": "4.1 Maximum Likelihood Estimator",
    "text": "4.1 Maximum Likelihood Estimator\n\\(likelihood = P(D|model, parameter)\\)\nThe likelihood is the probability to observe the Data given a certain model (which is described by its parameter).\nIt is an approach to optimize a model/parameter to find the set of parameters that describes best the observed data.\nA simple example, we want to estimate the average of random vectors and we assume that our model is a normal distribution (so we assume that the data originated from a normal distribution). We want to optimize the two parameters that describe a normal distribution: the mean, and the sd:\n\nXobs = rnorm(100, sd = 1.0)\n# Now we assume that mean = 0, and sd = 0.2 are unknown but we want to find them, let's write the likelihood function:\nlikelihood = function(par) { # we give two parameters, mean and sd\n  lls = dnorm(Xobs, mean = par[1], sd = par[2], log = TRUE) # calculate for each observation to observe the data given our model\n  # we use the logLikilihood for numerical reasons\n  return(sum(lls))\n}\n\nlikelihood(c(0, 0.2))\n## [1] -1274.247\n# let's try all values of sd:\nlikelihood_mean = function(p) likelihood(c(p, 1.0))\nplot(seq(-5, 5.0, length.out = 100), sapply(seq(-5, 5.0, length.out = 100), likelihood_mean), xlab = 'Different mean values', ylab = \"negative logLikelihood\")\n\n\n\n\n# The optimum is at 0, which corresponds to our mean we used to sample Xobs\n\nHowever it is tedious to try all values manually to find the best value, especially if we have to optimize several values. For that we can use an optimizer in R which finds for us the best set of parameters:\n\nopt = optim(c(0.0, 1.0), fn = function(par) -likelihood(par), hessian = TRUE )\n\nWe can use the shape of the likelihood to calculate standard errors for our estimates:\n\nst_errors = sqrt(diag(solve(opt$hessian)))\n\nWith that we can calculate the confidence interval for our estimates. When the estimator is repeatedly used, 95% of the calculated confidence intervals will include the true value!\n\ncbind(opt$par-1.96*st_errors, opt$par+1.96*st_errors)\n##            [,1]      [,2]\n## [1,] -0.1706372 0.2355346\n## [2,]  0.8925465 1.1797585\n\nIn short, if we would do a t.test for our Xobs (to test whether the mean is stat. significant different from zero), the test would be non significant, and a strong indicator for that is when the 0 is within the confidence interval. Let’s compare our CI to the one calculated by the t-test:\n\nt.test(Xobs)\n## \n##  One Sample t-test\n## \n## data:  Xobs\n## t = 0.31224, df = 99, p-value = 0.7555\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  -0.1741130  0.2391426\n## sample estimates:\n##  mean of x \n## 0.03251482\n\nAlmost the same! The t-test also calculates the MLE to get the standard error and the confidence interval."
  },
  {
    "objectID": "4-SimpleRegression.html#the-theory-of-linear-regression",
    "href": "4-SimpleRegression.html#the-theory-of-linear-regression",
    "title": "4  Simple linear regression",
    "section": "4.2 The theory of linear regression",
    "text": "4.2 The theory of linear regression\nIf we want to test for an association between two continuous variables, we can calculate the correlation between the two - and with the cor.test function we can test even for significance. But, the correlation doesn’t report the magnitude, the strength, of the effect:\n\nX = runif(100)\npar(mfrow = c(1,1))\nplot(X, 0.5*X, ylim = c(0, 1), type = \"p\", pch = 15, col = \"red\", xlab = \"X\", ylab = \"Y\")\npoints(X, 1.0*X, ylim = c(0, 1), type = \"p\", pch = 15, col = \"blue\", xlab = \"X\", ylab = \"Y\")\n\n\n\ncor(X, 0.5*X)\n## [1] 1\ncor(X, 1.0*X)\n## [1] 1\n\nBoth have a correlation factor of 1.0! But we see clearly that the blue line has an stronger effect on Y then the red line.\nSolution: Linear regression models\nThey describe the relationship between a dependent variable and one or more explanatory variables:\n\\[\ny = a_0 +a_1*x\n\\]\n(x = explanatory variable; y = dependent variable; a0=intercept; a1= slope)\nExamples:\n\nplot(X, 0.5*X, ylim = c(0, 1), type = \"p\", pch = 16, col = \"black\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\npoints(X, 0.5*X, col = \"red\", type = \"l\", lwd = 1.5)\npoints(X, 1.0*X, ylim = c(0, 1), type = \"p\", pch = 16, col = \"black\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\npoints(X, 1.0*X, ylim = c(0, 1), type = \"l\", pch = 16, col = \"blue\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\nlegend(\"topleft\", col = c(\"red\", \"blue\"), lty = 1,legend = c('Y = 0.5*X+0', 'Y = 1.0**X+0'))\n\n\n\n\nWe can get the parameters (slope and intercept) with the MLE. However, we need first to make another assumptions, usually the model line doesn’t perfectly the data because there is an observational error on Y, so the points scatter around the line:\n\nplot(X, 0.5*X+rnorm(100, sd = 0.05), ylim = c(0, 1), type = \"p\", pch = 16, col = \"black\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\npoints(X, 0.5*X, col = \"red\", type = \"l\", lwd = 1.5)\npoints(X, 1.0*X+rnorm(100, sd = 0.05), ylim = c(0, 1), type = \"p\", pch = 16, col = \"black\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\npoints(X, 1.0*X, ylim = c(0, 1), type = \"l\", pch = 16, col = \"blue\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\nlegend(\"topleft\", col = c(\"red\", \"blue\"), lty = 1,legend = c('Y = 0.5*X+0', 'Y = 1.0**X+0'))\n\n\n\n\nAnd the model extends to:\n\\[\ny = a_0 + a_1*x +\\epsilon, \\epsilon \\sim N(0, sd)\n\\]\nWhich we can also rewrite into:\n\\[\ny = N(a_0 + a_1*x, sd)\n\\]\nWhich is very similar to our previous MLE, right? The only difference is now that the mean depends now on x, let’s optimize it again:\n\nXobs = rnorm(100, sd = 1.0)\nY = Xobs + rnorm(100,sd = 0.2)\nlikelihood = function(par) { # three parameters now\n  lls = dnorm(Y, mean = Xobs*par[2]+par[1], sd = par[3], log = TRUE) # calculate for each observation the probability to observe the data given our model\n  # we use the logLikilihood because of numerical reasons\n  return(sum(lls))\n}\n\nlikelihood(c(0, 0, 0.2))\n## [1] -1162.229\nopt = optim(c(0.0, 0.0, 1.0), fn = function(par) -likelihood(par), hessian = TRUE )\n\nopt$par\n## [1] 0.002927292 0.997608527 0.216189328\n\nOur true parameters are 0.0 for the intercept, 1.0 for the slope, and 0.22 for the sd of the observational error.\nNow, we want to test whether the effect (slope) is statistically significant different from 0:\n\ncalculate standard error\ncalculate t-statistic\ncalculate p-value\n\n\nst_errors = sqrt(diag(solve(opt$hessian)))\nst_errors[2]\n## [1] 0.02226489\nt_statistic = opt$par[2] / st_errors[2]\npt(t_statistic, df = 100-3, lower.tail = FALSE)*2\n## [1] 1.264962e-66\n\nThe p-value is smaller than \\(\\alpha\\), so the effect is significant! However, it would be tedious to do that always by hand, and because it is probably one of the most used analysis, there’s a function for it in R:\n\nmodel = lm(Y~Xobs) # 1. Get estimates, MLE\nmodel\n## \n## Call:\n## lm(formula = Y ~ Xobs)\n## \n## Coefficients:\n## (Intercept)         Xobs  \n##    0.002927     0.997569\nsummary(model) # 2. Calculate standard errors, CI, and p-values\n## \n## Call:\n## lm(formula = Y ~ Xobs)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.49919 -0.13197 -0.01336  0.14239  0.64505 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 0.002927   0.021838   0.134    0.894    \n## Xobs        0.997569   0.022490  44.355   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2184 on 98 degrees of freedom\n## Multiple R-squared:  0.9526, Adjusted R-squared:  0.9521 \n## F-statistic:  1967 on 1 and 98 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "4-SimpleRegression.html#understanding-the-linear-regression",
    "href": "4-SimpleRegression.html#understanding-the-linear-regression",
    "title": "4  Simple linear regression",
    "section": "4.3 Understanding the linear regression",
    "text": "4.3 Understanding the linear regression\n\nBesides the MLE, there are also several tests in a regression. The most important are\n\nsignificance of each parameter. \u000bt-test: H0 = variable has no effect, that means the estimator for the parameter is 0\u000b\nsignificance of the model. \u000bF-test: H0 = none of the explanatory variables has an effect, that means all estimators are 0\n\nExample:\n\npairs(airquality)\n\n\n\n# first think about what is explanatory / predictor \n# and what is the dependent variable (e.g. in Ozone and Temp)\n\n# par(mfrow = c(1, 1))\nplot(Ozone ~ Temp, data = airquality)\n\n\n\nfit1 = lm(Ozone ~ Temp, data = airquality)\nsummary(fit1)\n## \n## Call:\n## lm(formula = Ozone ~ Temp, data = airquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -40.729 -17.409  -0.587  11.306 118.271 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\n## Temp           2.4287     0.2331  10.418  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 23.71 on 114 degrees of freedom\n##   (37 observations deleted due to missingness)\n## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 \n## F-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n# gives a negative values for the intercept = negative Ozone levels when Temp = 0\n# this does not make sense (>extrapolation)\n\n# we can also fit a model without intercept, \n# without means: intercept = 0; y = a*x \n# although this doesn't make much sense here\nfit2 = lm(Ozone ~ Temp - 1, data = airquality)\nsummary(fit2)\n## \n## Call:\n## lm(formula = Ozone ~ Temp - 1, data = airquality)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -38.47 -23.26 -12.46  15.15 121.96 \n## \n## Coefficients:\n##      Estimate Std. Error t value Pr(>|t|)    \n## Temp  0.56838    0.03498   16.25   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 29.55 on 115 degrees of freedom\n##   (37 observations deleted due to missingness)\n## Multiple R-squared:  0.6966, Adjusted R-squared:  0.6939 \n## F-statistic:   264 on 1 and 115 DF,  p-value: < 2.2e-16\n\nplot(Ozone ~ Temp, data = airquality, xlim = c(0,100), ylim = c(-150, 150))\nabline(fit1, col = \"green\")\nabline(fit2, col = \"red\", lty = 2)\n\n\n\n\n# there is no need to check normality of Ozone\nhist(airquality$Ozone) # this is not normal, and that's no problem !\n\n\n\n\n\n4.3.1 Model diagnostics\nThe regression optimizes the parameters under the condition that the model is correct (e.g. there is really a linear relationship). If the model is not specified correctly, the parameter values are still estimated - to the best of the model’s ability, but the result will be misleading, e.g. p-values and effect sizes\nWhat could be wrong:\n\nthe distribution (e.g. error not normal)\nthe shape of the relationship between explanatory variable and dependent variable (e.g., could be non-linear)\n\nThe model’s assumptions must always be checked!\nWe can check the model by looking at the residuals (which are predicted - observed values) which should be normally distributed and should show no patterns:\n\nX = runif(50)\nY = X + rnorm(50, sd = 0.2)\nfit = lm(Y~X)\npar(mfrow = c(1, 3))\nplot(X, Y)\nabline(fit, col = \"red\")\nplot(X, predict(fit) - Y, ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\nhist(predict(fit) - Y, main = \"\", xlab = \"Residuals\")\n\n\n\npar(mfrow = c(1,1))\n\nThe residuals should match the model assumptions. For linear regression:\n\nnormal distribution\nconstant variance\nindependence of the data points\n\nExample:\n\nfit1 = lm(Ozone~Temp, data = airquality)\nresiduals(fit1)\n##           1           2           3           4           6           7 \n##  25.2723695   8.1288530 -20.7285536  14.4158861  14.7010729  12.1297762 \n##           8           9          11          12          13          14 \n##  22.7019960   6.8445894 -25.7285536  -4.5850371  -2.2989271  -4.1563338 \n##          15          16          17          18          19          20 \n##  24.1306993   5.5584795  20.7010729  14.5594026  11.8436662   7.4158861 \n##          21          22          23          24          28          29 \n##   4.7019960 -19.2998503   2.8445894  30.8445894   7.2723695  -4.7294767 \n##          30          31          38          40          41          44 \n##  70.1279299  -0.5859602 -23.1581800  -0.5878065 -25.3016966 -29.1581800 \n##          47          48          49          50          51          62 \n## -19.0146635   9.1288530   9.1297762 -18.2998503 -24.5859602  77.9844134 \n##          63          64          66          67          68          69 \n## -10.4442899 -17.7294767   9.4131167 -14.5868833  10.2696001  20.5547869 \n##          70          71          73          74          76          77 \n##  20.5547869  15.8408968 -20.2998503 -22.7294767 -40.3007734  -1.7294767 \n##          78          79          80          81          82          85 \n## -17.1581800   3.9844134  14.6983034   3.5557101 -16.7285536  18.1270068 \n##          86          87          88          89          90          91 \n##  48.5557101 -32.1581800  -9.8729932  15.2696001 -11.8729932   9.4131167 \n##          92          93          94          95          96          97 \n##   9.2705233 -10.7294767 -40.7294767 -36.1581800  16.1270068 -24.4442899 \n##          98          99         100         101         104         105 \n##   1.6983034  52.8408968  17.4121935  38.4121935 -17.8729932 -24.1581800 \n##         106         108         109         110         111         112 \n##  17.6992266 -18.0146635  14.1279299 -14.5859602 -11.4433668   1.5566332 \n##         113         114         116         117         118         120 \n## -19.0146635 -18.8711470   0.1279299 118.2705233  11.1270068 -12.5887296 \n##         121         122         123         124         125         126 \n##  36.6973803  -2.1600263   3.6973803  21.9834902   1.5547869  -5.8739164 \n##         127         128         129         130         131         132 \n##  12.1260836 -17.3016966 -25.0155866 -27.3007734 -19.4433668 -14.1572569 \n##         133         134         135         136         137         138 \n##  -6.2998503  -5.7294767 -16.5859602 -12.0146635 -16.4424437 -12.4424437 \n##         139         140         141         142         143         144 \n##   3.5566332   2.2723695 -24.5859602   5.8436662 -36.1581800   4.5584795 \n##         145         146         147         148         149         151 \n##  -2.4424437 -13.7294767 -13.5850371   7.9871828   6.9862596 -21.1572569 \n##         152         153 \n## -19.5859602   1.8436662\nhist(residuals(fit1))\n\n\n\n# residuals are not normally distributed\n# we do not use a test for this, but instead look at the residuals visually\n\n# let's plot residuals versus predictor\nplot(airquality$Temp[!is.na(airquality$Ozone)], residuals(fit1))\n\n\n\n\n# model checking plots\noldpar= par(mfrow = c(2,2))\nplot(fit1)\n\n\n\npar(oldpar)\n#> there's a pattern in the residuals > the model does not fit very well!\n\n\n\n4.3.2 Linear regression with a categorical variable\nWe can also use categorical variables as an explanatory variable:\n\nm = lm(weight~group, data = PlantGrowth)\nsummary(m)\n## \n## Call:\n## lm(formula = weight ~ group, data = PlantGrowth)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.0710 -0.4180 -0.0060  0.2627  1.3690 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   5.0320     0.1971  25.527   <2e-16 ***\n## grouptrt1    -0.3710     0.2788  -1.331   0.1944    \n## grouptrt2     0.4940     0.2788   1.772   0.0877 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6234 on 27 degrees of freedom\n## Multiple R-squared:  0.2641, Adjusted R-squared:  0.2096 \n## F-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\nThe lm estimates an effect/intercept for each level in the categorical variable. The first level of the categorical variable is used as a reference, i.e. the true effect for grouptrt1 is Intercept+grouptrt1 = 4.661 and grouptrt2 is 5.5242. Moreover, the lm tests for a difference of the reference to the other levels. So with this model we know whether the control is significant different from treatment 1 and 2 but we cannot say anything about the difference between trt1 and trt2.\nIf we are interested in testing trt1 vs trt2 we can, for example, change the reference level of our variable:\n\ntmp = PlantGrowth\ntmp$group = relevel(tmp$group, ref = \"trt1\")\nm = lm(weight~group, data = tmp)\nsummary(m)\n## \n## Call:\n## lm(formula = weight ~ group, data = tmp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.0710 -0.4180 -0.0060  0.2627  1.3690 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   4.6610     0.1971  23.644  < 2e-16 ***\n## groupctrl     0.3710     0.2788   1.331  0.19439    \n## grouptrt2     0.8650     0.2788   3.103  0.00446 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6234 on 27 degrees of freedom\n## Multiple R-squared:  0.2641, Adjusted R-squared:  0.2096 \n## F-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\nAnother example:\n\nlibrary(effects)\n## Loading required package: carData\n## lattice theme set by effectsTheme()\n## See ?effectsTheme for details.\nlibrary(jtools)\n\nsummary(chickwts)\n##      weight             feed   \n##  Min.   :108.0   casein   :12  \n##  1st Qu.:204.5   horsebean:10  \n##  Median :258.0   linseed  :12  \n##  Mean   :261.3   meatmeal :11  \n##  3rd Qu.:323.5   soybean  :14  \n##  Max.   :423.0   sunflower:12\n\nplot(weight ~ feed, chickwts)\n\n\n\nfit4 = lm(weight ~ feed, chickwts)\n\nsummary(fit4)\n## \n## Call:\n## lm(formula = weight ~ feed, data = chickwts)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -123.909  -34.413    1.571   38.170  103.091 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)    323.583     15.834  20.436  < 2e-16 ***\n## feedhorsebean -163.383     23.485  -6.957 2.07e-09 ***\n## feedlinseed   -104.833     22.393  -4.682 1.49e-05 ***\n## feedmeatmeal   -46.674     22.896  -2.039 0.045567 *  \n## feedsoybean    -77.155     21.578  -3.576 0.000665 ***\n## feedsunflower    5.333     22.393   0.238 0.812495    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 54.85 on 65 degrees of freedom\n## Multiple R-squared:  0.5417, Adjusted R-squared:  0.5064 \n## F-statistic: 15.36 on 5 and 65 DF,  p-value: 5.936e-10\nanova(fit4) #get overall effect of feeding treatment\n## Analysis of Variance Table\n## \n## Response: weight\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## feed       5 231129   46226  15.365 5.936e-10 ***\n## Residuals 65 195556    3009                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(allEffects(fit4))\n\n\n\nplot(allEffects(fit4, partial.residuals = T))\n\n\n\neffect_plot(fit4, pred = feed, interval = TRUE, plot.points = F)\n\n\n\n\nold.par = par(mfrow = c(2, 2))\nplot(fit4)\n\n\n\npar(old.par)\n\nboxplot(residuals(fit4) ~ chickwts$feed)\n\n\n\n\n\n\n4.3.3 Linear regression with a quadratic term\n\n## what does simple linear regression mean?\n# simple = one predictor!\n# linear = linear in the parameters\n# a0 + a1 * x + a2 * x^2 \n# even if we add a quadratic term, this is a linear combination\n# this is called polynomial\n\nfit3 = lm(Ozone ~ Temp + I(Temp^2), data = airquality)\nsummary(fit3)\n## \n## Call:\n## lm(formula = Ozone ~ Temp + I(Temp^2), data = airquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -37.619 -12.513  -2.736   9.676 123.909 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 305.48577  122.12182   2.501 0.013800 *  \n## Temp         -9.55060    3.20805  -2.977 0.003561 ** \n## I(Temp^2)     0.07807    0.02086   3.743 0.000288 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 22.47 on 113 degrees of freedom\n##   (37 observations deleted due to missingness)\n## Multiple R-squared:  0.5442, Adjusted R-squared:  0.5362 \n## F-statistic: 67.46 on 2 and 113 DF,  p-value: < 2.2e-16\n\noldpar= par(mfrow = c(2,2))\nplot(fit3)\n\n\n\npar(oldpar)\n\n\n# Residual vs. fitted looks okay, but Outliers are still there, and additionally\n# too wide. But for now, let's plot prediction with uncertainty (plot line plus confidence interval)\n\nplot(Ozone ~ Temp, data = airquality)\n\n# if the relationship between x and y is not linear, we cannot use abline\n# instead we predict values of x for different values of y based on the model \nnewDat = data.frame(Temp = 55:100)\npredictions = predict(fit3, newdata = newDat, se.fit = T)\n# and plot these into our figure:\nlines(newDat$Temp, predictions$fit, col= \"red\")\n# let's also plot the confidence intervals:\nlines(newDat$Temp, predictions$fit + 1.96*predictions$se.fit, col= \"red\", lty = 2)\nlines(newDat$Temp, predictions$fit - 1.96*predictions$se.fit, col= \"red\", lty = 2)\n\n# add a polygon (shading for confidence interval)\nx = c(newDat$Temp, rev(newDat$Temp))\ny = c(predictions$fit - 1.96*predictions$se.fit, \n      rev(predictions$fit + 1.96*predictions$se.fit))\n\npolygon(x,y, col=\"#99009922\", border = F )\n\n\n\n\n\n# alternative: use package effects\n#install.packages(\"effects\")\nlibrary(effects)\nplot(allEffects(fit3))\n\n\n\nplot(allEffects(fit3, partial.residuals = T)) \n\n\n\n#to check patterns in residuals (plots measurements and partial residuals)\n\n# or jtools package\nlibrary(jtools)\neffect_plot(fit3, pred = Temp, interval = TRUE, plot.points = TRUE)"
  },
  {
    "objectID": "4-SimpleRegression.html#exercises",
    "href": "4-SimpleRegression.html#exercises",
    "title": "4  Simple linear regression",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\nYou will work with the following datasets:\n\nregrowth {EcoData}\nbirdabundance {EcoData}\nsimulated data\n\n\n4.4.1 Analyzing the “regrowth” dataset\nImagine you have a garden with some fruit trees and you were thinking of adding some berry bushes between them. However, you don’t want them to suffer from malnutrition so you want to estimate the volume of root biomass as a function of the fruit biomass.\nCarry out the following tasks\n\nPerform a simple linear regression for the influence of fruit biomass on root biomass.\nVisualize the data and add the regression line to the plot.\n\nYou will need the following functions:\n\nlm()\nsummary()\nplot()\nabline()\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse your results to chose the correct statement(s) on elearning-extern (Q1) (“04_Test for Exercise in R - simple regression”).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the code that you need to interpret the results.\n\nlibrary(EcoData)\n# simple linear regression\nfit <- lm(Root ~ Fruit, data = regrowth)\n\n# check summary for regression coefficient and p-value\nsummary(fit)\n## \n## Call:\n## lm(formula = Root ~ Fruit, data = regrowth)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.25105 -0.69970 -0.01755  0.66982  1.63933 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 4.184256   0.337987  12.380  6.6e-15 ***\n## Fruit       0.050444   0.005264   9.584  1.1e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8111 on 38 degrees of freedom\n## Multiple R-squared:  0.7073, Adjusted R-squared:  0.6996 \n## F-statistic: 91.84 on 1 and 38 DF,  p-value: 1.099e-11\n\n# plot root explained by fruit biomass\nplot(Root ~ Fruit, data = regrowth, \n     ylab = \"Root biomass in cubic meters\",\n     xlab = \"Fruit biomass in g\")\n\nabline(fit) # add regression line\nabline(v = 70, col = \"purple\") # add line at x value (here fruit biomass of 70g)\nabline(h = 4.184256 + 0.050444*70, col = \"brown\") # add line at y value according to x = 70 using the intercept and regression coefficient of x\n\n\n\n\n\n\n\n\n\n4.4.2 Analyzing the “birdabundance” dataset\nThe dataset provides bird abundances in forest fragments with different characteristics in Australia. We want to look at the relationship of the variables “abundance”, “distance” and “grazing”.\n\n\n\n\n\n\nQuestions\n\n\n\nFirst, answer the following questions on elearning-extern (Q 2-4):\n\nWhat is the most reasonable research question regarding these variables?\nWhat is the response variable?\nWhat is the predictor variable?\n\nThen, perform the following tasks:\n\nFit a simple linear regression relating the response variable to the categorical predictor (that is the one with five levels, make sure that it is indeed a factor using as.factor())\nApply an ANOVA to your model.\n\nYou may need the following functions:\n\nlm()\nsummary()\nanova()\n\nUse your results to chose the correct statement(s) on elearning-extern (Q5).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA reasonable research question is how abundance is influenced by distance and/or grazing. Here, the response variable is abundance, while the predictors are distance and/or grazing.\nThis is the code that you need to interpret the results.\n\n# change variable from integer to factor\nbirdabundance$GRAZE <- as.factor(birdabundance$GRAZE) \nfit <- lm(ABUND ~ GRAZE, data = birdabundance)\nsummary(fit)\n## \n## Call:\n## lm(formula = ABUND ~ GRAZE, data = birdabundance)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -18.3867  -4.1159   0.0269   5.1484  16.4133 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   28.623      2.086  13.723  < 2e-16 ***\n## GRAZE2        -6.673      3.379  -1.975   0.0537 .  \n## GRAZE3        -7.336      2.850  -2.574   0.0130 *  \n## GRAZE4        -8.052      3.526  -2.284   0.0266 *  \n## GRAZE5       -22.331      2.950  -7.571 6.85e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.52 on 51 degrees of freedom\n## Multiple R-squared:  0.5449, Adjusted R-squared:  0.5092 \n## F-statistic: 15.27 on 4 and 51 DF,  p-value: 2.846e-08\n\n# anova to check global effect of the factor grazing intensity\nanova(fit)\n## Analysis of Variance Table\n## \n## Response: ABUND\n##           Df Sum Sq Mean Sq F value    Pr(>F)    \n## GRAZE      4 3453.7  863.42  15.267 2.846e-08 ***\n## Residuals 51 2884.2   56.55                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# boxplot\nplot(ABUND ~ GRAZE, data = birdabundance)\n\n\n\n\n\n\n\n\n\n4.4.3 Model validation: Residual checks\nNow, we will have a closer look at model diagnostics and residual checks in particular. Of course, we should have done this for all models above as well (we simply didn’t do this because of time restrictions). So remember that you always have to validate your model, if you want to be sure that your conclusions are correct.\nFor this exercise, you can prepare a dataset yourself called “dat” with the variables “x” and “y”. Simply copy the following code to generate the data:\n\nset.seed(234)\nx = rnorm(40, mean = 10, sd = 5)\ny = 10 - 2*x + 0.9 * x^2 + rnorm(40, mean=5, sd = 20)\ndat <- data.frame(x, y)\nhead(dat)\n##           x          y\n## 1 13.303849 152.093910\n## 2 -0.264915   6.831275\n## 3  2.503970  45.207691\n## 4 17.356166 240.274237\n## 5 17.295693 240.917066\n## 6 10.700695 117.691234\n\nPerform the following tasks:\n\nFit a simple linear regression.\nCheck the residuals.\nPerform another simple linear regression with a modified formula, if needed.\nCreate a scatter plot of the data and add a regression line for the first fit in black and one for the second fit in red. The second model cannot be plotted with the abline() function. Use the following code instead:\n\n\nlines(sort(x), predict(fit2, newdata = data.frame(x = sort(x))), col = \"red\")\n\nYou may also need the following functions:\n\nlm()\nsummary()\npar(mfrow = c(2, 2))\nplot()\nabline()\n\nUse your results to answer the following questions on elearning-extern (Q 6-8).\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat pattern do the residuals of the first regression model show when plotted against the fitted values?\nWhat do you have to do to improve your first regression model?\nIdentify the correct statement(s) about the residuals of the modified model.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nset.seed(234)\nx = rnorm(40, mean = 10, sd = 5)\ny = 10 - 2*x + 0.9 * x^2 + rnorm(40, mean=5, sd = 20)\ndat <- data.frame(x, y)\n\n# simple linear regression\nfit <- lm(y ~ x, dat)\n\n# check residuals\nop = par(mfrow=c(2,2))\nplot(fit) # residuals show a parabolic relationship (see first plot)  -> to improve, fit a quadratic relationship\n\n\n\npar(op)\n\n# scatter plot\nplot(y ~ x, data = dat)\nabline(fit)\n\n\n\n\nsummary(fit) # significantly positively correlated, but this doesn't tell the full story because the residuals are not okay\n## \n## Call:\n## lm(formula = y ~ x, data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -39.884 -22.208  -4.948  10.602 118.164 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   -8.459     10.973  -0.771    0.446    \n## x             11.465      1.019  11.248 1.18e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 32.11 on 38 degrees of freedom\n## Multiple R-squared:  0.769,  Adjusted R-squared:  0.763 \n## F-statistic: 126.5 on 1 and 38 DF,  p-value: 1.176e-13\n\n# improved regression model\nfit2 = lm(y ~ x + I(x^2), dat)\n\n# check residuals\nop = par(mfrow=c(2,2))\nplot(fit2) # no pattern in residuals anymore (first plot) -> fit is fine\n\n\n\npar(op)\n\n# scatter plot\nplot(y ~ x, data = dat)\nabline(fit)\nlines(sort(x), predict(fit2, newdata = data.frame(x = sort(x))), col = \"red\")\n\n\n\n\n\nsummary(fit2) # significantly negatively correlated, trustworthy now, because residuals are sufficiently uniformly distributed (first plot in plot(fit2))\n## \n## Call:\n## lm(formula = y ~ x + I(x^2), data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -33.174 -11.444   0.938  10.164  40.666 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 17.87505    6.00812   2.975  0.00513 ** \n## x           -1.10100    1.27706  -0.862  0.39417    \n## I(x^2)       0.80752    0.07526  10.730 6.49e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 16.05 on 37 degrees of freedom\n## Multiple R-squared:  0.9438, Adjusted R-squared:  0.9408 \n## F-statistic: 310.9 on 2 and 37 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "5-MultipleRegression.html#confounder",
    "href": "5-MultipleRegression.html#confounder",
    "title": "5  Multiple regression",
    "section": "5.1 Confounder",
    "text": "5.1 Confounder\nConfounders have effects on the response and another predictor.\n\nClimate = runif(100)\nTemp = Climate + rnorm(100, sd = 0.2)\nGrowth = 0.5*Temp - 1.0*Climate + rnorm(100, sd = 0.2)\n\nsummary(lm(Growth~Temp))\n## \n## Call:\n## lm(formula = Growth ~ Temp)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.55719 -0.18748 -0.01354  0.18858  0.59337 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.16604    0.04228  -3.927  0.00016 ***\n## Temp        -0.19311    0.06602  -2.925  0.00428 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2472 on 98 degrees of freedom\n## Multiple R-squared:  0.0803, Adjusted R-squared:  0.07091 \n## F-statistic: 8.556 on 1 and 98 DF,  p-value: 0.004279\nsummary(lm(Growth~Temp+Climate)) # correct effects!!\n## \n## Call:\n## lm(formula = Growth ~ Temp + Climate)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.41912 -0.13228 -0.00661  0.12988  0.41630 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.009234   0.038203   0.242     0.81    \n## Temp         0.568083   0.102652   5.534 2.66e-07 ***\n## Climate     -1.088041   0.127964  -8.503 2.27e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1881 on 97 degrees of freedom\n## Multiple R-squared:  0.473,  Adjusted R-squared:  0.4622 \n## F-statistic: 43.54 on 2 and 97 DF,  p-value: 3.205e-14\n\nIdentifying confounders is the most important challenge in observational studies: For example, several studies showed that overweight adults have lower mortality. However, another study shows that these earlier results might have come up due to confounding: smoking!\n\nsmokers: higher mortality and lower BMI -> people with lower BMI have higher mortality rates\nWhen we correct for the confounder smoking, the correlation between BMI and mortality goes in the other direction, i.e. obese people have higher mortality!\n\nConfounders can even lead to observed correlations where in reality there is no such correlation. This is called spurious correlation.\n\n\n\n\n\n\nWarning\n\n\n\nConclusion: Confounders can cause correlations where no causal relationship exists."
  },
  {
    "objectID": "5-MultipleRegression.html#multiple-lm",
    "href": "5-MultipleRegression.html#multiple-lm",
    "title": "5  Multiple regression",
    "section": "5.2 Multiple LM",
    "text": "5.2 Multiple LM\nThe multiple linear regression can deal with confounders:\n\nUnivariate (simple) linear regression describes how y depends on x using a polynomial of x1 e.g.: \\[\ny = a_0 + a_1*x_1 + a_2*x_1^2\n\\]\nMultiple linear regression expands simple linear regression to a polynomial of several explanatory variables x1, x2… e.g.: \\[\ny = a_0 + a_1*x_1 + a_2*x_2 + a_3*x_3\n\\]\nIdea: if we jointly consider “all” variables in the model formula, the influence of confounding variables is incorporated\n\n\n## first remove observations with NA values\nnewAirquality = airquality[complete.cases(airquality),]\nsummary(newAirquality)\n##      Ozone          Solar.R           Wind            Temp      \n##  Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n##  1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n##  Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n##  Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n##  3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n##  Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n##      Month            Day       \n##  Min.   :5.000   Min.   : 1.00  \n##  1st Qu.:6.000   1st Qu.: 9.00  \n##  Median :7.000   Median :16.00  \n##  Mean   :7.216   Mean   :15.95  \n##  3rd Qu.:9.000   3rd Qu.:22.50  \n##  Max.   :9.000   Max.   :31.00\n\n# simple regression\nm0 = lm(Ozone ~ Temp , data = newAirquality)\nsummary(m0)\n## \n## Call:\n## lm(formula = Ozone ~ Temp, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -40.922 -17.459  -0.874  10.444 118.078 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -147.6461    18.7553  -7.872 2.76e-12 ***\n## Temp           2.4391     0.2393  10.192  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 23.92 on 109 degrees of freedom\n## Multiple R-squared:  0.488,  Adjusted R-squared:  0.4833 \n## F-statistic: 103.9 on 1 and 109 DF,  p-value: < 2.2e-16\nplot(m0)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(Ozone ~ Temp , data = newAirquality)\nabline(m0, col = \"blue\", lwd = 3)\n\n\n\n\n# Today: multiple linear regression\nm1 = lm(Ozone ~ Temp + Wind , data = newAirquality)\n# have a look at the residuals:\nop <- par(mfrow = c(2,2))\nplot(m1)\n\n\n\npar(op)\n\nsummary(m1)\n## \n## Call:\n## lm(formula = Ozone ~ Temp + Wind, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -42.156 -13.216  -3.123  10.598  98.492 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -67.3220    23.6210  -2.850  0.00524 ** \n## Temp          1.8276     0.2506   7.294 5.29e-11 ***\n## Wind         -3.2948     0.6711  -4.909 3.26e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 21.73 on 108 degrees of freedom\n## Multiple R-squared:  0.5814, Adjusted R-squared:  0.5736 \n## F-statistic: 74.99 on 2 and 108 DF,  p-value: < 2.2e-16\n\n# plotting multiple regression outputs\nlibrary(effects)\n## Loading required package: carData\n## lattice theme set by effectsTheme()\n## See ?effectsTheme for details.\nplot(allEffects(m1))\n\n\n\n\n\n## Omitted variable bias\nboth = lm(Ozone ~ Wind + Temp, newAirquality)\nwind = lm(Ozone ~ Wind , newAirquality)\ntemp = lm(Ozone ~ Temp, newAirquality)\nsummary(both)\n## \n## Call:\n## lm(formula = Ozone ~ Wind + Temp, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -42.156 -13.216  -3.123  10.598  98.492 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -67.3220    23.6210  -2.850  0.00524 ** \n## Wind         -3.2948     0.6711  -4.909 3.26e-06 ***\n## Temp          1.8276     0.2506   7.294 5.29e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 21.73 on 108 degrees of freedom\n## Multiple R-squared:  0.5814, Adjusted R-squared:  0.5736 \n## F-statistic: 74.99 on 2 and 108 DF,  p-value: < 2.2e-16\nsummary(wind)\n## \n## Call:\n## lm(formula = Ozone ~ Wind, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -43.513 -18.597  -5.035  15.814  88.437 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  99.0413     7.4724   13.25  < 2e-16 ***\n## Wind         -5.7288     0.7082   -8.09 9.09e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 26.42 on 109 degrees of freedom\n## Multiple R-squared:  0.3752, Adjusted R-squared:  0.3694 \n## F-statistic: 65.44 on 1 and 109 DF,  p-value: 9.089e-13\n\nslopes <- data.frame(\n  predictor = c(\"Wind\", \"Temp\"),\n  both.pred = round(coef(both)[2:3], digits = 2),\n  only.wind = c(round(coef(wind)[2], digits = 2), \"NA\"),\n  only.temp = c(\"NA\", round(coef(temp)[2], digits = 2))\n)\nslopes\n##      predictor both.pred only.wind only.temp\n## Wind      Wind     -3.29     -5.73        NA\n## Temp      Temp      1.83        NA      2.44\n\nOmitting Wind makes the effect of Temperature larger.\nProblem: Multiple regression can separate the effect of collinear explanatory variables, but only, if collinearity is not too strong.\nSolution: If the correlation is really strong, we can omit one variable and interpret the remaining collinear variable as representing both."
  },
  {
    "objectID": "5-MultipleRegression.html#interactions-between-variables",
    "href": "5-MultipleRegression.html#interactions-between-variables",
    "title": "5  Multiple regression",
    "section": "5.3 Interactions between variables",
    "text": "5.3 Interactions between variables\nIf one predictor influences the effect of the other predictor, we can include an interaction term into our model:\n\\[\ny \\sim a + b + a:b\n\\]\nor:\n\\[\ny \\sim a*b\n\\]\n\n# Include interaction\nm2 = lm(Ozone ~  scale(Wind)* scale(Temp) , data = newAirquality)\n# if including interactions, always scale your predictor variables!\n# scale: subtracts the mean and divides by standard deviation\nsummary(m2)\n## \n## Call:\n## lm(formula = Ozone ~ scale(Wind) * scale(Temp), data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -40.930 -11.193  -3.034   8.193  97.456 \n## \n## Coefficients:\n##                         Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)               38.469      2.137  18.002  < 2e-16 ***\n## scale(Wind)              -11.758      2.238  -5.253 7.68e-07 ***\n## scale(Temp)               17.544      2.239   7.837 3.62e-12 ***\n## scale(Wind):scale(Temp)   -7.367      1.848  -3.987 0.000123 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 20.37 on 107 degrees of freedom\n## Multiple R-squared:  0.6355, Adjusted R-squared:  0.6253 \n## F-statistic: 62.19 on 3 and 107 DF,  p-value: < 2.2e-16\nop <- par(mfrow = c(2,2))\nplot(m2)\n\n\n\npar(op)\n\nThe influence of temperature on growth depends on the amount of precipitation, or: If there’s not enough water, also higher temperatures cannot increase growth.\nExample:\n\n# How does everything change, if we have factorial predictors?\nnewAirquality$MonthFactor = as.factor(newAirquality$Month)\n\nm4 = lm(sqrt(Ozone) ~ MonthFactor + scale(Wind) * scale(Temp) * scale(Solar.R) , \n        data = newAirquality)\nsummary(m4)\n## \n## Call:\n## lm(formula = sqrt(Ozone) ~ MonthFactor + scale(Wind) * scale(Temp) * \n##     scale(Solar.R), data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.6096 -0.8869 -0.2067  0.7647  4.3191 \n## \n## Coefficients:\n##                                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                             6.12172    0.37148  16.479  < 2e-16 ***\n## MonthFactor6                           -0.54487    0.60633  -0.899 0.371025    \n## MonthFactor7                           -0.37571    0.51347  -0.732 0.466072    \n## MonthFactor8                           -0.03770    0.52839  -0.071 0.943262    \n## MonthFactor9                           -0.74343    0.43308  -1.717 0.089179 .  \n## scale(Wind)                            -0.76983    0.16456  -4.678 9.18e-06 ***\n## scale(Temp)                             1.35350    0.20937   6.465 3.86e-09 ***\n## scale(Solar.R)                          0.65689    0.16212   4.052 0.000101 ***\n## scale(Wind):scale(Temp)                -0.30440    0.14655  -2.077 0.040379 *  \n## scale(Wind):scale(Solar.R)             -0.07695    0.17222  -0.447 0.655999    \n## scale(Temp):scale(Solar.R)              0.22985    0.15451   1.488 0.140040    \n## scale(Wind):scale(Temp):scale(Solar.R)  0.03202    0.15179   0.211 0.833366    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.328 on 99 degrees of freedom\n## Multiple R-squared:  0.7335, Adjusted R-squared:  0.7039 \n## F-statistic: 24.78 on 11 and 99 DF,  p-value: < 2.2e-16\n\nm5 = lm(sqrt(Ozone) ~ MonthFactor + scale(Wind) + scale(Temp) + scale(Solar.R) \n                      + scale(Wind):scale(Temp)\n                      + scale(Wind):scale(Solar.R)\n                      + scale(Temp):scale(Solar.R), \n        data = newAirquality)\nsummary(m5)\n## \n## Call:\n## lm(formula = sqrt(Ozone) ~ MonthFactor + scale(Wind) + scale(Temp) + \n##     scale(Solar.R) + scale(Wind):scale(Temp) + scale(Wind):scale(Solar.R) + \n##     scale(Temp):scale(Solar.R), data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.6023 -0.9182 -0.2180  0.7713  4.3209 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                 6.12350    0.36960  16.568  < 2e-16 ***\n## MonthFactor6               -0.54871    0.60315  -0.910   0.3652    \n## MonthFactor7               -0.39194    0.50524  -0.776   0.4397    \n## MonthFactor8               -0.04701    0.52402  -0.090   0.9287    \n## MonthFactor9               -0.74873    0.43028  -1.740   0.0849 .  \n## scale(Wind)                -0.75588    0.14997  -5.040 2.07e-06 ***\n## scale(Temp)                 1.35192    0.20823   6.492 3.29e-09 ***\n## scale(Solar.R)              0.65178    0.15953   4.086 8.88e-05 ***\n## scale(Wind):scale(Temp)    -0.31305    0.14002  -2.236   0.0276 *  \n## scale(Wind):scale(Solar.R) -0.09259    0.15469  -0.599   0.5508    \n## scale(Temp):scale(Solar.R)  0.23573    0.15126   1.558   0.1223    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.321 on 100 degrees of freedom\n## Multiple R-squared:  0.7334, Adjusted R-squared:  0.7068 \n## F-statistic: 27.51 on 10 and 100 DF,  p-value: < 2.2e-16\n\n# short form for including only two-way interactions:\n\nm5 = lm(sqrt(Ozone) ~ MonthFactor + (scale(Wind) + scale(Temp) + scale(Solar.R))^2,\n        data = newAirquality)\nsummary(m5)\n## \n## Call:\n## lm(formula = sqrt(Ozone) ~ MonthFactor + (scale(Wind) + scale(Temp) + \n##     scale(Solar.R))^2, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.6023 -0.9182 -0.2180  0.7713  4.3209 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)                 6.12350    0.36960  16.568  < 2e-16 ***\n## MonthFactor6               -0.54871    0.60315  -0.910   0.3652    \n## MonthFactor7               -0.39194    0.50524  -0.776   0.4397    \n## MonthFactor8               -0.04701    0.52402  -0.090   0.9287    \n## MonthFactor9               -0.74873    0.43028  -1.740   0.0849 .  \n## scale(Wind)                -0.75588    0.14997  -5.040 2.07e-06 ***\n## scale(Temp)                 1.35192    0.20823   6.492 3.29e-09 ***\n## scale(Solar.R)              0.65178    0.15953   4.086 8.88e-05 ***\n## scale(Wind):scale(Temp)    -0.31305    0.14002  -2.236   0.0276 *  \n## scale(Wind):scale(Solar.R) -0.09259    0.15469  -0.599   0.5508    \n## scale(Temp):scale(Solar.R)  0.23573    0.15126   1.558   0.1223    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.321 on 100 degrees of freedom\n## Multiple R-squared:  0.7334, Adjusted R-squared:  0.7068 \n## F-statistic: 27.51 on 10 and 100 DF,  p-value: < 2.2e-16\n# get overall effect of Month:\nanova(m5)\n## Analysis of Variance Table\n## \n## Response: sqrt(Ozone)\n##                             Df  Sum Sq Mean Sq F value    Pr(>F)    \n## MonthFactor                  4 158.726  39.681 22.7249 2.261e-13 ***\n## scale(Wind)                  1 149.523 149.523 85.6296 4.282e-15 ***\n## scale(Temp)                  1 126.124 126.124 72.2290 1.899e-13 ***\n## scale(Solar.R)               1  19.376  19.376 11.0961 0.0012129 ** \n## scale(Wind):scale(Temp)      1  20.639  20.639 11.8198 0.0008556 ***\n## scale(Wind):scale(Solar.R)   1   1.803   1.803  1.0328 0.3119518    \n## scale(Temp):scale(Solar.R)   1   4.241   4.241  2.4288 0.1222856    \n## Residuals                  100 174.616   1.746                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# this is doing a type I ANOVA = sequential\n# order in which you include the predictors changes the estimates and p-values\n\n# If you want to do a type II ANOVA, use ANova() from the car package\nlibrary(car)\nAnova(m5) # Anova with capital A\n## Anova Table (Type II tests)\n## \n## Response: sqrt(Ozone)\n##                             Sum Sq  Df F value    Pr(>F)    \n## MonthFactor                  9.557   4  1.3683 0.2503349    \n## scale(Wind)                 41.993   1 24.0488 3.641e-06 ***\n## scale(Temp)                 78.938   1 45.2067 1.112e-09 ***\n## scale(Solar.R)              23.189   1 13.2797 0.0004276 ***\n## scale(Wind):scale(Temp)      8.728   1  4.9983 0.0275955 *  \n## scale(Wind):scale(Solar.R)   0.626   1  0.3582 0.5508395    \n## scale(Temp):scale(Solar.R)   4.241   1  2.4288 0.1222856    \n## Residuals                  174.616 100                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#type II ANOVA: all other predictors have already been taken into account\n# Does an additional predictor explain some of the variance on top of that?"
  },
  {
    "objectID": "5-MultipleRegression.html#model-selection",
    "href": "5-MultipleRegression.html#model-selection",
    "title": "5  Multiple regression",
    "section": "5.4 Model selection",
    "text": "5.4 Model selection\nWe’ve learned that we should include variables in the model that are collinear, that is they correlate with other predictors, but how many and which factors should we include?\nFamous example: Female hurricanes are deadlier than male hurricanes (Jung et al., 2014)\nThey have analyzed the number of fatalities of hurricane and claimed that there is an effect of femininity of the name on the number of deads (while correcting for confounders). They recommend to give hurricans only male names because it would considerably reduce the number of deads.\n\nlibrary(DHARMa)\n## This is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\nlibrary(effects)\n?hurricanes\nstr(hurricanes)\n## Classes 'tbl_df', 'tbl' and 'data.frame':    92 obs. of  14 variables:\n##  $ Year                    : num  1950 1950 1952 1953 1953 ...\n##  $ Name                    : chr  \"Easy\" \"King\" \"Able\" \"Barbara\" ...\n##  $ MasFem                  : num  6.78 1.39 3.83 9.83 8.33 ...\n##  $ MinPressure_before      : num  958 955 985 987 985 960 954 938 962 987 ...\n##  $ Minpressure_Updated_2014: num  960 955 985 987 985 960 954 938 962 987 ...\n##  $ Gender_MF               : num  1 0 0 1 1 1 1 1 1 1 ...\n##  $ Category                : num  3 3 1 1 1 3 3 4 3 1 ...\n##  $ alldeaths               : num  2 4 3 1 0 60 20 20 0 200 ...\n##  $ NDAM                    : num  1590 5350 150 58 15 ...\n##  $ Elapsed_Yrs             : num  63 63 61 60 60 59 59 59 58 58 ...\n##  $ Source                  : chr  \"MWR\" \"MWR\" \"MWR\" \"MWR\" ...\n##  $ ZMasFem                 : num  -0.000935 -1.670758 -0.913313 0.945871 0.481075 ...\n##  $ ZMinPressure_A          : num  -0.356 -0.511 1.038 1.141 1.038 ...\n##  $ ZNDAM                   : num  -0.439 -0.148 -0.55 -0.558 -0.561 ...\n\nlibrary(glmmTMB)\n## Warning in checkDepPackageVersion(dep_pkg = \"TMB\"): Package version inconsistency detected.\n## glmmTMB was built with TMB version 1.9.3\n## Current TMB version is 1.9.4\n## Please re-install glmmTMB from source or restore original 'TMB' package (see '?reinstalling' for more information)\n\nm1 = glmmTMB(alldeaths ~ MasFem*\n                             (Minpressure_Updated_2014 + scale(NDAM)),\n                           data = hurricanes, family = nbinom2)\nsummary(m1)\n##  Family: nbinom2  ( log )\n## Formula:          alldeaths ~ MasFem * (Minpressure_Updated_2014 + scale(NDAM))\n## Data: hurricanes\n## \n##      AIC      BIC   logLik deviance df.resid \n##    660.7    678.4   -323.4    646.7       85 \n## \n## \n## Dispersion parameter for nbinom2 family (): 0.787 \n## \n## Conditional model:\n##                                  Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)                     69.661590  23.425598   2.974 0.002942 ** \n## MasFem                          -5.855078   2.716589  -2.155 0.031138 *  \n## Minpressure_Updated_2014        -0.069870   0.024251  -2.881 0.003964 ** \n## scale(NDAM)                     -0.494094   0.455968  -1.084 0.278536    \n## MasFem:Minpressure_Updated_2014  0.006108   0.002813   2.171 0.029901 *  \n## MasFem:scale(NDAM)               0.205418   0.061956   3.316 0.000915 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nInteractions -> we need to scale variables:\n\nm2 = glmmTMB(alldeaths ~ scale(MasFem)*\n                             (scale(Minpressure_Updated_2014) + scale(NDAM)+scale(sqrt(NDAM))),\n                           data = hurricanes, family = nbinom2)\nsummary(m2)\n##  Family: nbinom2  ( log )\n## Formula:          \n## alldeaths ~ scale(MasFem) * (scale(Minpressure_Updated_2014) +  \n##     scale(NDAM) + scale(sqrt(NDAM)))\n## Data: hurricanes\n## \n##      AIC      BIC   logLik deviance df.resid \n##    634.9    657.6   -308.4    616.9       83 \n## \n## \n## Dispersion parameter for nbinom2 family (): 1.12 \n## \n## Conditional model:\n##                                               Estimate Std. Error z value\n## (Intercept)                                    2.28082    0.10850  21.022\n## scale(MasFem)                                  0.05608    0.10672   0.525\n## scale(Minpressure_Updated_2014)               -0.14267    0.17804  -0.801\n## scale(NDAM)                                   -1.11104    0.28030  -3.964\n## scale(sqrt(NDAM))                              2.10764    0.36487   5.776\n## scale(MasFem):scale(Minpressure_Updated_2014)  0.07371    0.19618   0.376\n## scale(MasFem):scale(NDAM)                     -0.10159    0.27080  -0.375\n## scale(MasFem):scale(sqrt(NDAM))                0.32960    0.36594   0.901\n##                                               Pr(>|z|)    \n## (Intercept)                                    < 2e-16 ***\n## scale(MasFem)                                    0.599    \n## scale(Minpressure_Updated_2014)                  0.423    \n## scale(NDAM)                                   7.38e-05 ***\n## scale(sqrt(NDAM))                             7.63e-09 ***\n## scale(MasFem):scale(Minpressure_Updated_2014)    0.707    \n## scale(MasFem):scale(NDAM)                        0.708    \n## scale(MasFem):scale(sqrt(NDAM))                  0.368    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe effect of femininity is gone! Already with the scaled variables, but also with the transformation with the NDAM variable. The question was raised which of both is more reasonable, whether the relationship between damage and mortality isn’t a straight line or that the gender of the hurricane names affect deaths (Bob O’Hara and GrrlScientist). They argue that the model with the transformed variable fits the data better which brings us to the topic of this section, how to choose between different models? Answering this question if the goal of model selection.\nWhy not include all the variables we can measure in our model? Problem with the full model:\n\nIf you have more parameters than data points, the model cannot be fitted at all\nEven with n (samples) ~ k (number of parameters), model properties become very unfavorable (high p-values and uncertainties/standard errors) –> Overfitting\n\nA “good model” depends on the goal of the analysis, do we want to optimize:\n\nPredictive ability – how well can we predict with the model?\nInferential ability – do we identify the true values for the parameters (true effects), are the p-values correct, can we correctly say that a variable has an effect?\n\nThe more complex a model gets, the better it fits to the data, but there’s a downside, the bias-variance tradeoff.\nExplanation bias-variance tradeoff\nExplanation LRT and AIC\nProblem of p-hacking\nExample:\n\n# Compare different competing models:\n# let's compare models m3 and m5 to decide which one explains our data better:\n# 1. LRT\nanova(m3, m5)\n# RSS = residual sum of squares = variance not explained by the model\n# smaller RSS = better model\n# p-value\n\n#2. AIC\nAIC(m3)\nAIC(m5)\n# also here, model m5 is better\n\n\n#### Demonstration: Why interpretation of effect sizes and p-values \n### after extensive model selection is not a good idea:\nlibrary(MASS)\nset.seed(1)\n#make up predictors:\ndat = data.frame(matrix(runif(20000), ncol = 100))\n# create a response variable\ndat$y = rnorm(200)\nfullModel = lm(y ~ ., data = dat)\nsum <- summary(fullModel)\nmean(sum$coefficients[,4] < 0.05)\n# 0.019: less than 2 % false positives = type I error rate\n\nselection = stepAIC(fullModel)\nsum.sel <- summary(selection)\nmean(sum.sel$coefficients[,4] < 0.05)\n# 0.48: Now almost 50 % of our results are false positives!!!"
  },
  {
    "objectID": "5-MultipleRegression.html#exercises",
    "href": "5-MultipleRegression.html#exercises",
    "title": "5  Multiple regression",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\nFormula syntax\n\n\n\n\n\n\n\nFormula\nMeaning\nDetails\n\n\n\n\ny~x_1\n\\(y=a_0 +a_1*x_1\\)\nSlope+Intercept\n\n\ny~x_1 - 1\n\\(y=a_1*x_1\\)\nSlope, no intercept\n\n\ny~I(x_1^2)\n\\(y=a_0 + a_1*(x_1^2)\\)\nQuadratic effect\n\n\ny~x_1+x_2\n\\(y=a_0+a_1*x_1+a_2*x_2\\)\nMultiple linear regression (two variables)\n\n\ny~x_1:x_2\n\\(y=a_0+a_1*(x_1*x_2)\\)\nInteraction between x1 and x2\n\n\ny~x_1*x_2\n\\(y=a_0+a_1*(x_1*x_2)+a_2*x_1+a_3*x_2\\)\nInteraction and main effects\n\n\n\nIn this exercise you will:\n\nperform multiple linear regressions\ninterpret regression output and check the residuals\nplot model predictions including interactions\n\nBefore you start, remember to clean your global environment (if you haven’t already) using rm(list=ls()).\nTo conduct the exercise, please load the following packages:\n\nlibrary(effects)\nlibrary(MuMIn)\n\nYou will work with the following datasets:\n\nmtcars\nCement{MuMIn}\n\nThe second dataset is from the MuMIn package (as shown by the curly brackets).\n\n5.5.0.1 Useful functions\nfor multiple linear regression\nlm() - fit linear model\nsummary(fit) - apply to fitted model object to display regression table\nplot(fit) - plot residuals for model validation\nanova(fit) - apply type I ANOVA (variables included sequentially) to model to test for effects all levels of a factor\nAnova(fit) - car package; use type II ANOVA (effects for predictors when all other predictors are already included) for overall effects\nscale() - scale variable\nsqrt() - square-root\nlog() - calculates natural logarithm\nplot(allEffects(fit)) - apply to fitted model object to plot marginal effect; effects package\npar() - change graphical parameters\nuse oldpar <- par(mfrow = c(number_rows, number_cols)) to change figure layout including more than 1 plot per figure\nuse par(oldpar) to reset graphic parameter\nfor model selection\nstepAIC(fullModel) - perform stepwise AIC model selection; apply to full model object, MASS package\ndredge(fullModel) - perform global model selection, MuMIn package\nmodel.avg() - perform model averaging\nAIC(fit) - get AIC for a fitted model\nanova(fit1, fit2) - compare two fitted models via Likelihood Ratio Test (LRT)\n\n\n5.5.1 Analyzing the mtcars dataset\nImagine a start up company wants to rebuild a car with a nice retro look from the 70ies. The car should be modern though, meaning the fuel consumption should be as low as possible. They’ve discovered the mtcars dataset with all the necessary measurements and they’ve somehow heard about you and your R skills and asked you for help. And of course you promised to help, kind as you are.\nThe company wants you to find out which of the following characteristics affects the fuel consumption measured in miles per gallon (mpg). Lower values for mpg thus reflect a higher fuel consumption. The company wants you to include the following variables into your analysis:\n\nnumber of cylinders (cyl)\nweight (wt)\nhorsepower (hp)\nwhether the car is driven manually or with automatic (am)\n\nIn addition, Pawl, one of the founders of the company suggested that the effect of weight (wt) might be irrelevant for powerful cars (high hp values). You are thus asked to test for this interaction in your analysis as well.\n\n\n\n\n\n\nQuestion\n\n\n\nCarry out the following tasks:\n\nPerform a multiple linear regression (change class for cyl and am to factor)\nCheck the model residuals\nInterpret and plot all effects\n\nYou may need the following functions:\n\nas.factor()\nlm()\nsummary()\nanova()\nplot()\nallEffects()\n\nUse your results to answer the questions on elearning-extern (Q 1-2) (“05_Test for Exercise in R - multiple regression”).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the code that you need to interpret the results.\n\n# change am and cyl from numeric to factor\nmtcars$am <- as.factor(mtcars$am)\nmtcars$cyl <- as.factor(mtcars$cyl)\n\n# multiple linear regression and results:\n# (we need to scale (standardize) the predictors wt and hp, since we include their interaction)\ncarsfit <- lm(mpg ~ am + cyl + scale(wt) * scale(hp), dat = mtcars)\n# weight is included as the first predictor in order to have\n# it as the grouping factor in the allEffects plot\n\nsummary(carsfit)\n## \n## Call:\n## lm(formula = mpg ~ am + cyl + scale(wt) * scale(hp), data = mtcars)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.4121 -1.6789 -0.4446  1.3752  4.4338 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)          19.9064     1.5362  12.958 1.36e-12 ***\n## am1                   0.1898     1.4909   0.127 0.899740    \n## cyl6                 -1.2818     1.5291  -0.838 0.409813    \n## cyl8                 -1.3942     2.1563  -0.647 0.523803    \n## scale(wt)            -3.6248     0.9665  -3.750 0.000938 ***\n## scale(hp)            -1.8602     0.8881  -2.095 0.046503 *  \n## scale(wt):scale(hp)   1.5631     0.7027   2.224 0.035383 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.246 on 25 degrees of freedom\n## Multiple R-squared:  0.888,  Adjusted R-squared:  0.8612 \n## F-statistic: 33.05 on 6 and 25 DF,  p-value: 1.021e-10\n# The first level of each factor is used as a reference, i.e. in this case a manual gear shift with 4 gears.\n# From the coefficient cyl6 we see that there is no significant difference in fuel consumption (= our response) between 4 gears (the reference) and 6 gears.\n# In contrast, the predictors weight (wt) and horsepower (hp) have a significant negative effect on the range (mpg), so that they both increase fuel consumption.\n\n# check residuals\nold.par = par(mfrow = c(2, 2))\nplot(carsfit)\n\n\n\npar(old.par)\n\n# plot effects\nplot(allEffects(carsfit))\n## Warning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\n## predictors scale(wt), scale(hp) are one-column matrices that were converted to\n## vectors\n\n## Warning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\n## predictors scale(wt), scale(hp) are one-column matrices that were converted to\n## vectors\n\n## Warning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\n## predictors scale(wt), scale(hp) are one-column matrices that were converted to\n## vectors\n\n\n\n# We can see in the wt*hp plot, that for high values of hp wt has no effect on the response mpg. We conclude that Pawl was right.\n\n\n\n\n\n\n5.5.2 Model-selection with the Cement dataset\nThe process of cement hardening involves exogenous chemical reactions and thus produces heat. The amount of heat produced by the cement depends on the mixture of its constituents. The Cement dataset includes heat measurements for different types of cement that consist of different relative amounts of calcium aluminate (X1), tricalcium silicate (X2), tetracalcium alumino ferrite (X3) and dicalcium silicate (X4). A cement producing company wants to optimize the composition of its product and wants to know, which of these compounds are mainly responsible for heat production.\n\n\n\n\n\n\nWarning\n\n\n\nWe only do a model selection here for educational reasons. For your analysis, and if your goal is not a predictive model, think about the model structure before you do the analysis and then stick to it! See here the section about p-hacking (and also consider that AIC selection will/can remove confounders which will violate causality and can lead to spurious correlations!\n\n\n\n\n\n\n\n\nQuestions\n\n\n\nCarry out the following tasks:\n\nPerform a multiple linear regression including all predictor variables and all two-way interactions (remember the notation (var1 + var2 + var3)^2.\nPerform forward, backward, and global model selection and compare the results\nFit the model considered optimal by global model selection and compare it with the full model based on AIC (or AICc) and LRT.\n\nYou may need the following functions:\n\nlm()\nsummary()\nstepAIC()\noptions()\ndredge()\nAIC() or AICc() (for small datasets)\nanova()\n\nUse your results to answer the questions on elearning-extern (Q 3-5) (“05_Test for Exercise in R - multiple regression”).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the code that you need to obtain the results.\n\nlibrary(MuMIn)\nlibrary(MASS)\n\n# full model ->  has 11 coefficients\nfull = lm(y ~ (X1 + X2 + X3 + X4)^2, data = Cement)\nsummary(full)\n\n# forward model selection\nms_forw = stepAIC(full, direction = \"forward\")\nsummary(ms_forw)\n# lists 11 coefficients (i.e. selects full model)\n\n# backward model selection\nms_back = stepAIC(full, direction = \"backward\")\nsummary(ms_back)\n# lists 10 coefficients\n\n# global model selection\noptions(na.action = \"na.fail\")\ndd = dredge(full)\nhead(dd)\n# The first row lists the best performing model: it includes only the intercept and effects for X1 and X2 (= 3 coefficients).\n\n# Fit the model considered optimal by global model selection and compare it with the full model based on AIC (or AICc) and LRT:\nopt = lm(y ~ X1 + X2, data = Cement)\nsummary(opt)\n\nAIC(opt,full) # full model is better according to AIC (lower AIC)\nanova(opt, full) # -> LRT: no significant difference between the models\n\n# sample size in the Cement dataset:\nstr(Cement)  # or\nnrow(Cement)\n\n# If the sample size is low, a corrected version of the AIC is recommended to avoid overfitting:\nAICc(opt,full) # This is inf! -> optimal model is better according to AICc"
  },
  {
    "objectID": "6-GLM.html#logistic-regression",
    "href": "6-GLM.html#logistic-regression",
    "title": "6  Generalized linear models",
    "section": "6.1 Logistic Regression",
    "text": "6.1 Logistic Regression\nFor the binomial model we can use the logit link:\n\\[\nlogit(y) = a_0 +a_1*x\n\\]\nAnd with the inverse link:\n\\[\ny = \\frac{1}{1+e^{-(a_0 + a_1) }}\n\\]\nYou can interpret the glm outputs basically like lm outputs.\nBUT: To get absolute response values, you have to transform the output with the inverse link function. For the logit, e.g. an intercept of 0 means a predicted value of 0.5. Different overall statistics: no R2 instead Pseudo R2 = 1 - Residual deviance / Null deviance\u000b(deviance is based on the likelihood):\n\n# logistic regression with categorical predictor\nm1 = glm(survived ~ sex, data = titanic, family = \"binomial\")\nsummary(m1)\n## \n## Call:\n## glm(formula = survived ~ sex, family = \"binomial\", data = titanic)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.6124  -0.6511  -0.6511   0.7977   1.8196  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)   0.9818     0.1040   9.437   <2e-16 ***\n## sexmale      -2.4254     0.1360 -17.832   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1741.0  on 1308  degrees of freedom\n## Residual deviance: 1368.1  on 1307  degrees of freedom\n## AIC: 1372.1\n## \n## Number of Fisher Scoring iterations: 4\n\n# 2 groups: sexmale = difference of mean for male from mean for female\n# intercept = linear term for female: \n0.98 \n## [1] 0.98\n# but: this has to be transformed back to original scale before being interpreted!!!\n# survival probability for females\nplogis(0.98)\n## [1] 0.7271082\n# applies inverse logit function\n\n\n# linear term for male\n0.98 - 2.43\n## [1] -1.45\n# survival probability\nplogis(0.98 - 2.43)\n## [1] 0.1900016\n\n# predicted linear term\ntable(predict(m1))\n## \n## -1.44362529285895 0.981813020919237 \n##               843               466\n# predicted response\ntable(predict(m1, type = \"response\"))\n## \n## 0.190984578884942 0.727467811158278 \n##               843               466\n\n\nplot(allEffects(m1))\n\n\n\n\n# more predictors\nm2 = glm(survived ~ sex + age, titanic, family = binomial)\nsummary(m2)\n## \n## Call:\n## glm(formula = survived ~ sex + age, family = binomial, data = titanic)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.7247  -0.6859  -0.6603   0.7555   1.8737  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  1.235414   0.192032   6.433 1.25e-10 ***\n## sexmale     -2.460689   0.152315 -16.155  < 2e-16 ***\n## age         -0.004254   0.005207  -0.817    0.414    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1414.6  on 1045  degrees of freedom\n## Residual deviance: 1101.3  on 1043  degrees of freedom\n##   (263 observations deleted due to missingness)\n## AIC: 1107.3\n## \n## Number of Fisher Scoring iterations: 4\n\n\n# Calculate Pseudo R2: 1 - Residual deviance / Null deviance\n1 - 1101.3/1414.6 # Pseudo R2 of model\n## [1] 0.221476\n\n# Anova\nanova(m2, test = \"Chisq\")\n## Analysis of Deviance Table\n## \n## Model: binomial, link: logit\n## \n## Response: survived\n## \n## Terms added sequentially (first to last)\n## \n## \n##      Df Deviance Resid. Df Resid. Dev Pr(>Chi)    \n## NULL                  1045     1414.6             \n## sex   1  312.612      1044     1102.0   <2e-16 ***\n## age   1    0.669      1043     1101.3   0.4133    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nplot(allEffects(m2))\n\n\n\n\nResidual check:\n\n# Model diagnostics\n# do not use the plot(model) residual checks\n# use DHARMa package\nlibrary(DHARMa)\nres = simulateResiduals(m2)\nplot(res)"
  },
  {
    "objectID": "6-GLM.html#poisson-regression",
    "href": "6-GLM.html#poisson-regression",
    "title": "6  Generalized linear models",
    "section": "6.2 Poisson Regression",
    "text": "6.2 Poisson Regression\nPoisson regression is used for count data. Properties of count data are: no negative values, only integers, y ~ Poisson(lambda); where lambda = mean = variance, log link function (lambda must be positive).\nExample:\n\nhead(birdfeeding)\n##   feeding attractiveness\n## 1       3              1\n## 2       6              1\n## 3       8              1\n## 4       4              1\n## 5       2              1\n## 6       7              2\n\nplot(feeding ~ attractiveness, birdfeeding)\n\n\n\n\nfit = glm(feeding ~ attractiveness, birdfeeding, family = \"poisson\")\nsummary(fit)\n## \n## Call:\n## glm(formula = feeding ~ attractiveness, family = \"poisson\", data = birdfeeding)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -1.55377  -0.72834   0.03699   0.59093   1.54584  \n## \n## Coefficients:\n##                Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)     1.47459    0.19443   7.584 3.34e-14 ***\n## attractiveness  0.14794    0.05437   2.721  0.00651 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 25.829  on 24  degrees of freedom\n## Residual deviance: 18.320  on 23  degrees of freedom\n## AIC: 115.42\n## \n## Number of Fisher Scoring iterations: 4\n\n# feeding for a bird with attractiveness 3\n# linear term\n1.47 + 0.148 * 3\n## [1] 1.914\n# pieces of food, using inverse of the link function, log --> exp\nexp(1.47 + 0.148 * 3)\n## [1] 6.780155\n\n\nplot(allEffects(fit))\n\n\n\n\n\n# checking residuals\nres = simulateResiduals(fit)\nplot(res, quantreg = F)\n## Warning in smooth.spline(pred, res, df = 10): not using invalid df; must have 1\n## < df <= n := #{unique x} = 5\n\n\n\n# the warning is because of a recent change in DHARMa \n# qgam requires more data points\n\nNormal versus Poisson distribution:\n\nN(mean, sd)\u000bThis means that fitting a normal distribution estimates a parameter for the variance (sd)\nPoisson(lambda)\u000blambda = mean = variance\u000bThis means that a Poisson regression does not fit a separate parameter for the variance\n\nSo in the glm always assume that the variance is the mean, which is a strong assumption. In reality it can often occur that the variance is greater than expected (Overdispersion) or smaller than expected (Underdispersion). (this cannot happen for the lm because there we estimate a variance parameter (residual variance)). Overdispersion can have a HUGE influence on the MLEs and particularly on the p-values!\nWe can use the DHARMa package to check for Over or Underdispersion:\n\n# test for overdispersion\ntestDispersion(fit)\n\n\n\n## \n##  DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n##  simulated\n## \n## data:  simulationOutput\n## dispersion = 0.74488, p-value = 0.384\n## alternative hypothesis: two.sided\n\n# Dispersion test is necessary for all poisson or binomial models with k/n \n# if positive, you can chose family = quasi-poisson or quasi-binomial\n# or use negative binomial distribution instead of poisson"
  },
  {
    "objectID": "6-GLM.html#exercises",
    "href": "6-GLM.html#exercises",
    "title": "6  Generalized linear models",
    "section": "6.3 Exercises",
    "text": "6.3 Exercises\nIn this exercise you will practice to:\n\nchoose the correct model for your data\ninterpret the model output of generalized linear models\ncheck the residuals using the DHARMa package\n\nBefore you start, remember to clean your global environment (if you haven’t already) using rm(list=ls()).\nFor your analyses, you will use the datasets nitrofen and melanoma which are both implemented in the boot package. You will also need the packages DHARMa and effects. Please install and load these packages, if you haven’t done this yet:\n\nlibrary(boot)\n## \n## Attaching package: 'boot'\n## The following objects are masked from 'package:EcoData':\n## \n##     melanoma, nitrofen\nlibrary(DHARMa)\nlibrary(effects)\n\n\n6.3.1 Analyzing the nitrofen dataset\nThe Ministry of Agriculture has appointed your university to investigate the toxicity of the herbicide nitrofen on the fertility of the waterflea species Ceriodaphnia dubia in order to asses the implications of the herbicide for ecosystems.\nIn an experiment conducted by your fellow researchers, the offspring of the waterflea species Ceriodaphnia dubia were counted as a function of different concentrations of the herbicide. Your job is to do the analysis.\n\n\n\n\n\n\nQuestion\n\n\n\nCarry out the following tasks:\n\nConvert the variable conc into a factor.\nFit a suitable model for the relationship between total amount of offspring (total) and nitrofen concentration (conc).\nTest for overdispersion.\nInterpret and plot the effect.\nTest for the overall effect of nitrofen concentration.\n\nYou may need the following functions:\n\nstr()\nas.factor()\nglm()\nsummary()\ntestDispersion()\nplot()\nallEffects()\nanova(…, test = “Chisq”)\n\nUse your results to answer the questions on elearning-extern (Q 1-2) (“06 - Test for Exercise in R”).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# prepare data\nnitrofen$conc <- as.factor(nitrofen$conc) # change variable conc to a factor\n\n# plot the relation that we want to fit\nplot(total ~ conc, nitrofen)\n\n\n\n\n# Fit a suitable model for the relationship between total amount of offspring (total) and nitrofen concentration (conc):\nfit <- glm(total ~ conc, family = \"poisson\", data=nitrofen)\nsummary(fit)\n## \n## Call:\n## glm(formula = total ~ conc, family = \"poisson\", data = nitrofen)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.4641  -0.4339   0.0444   0.3164   3.0804  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  3.44681    0.05643  61.078  < 2e-16 ***\n## conc80       0.00318    0.07974   0.040    0.968    \n## conc160     -0.10395    0.08196  -1.268    0.205    \n## conc235     -0.60190    0.09486  -6.345 2.22e-10 ***\n## conc310     -1.65505    0.14089 -11.747  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 312.484  on 49  degrees of freedom\n## Residual deviance:  50.719  on 45  degrees of freedom\n## AIC: 297.81\n## \n## Number of Fisher Scoring iterations: 5\n\n# test for overdispersion\ntestDispersion(fit)\n\n\n\n## \n##  DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n##  simulated\n## \n## data:  simulationOutput\n## dispersion = 0.62249, p-value = 0.048\n## alternative hypothesis: two.sided\n\n# plot effect\nplot(allEffects(fit))\n\n\n\n\n# log link to calculate predicted values at the response scale:\n# predicted response = exp(Intercept + Estimate * predictor)\nexp(3.44681) # or\n## [1] 31.40007\nexp(coef(fit)[1])\n## (Intercept) \n##        31.4\n\n# Test for the overall effect of *conc* on the total number of offspring\nanova(fit, test = \"Chisq\")\n## Analysis of Deviance Table\n## \n## Model: poisson, link: log\n## \n## Response: total\n## \n## Terms added sequentially (first to last)\n## \n## \n##      Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \n## NULL                    49    312.484              \n## conc  4   261.76        45     50.719 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n6.3.2 Analyzing the melanoma dataset\nIn the following, you will investigate the melanoma dataset provided by the University Hospital of Odense, Denmark. The data are of patients with malignant melanomas. You can find a more detailed description in the help of melanoma. The question you want to investigate is whether the occurrence of ulceration (ulcer, yes = 1, no = 0) is related to the thickness of the tumor (thickness in mm) and the sex of the patient (sex, male = 1, female = 0) and the interaction of the two.\n\n\n\n\n\n\nQuestion\n\n\n\nCarry out the following tasks:\n\nFit an appropriate model to answer the research question.\nCheck the model residuals.\nInterpret and plot all effects including an analysis of the deviance.\n\nYou may need the following functions:\n\nstr()\nglm()\nsummary()\nanova()\nsimulateResiduals()\nplot()\nallEffects()\n\nUse your results to answer the questions on elearning-extern (Q 3-5).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# get the data\ndat <- melanoma\n\n# Fit an appropriate model to answer the research question.\ndat$sex <- as.factor(dat$sex) # change variable sex to factor (this is optional, not necessary with binary values 0/1)\nfit <- glm(ulcer ~ thickness * sex, family = \"binomial\", data=dat)\n\n# Check residuals\nres <- simulateResiduals(fit, n = 500)\nplot(res)\n\n\n\n\n# model interpretation\nsummary(fit)\n## \n## Call:\n## glm(formula = ulcer ~ thickness * sex, family = \"binomial\", data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.4966  -0.8155  -0.7043   1.0172   1.7767  \n## \n## Coefficients:\n##                Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)    -1.40642    0.31902  -4.409 1.04e-05 ***\n## thickness       0.36927    0.11368   3.248  0.00116 ** \n## sex1           -0.02579    0.55535  -0.046  0.96296    \n## thickness:sex1  0.14527    0.17656   0.823  0.41064    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 281.13  on 204  degrees of freedom\n## Residual deviance: 234.04  on 201  degrees of freedom\n## AIC: 242.04\n## \n## Number of Fisher Scoring iterations: 5\nanova(fit, test = \"Chisq\")\n## Analysis of Deviance Table\n## \n## Model: binomial, link: logit\n## \n## Response: ulcer\n## \n## Terms added sequentially (first to last)\n## \n## \n##               Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \n## NULL                            204     281.13              \n## thickness      1   45.374       203     235.76 1.628e-11 ***\n## sex            1    1.039       202     234.72    0.3080    \n## thickness:sex  1    0.681       201     234.04    0.4094    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Calculate Pseudo R2: 1 - Residual deviance / Null deviance\n1 - 234.04/281.13 # Pseudo R2 of model\n## [1] 0.1675026\n\n# plot effects\nplot(allEffects(fit))\n\n\n\n\nAs the residuals look quite suspicious and all quantile regressions significantly deviate, we can try to improve the model with a quadratic term for thickness.\n\nfit <- glm(ulcer ~ thickness * sex + I(thickness^2), family = \"binomial\", data=dat)\nsummary(fit)\n## \n## Call:\n## glm(formula = ulcer ~ thickness * sex + I(thickness^2), family = \"binomial\", \n##     data = dat)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.2511  -0.7302  -0.5116   0.7949   2.1875  \n## \n## Coefficients:\n##                 Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)    -2.466019   0.411511  -5.993 2.07e-09 ***\n## thickness       1.066836   0.193715   5.507 3.65e-08 ***\n## sex1            0.236510   0.518545   0.456    0.648    \n## I(thickness^2) -0.057746   0.012766  -4.523 6.09e-06 ***\n## thickness:sex1 -0.009476   0.133618  -0.071    0.943    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 281.13  on 204  degrees of freedom\n## Residual deviance: 214.05  on 200  degrees of freedom\n## AIC: 224.05\n## \n## Number of Fisher Scoring iterations: 4\nres <- simulateResiduals(fit)\nplot(res)\n\n\n\n\n# plot effects\nplot(allEffects(fit))\n\n\n\n\nThe quadratic term solves the problem of the residuals. The effects plots look quite different. There seems to be a maximum of ulcer around a thickness of 10 that we would have missed without the quadratic term."
  },
  {
    "objectID": "7-Multivariate.html#unconstrained",
    "href": "7-Multivariate.html#unconstrained",
    "title": "7  Multivariate Statistics",
    "section": "7.1 Unconstrained",
    "text": "7.1 Unconstrained\n\n## PCA\n# we use the same dataset of flower characteristics of three species of iris\npairs(iris, col = iris$Species)\n\n\n\n\nhead(iris)\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n## 1          5.1         3.5          1.4         0.2  setosa\n## 2          4.9         3.0          1.4         0.2  setosa\n## 3          4.7         3.2          1.3         0.2  setosa\n## 4          4.6         3.1          1.5         0.2  setosa\n## 5          5.0         3.6          1.4         0.2  setosa\n## 6          5.4         3.9          1.7         0.4  setosa\npca = prcomp(iris[, 1:4], scale = T) # always set scale = T\n# when data is very skewed --> better transform e.g. log\n\nsummary(pca)\n## Importance of components:\n##                           PC1    PC2     PC3     PC4\n## Standard deviation     1.7084 0.9560 0.38309 0.14393\n## Proportion of Variance 0.7296 0.2285 0.03669 0.00518\n## Cumulative Proportion  0.7296 0.9581 0.99482 1.00000\n# standard deviation^2 is variance!!!\n# cum prop of PC2 is the variance that is visualized in a biplot\n\n\n# plot the result\n# absolute variance of each component\nplot(pca) # see row1 of the summary(pca): (sd)^2 = variance\n\n\n\n\n# rel variance of each component\nbarplot(summary(pca)$importance[2, ], \n        ylab=\"proportion of variance explained\") # displays % of variance explained by PCs\n\n\n\n\n# Biplot\nbiplot(pca) # displays PC1 and PC2 AND rotation (vectors) of the different variables AND observations\n\n\n\n\n## distance-based approach: NMDS\n\nlibrary(vegan)\n## Loading required package: permute\n## Loading required package: lattice\n## This is vegan 2.6-4\n\n\n\n?vegan\n\n# community dataset for plants in dunes (included in vegan package):\n\ndata(\"dune\")\nstr(dune) # display structure of the dataset\n## 'data.frame':    20 obs. of  30 variables:\n##  $ Achimill: num  1 3 0 0 2 2 2 0 0 4 ...\n##  $ Agrostol: num  0 0 4 8 0 0 0 4 3 0 ...\n##  $ Airaprae: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Alopgeni: num  0 2 7 2 0 0 0 5 3 0 ...\n##  $ Anthodor: num  0 0 0 0 4 3 2 0 0 4 ...\n##  $ Bellpere: num  0 3 2 2 2 0 0 0 0 2 ...\n##  $ Bromhord: num  0 4 0 3 2 0 2 0 0 4 ...\n##  $ Chenalbu: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Cirsarve: num  0 0 0 2 0 0 0 0 0 0 ...\n##  $ Comapalu: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Eleopalu: num  0 0 0 0 0 0 0 4 0 0 ...\n##  $ Elymrepe: num  4 4 4 4 4 0 0 0 6 0 ...\n##  $ Empenigr: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Hyporadi: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Juncarti: num  0 0 0 0 0 0 0 4 4 0 ...\n##  $ Juncbufo: num  0 0 0 0 0 0 2 0 4 0 ...\n##  $ Lolipere: num  7 5 6 5 2 6 6 4 2 6 ...\n##  $ Planlanc: num  0 0 0 0 5 5 5 0 0 3 ...\n##  $ Poaprat : num  4 4 5 4 2 3 4 4 4 4 ...\n##  $ Poatriv : num  2 7 6 5 6 4 5 4 5 4 ...\n##  $ Ranuflam: num  0 0 0 0 0 0 0 2 0 0 ...\n##  $ Rumeacet: num  0 0 0 0 5 6 3 0 2 0 ...\n##  $ Sagiproc: num  0 0 0 5 0 0 0 2 2 0 ...\n##  $ Salirepe: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Scorautu: num  0 5 2 2 3 3 3 3 2 3 ...\n##  $ Trifprat: num  0 0 0 0 2 5 2 0 0 0 ...\n##  $ Trifrepe: num  0 5 2 1 2 5 2 2 3 6 ...\n##  $ Vicilath: num  0 0 0 0 0 0 0 0 0 1 ...\n##  $ Bracruta: num  0 0 2 2 2 6 2 2 2 2 ...\n##  $ Callcusp: num  0 0 0 0 0 0 0 0 0 0 ...\n?dune\nsummary(dune) # display summary of the dataset (summary statistics for each variable)\n##     Achimill      Agrostol      Airaprae       Alopgeni       Anthodor   \n##  Min.   :0.0   Min.   :0.0   Min.   :0.00   Min.   :0.00   Min.   :0.00  \n##  1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.00   1st Qu.:0.00   1st Qu.:0.00  \n##  Median :0.0   Median :1.5   Median :0.00   Median :0.00   Median :0.00  \n##  Mean   :0.8   Mean   :2.4   Mean   :0.25   Mean   :1.80   Mean   :1.05  \n##  3rd Qu.:2.0   3rd Qu.:4.0   3rd Qu.:0.00   3rd Qu.:3.25   3rd Qu.:2.25  \n##  Max.   :4.0   Max.   :8.0   Max.   :3.00   Max.   :8.00   Max.   :4.00  \n##     Bellpere       Bromhord       Chenalbu       Cirsarve      Comapalu  \n##  Min.   :0.00   Min.   :0.00   Min.   :0.00   Min.   :0.0   Min.   :0.0  \n##  1st Qu.:0.00   1st Qu.:0.00   1st Qu.:0.00   1st Qu.:0.0   1st Qu.:0.0  \n##  Median :0.00   Median :0.00   Median :0.00   Median :0.0   Median :0.0  \n##  Mean   :0.65   Mean   :0.75   Mean   :0.05   Mean   :0.1   Mean   :0.2  \n##  3rd Qu.:2.00   3rd Qu.:0.50   3rd Qu.:0.00   3rd Qu.:0.0   3rd Qu.:0.0  \n##  Max.   :3.00   Max.   :4.00   Max.   :1.00   Max.   :2.0   Max.   :2.0  \n##     Eleopalu       Elymrepe      Empenigr      Hyporadi       Juncarti   \n##  Min.   :0.00   Min.   :0.0   Min.   :0.0   Min.   :0.00   Min.   :0.00  \n##  1st Qu.:0.00   1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.00   1st Qu.:0.00  \n##  Median :0.00   Median :0.0   Median :0.0   Median :0.00   Median :0.00  \n##  Mean   :1.25   Mean   :1.3   Mean   :0.1   Mean   :0.45   Mean   :0.90  \n##  3rd Qu.:1.00   3rd Qu.:4.0   3rd Qu.:0.0   3rd Qu.:0.00   3rd Qu.:0.75  \n##  Max.   :8.00   Max.   :6.0   Max.   :2.0   Max.   :5.00   Max.   :4.00  \n##     Juncbufo       Lolipere      Planlanc      Poaprat       Poatriv    \n##  Min.   :0.00   Min.   :0.0   Min.   :0.0   Min.   :0.0   Min.   :0.00  \n##  1st Qu.:0.00   1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.00  \n##  Median :0.00   Median :2.0   Median :0.0   Median :3.0   Median :4.00  \n##  Mean   :0.65   Mean   :2.9   Mean   :1.3   Mean   :2.4   Mean   :3.15  \n##  3rd Qu.:0.00   3rd Qu.:6.0   3rd Qu.:3.0   3rd Qu.:4.0   3rd Qu.:5.00  \n##  Max.   :4.00   Max.   :7.0   Max.   :5.0   Max.   :5.0   Max.   :9.00  \n##     Ranuflam      Rumeacet      Sagiproc    Salirepe       Scorautu  \n##  Min.   :0.0   Min.   :0.0   Min.   :0   Min.   :0.00   Min.   :0.0  \n##  1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0   1st Qu.:0.00   1st Qu.:2.0  \n##  Median :0.0   Median :0.0   Median :0   Median :0.00   Median :2.0  \n##  Mean   :0.7   Mean   :0.9   Mean   :1   Mean   :0.55   Mean   :2.7  \n##  3rd Qu.:2.0   3rd Qu.:0.5   3rd Qu.:2   3rd Qu.:0.00   3rd Qu.:3.0  \n##  Max.   :4.0   Max.   :6.0   Max.   :5   Max.   :5.00   Max.   :6.0  \n##     Trifprat       Trifrepe       Vicilath      Bracruta       Callcusp  \n##  Min.   :0.00   Min.   :0.00   Min.   :0.0   Min.   :0.00   Min.   :0.0  \n##  1st Qu.:0.00   1st Qu.:1.00   1st Qu.:0.0   1st Qu.:1.50   1st Qu.:0.0  \n##  Median :0.00   Median :2.00   Median :0.0   Median :2.00   Median :0.0  \n##  Mean   :0.45   Mean   :2.35   Mean   :0.2   Mean   :2.45   Mean   :0.5  \n##  3rd Qu.:0.00   3rd Qu.:3.00   3rd Qu.:0.0   3rd Qu.:4.00   3rd Qu.:0.0  \n##  Max.   :5.00   Max.   :6.00   Max.   :2.0   Max.   :6.00   Max.   :4.0\n\nNMDS = metaMDS(dune)\n## Run 0 stress 0.1192678 \n## Run 1 stress 0.1886532 \n## Run 2 stress 0.1192678 \n## ... Procrustes: rmse 1.258026e-05  max resid 3.164268e-05 \n## ... Similar to previous best\n## Run 3 stress 0.1183186 \n## ... New best solution\n## ... Procrustes: rmse 0.02027014  max resid 0.06496123 \n## Run 4 stress 0.1183186 \n## ... Procrustes: rmse 4.230386e-06  max resid 1.475206e-05 \n## ... Similar to previous best\n## Run 5 stress 0.1192678 \n## Run 6 stress 0.1809578 \n## Run 7 stress 0.1889638 \n## Run 8 stress 0.1808911 \n## Run 9 stress 0.3680059 \n## Run 10 stress 0.1183186 \n## ... Procrustes: rmse 1.16265e-05  max resid 3.451986e-05 \n## ... Similar to previous best\n## Run 11 stress 0.1183186 \n## ... Procrustes: rmse 4.643335e-06  max resid 1.388314e-05 \n## ... Similar to previous best\n## Run 12 stress 0.2361935 \n## Run 13 stress 0.1192679 \n## Run 14 stress 0.1192679 \n## Run 15 stress 0.1183186 \n## ... Procrustes: rmse 1.447333e-05  max resid 4.510486e-05 \n## ... Similar to previous best\n## Run 16 stress 0.1192679 \n## Run 17 stress 0.1192678 \n## Run 18 stress 0.1192679 \n## Run 19 stress 0.1808911 \n## Run 20 stress 0.1808911 \n## *** Best solution repeated 4 times\n# algorithm is iterative\n\nNMDS # gives information on NMDS: distance measure, stress (should be low)\n## \n## Call:\n## metaMDS(comm = dune) \n## \n## global Multidimensional Scaling using monoMDS\n## \n## Data:     dune \n## Distance: bray \n## \n## Dimensions: 2 \n## Stress:     0.1183186 \n## Stress type 1, weak ties\n## Best solution was repeated 4 times in 20 tries\n## The best solution was from try 3 (random start)\n## Scaling: centring, PC rotation, halfchange scaling \n## Species: expanded scores based on 'dune'\n# stress of >= 0.2 = be suspicious, stress >=0.3 indicates that ordination is arbitrary\n# increase Dimensions if k is too high\nordiplot(NMDS, type = \"t\") #\"t\" = text\n\n\n\n\n\n# if we have time:\n# distance measure can be changed (default is Bray-Curtis): see \n?vegdist # some recommendations there\nNMDS2 = metaMDS(dune, distance=\"euclidean\")\n## Run 0 stress 0.1174523 \n## Run 1 stress 0.1174523 \n## ... Procrustes: rmse 3.580249e-06  max resid 1.150625e-05 \n## ... Similar to previous best\n## Run 2 stress 0.1174523 \n## ... Procrustes: rmse 2.054651e-06  max resid 3.818306e-06 \n## ... Similar to previous best\n## Run 3 stress 0.1174523 \n## ... Procrustes: rmse 2.128171e-06  max resid 5.656827e-06 \n## ... Similar to previous best\n## Run 4 stress 0.1174523 \n## ... Procrustes: rmse 2.022272e-06  max resid 6.26572e-06 \n## ... Similar to previous best\n## Run 5 stress 0.1174523 \n## ... Procrustes: rmse 2.488733e-06  max resid 8.218602e-06 \n## ... Similar to previous best\n## Run 6 stress 0.1177339 \n## ... Procrustes: rmse 0.01706434  max resid 0.05525565 \n## Run 7 stress 0.1174523 \n## ... New best solution\n## ... Procrustes: rmse 1.353658e-06  max resid 2.894393e-06 \n## ... Similar to previous best\n## Run 8 stress 0.1174523 \n## ... Procrustes: rmse 1.402024e-06  max resid 4.031045e-06 \n## ... Similar to previous best\n## Run 9 stress 0.1174523 \n## ... Procrustes: rmse 2.805576e-06  max resid 7.787809e-06 \n## ... Similar to previous best\n## Run 10 stress 0.1174523 \n## ... Procrustes: rmse 1.140971e-06  max resid 2.993735e-06 \n## ... Similar to previous best\n## Run 11 stress 0.1177339 \n## ... Procrustes: rmse 0.0170643  max resid 0.05525551 \n## Run 12 stress 0.1174523 \n## ... Procrustes: rmse 1.964847e-06  max resid 5.765074e-06 \n## ... Similar to previous best\n## Run 13 stress 0.1174523 \n## ... Procrustes: rmse 1.231263e-06  max resid 2.345061e-06 \n## ... Similar to previous best\n## Run 14 stress 0.1174523 \n## ... Procrustes: rmse 1.05504e-06  max resid 2.45264e-06 \n## ... Similar to previous best\n## Run 15 stress 0.1177339 \n## ... Procrustes: rmse 0.01706422  max resid 0.05525614 \n## Run 16 stress 0.1177339 \n## ... Procrustes: rmse 0.01706442  max resid 0.05525521 \n## Run 17 stress 0.1177339 \n## ... Procrustes: rmse 0.01706456  max resid 0.05525899 \n## Run 18 stress 0.1177339 \n## ... Procrustes: rmse 0.01706413  max resid 0.05525604 \n## Run 19 stress 0.1177339 \n## ... Procrustes: rmse 0.01706403  max resid 0.05525579 \n## Run 20 stress 0.1174523 \n## ... Procrustes: rmse 2.00709e-06  max resid 5.652376e-06 \n## ... Similar to previous best\n## *** Best solution repeated 8 times\nNMDS2 \n## \n## Call:\n## metaMDS(comm = dune, distance = \"euclidean\") \n## \n## global Multidimensional Scaling using monoMDS\n## \n## Data:     dune \n## Distance: euclidean \n## \n## Dimensions: 2 \n## Stress:     0.1174523 \n## Stress type 1, weak ties\n## Best solution was repeated 8 times in 20 tries\n## The best solution was from try 7 (random start)\n## Scaling: centring, PC rotation \n## Species: expanded scores based on 'dune'\nordiplot(NMDS2, type = \"t\")\n\n\n\n\n\n\n\n\n\n\nDanger\n\n\n\n\n# Why we should be careful when interpreting patterns in ordination plots\nset.seed(123)\n\nrandom = data.frame(pollution = rnorm(30),\n                    temperature = rnorm(30),\n                    moisture = rnorm(30),\n                    tourists = rnorm(30),\n                    wind = rnorm(30),\n                    dogs = rnorm(30))\nhead(random)\n##     pollution temperature   moisture   tourists       wind       dogs\n## 1 -0.56047565   0.4264642  0.3796395  0.9935039  0.1176466  0.7877388\n## 2 -0.23017749  -0.2950715 -0.5023235  0.5483970 -0.9474746  0.7690422\n## 3  1.55870831   0.8951257 -0.3332074  0.2387317 -0.4905574  0.3322026\n## 4  0.07050839   0.8781335 -1.0185754 -0.6279061 -0.2560922 -1.0083766\n## 5  0.12928774   0.8215811 -1.0717912  1.3606524  1.8438620 -0.1194526\n## 6  1.71506499   0.6886403  0.3035286 -0.6002596 -0.6519499 -0.2803953\npca = prcomp(random, scale = T)\nbiplot(pca)\n\n\n\nsummary(pca) # similar variance on all axes\n## Importance of components:\n##                           PC1    PC2    PC3    PC4    PC5     PC6\n## Standard deviation     1.2054 1.1078 1.0649 0.9707 0.8530 0.71817\n## Proportion of Variance 0.2422 0.2046 0.1890 0.1570 0.1213 0.08596\n## Cumulative Proportion  0.2422 0.4467 0.6357 0.7928 0.9140 1.00000"
  },
  {
    "objectID": "7-Multivariate.html#constrained",
    "href": "7-Multivariate.html#constrained",
    "title": "7  Multivariate Statistics",
    "section": "7.2 Constrained",
    "text": "7.2 Constrained\n\n# 2 multivariate datasets (abundances + environment)\n\n\n## RDA\nstr(dune) # species composition\n## 'data.frame':    20 obs. of  30 variables:\n##  $ Achimill: num  1 3 0 0 2 2 2 0 0 4 ...\n##  $ Agrostol: num  0 0 4 8 0 0 0 4 3 0 ...\n##  $ Airaprae: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Alopgeni: num  0 2 7 2 0 0 0 5 3 0 ...\n##  $ Anthodor: num  0 0 0 0 4 3 2 0 0 4 ...\n##  $ Bellpere: num  0 3 2 2 2 0 0 0 0 2 ...\n##  $ Bromhord: num  0 4 0 3 2 0 2 0 0 4 ...\n##  $ Chenalbu: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Cirsarve: num  0 0 0 2 0 0 0 0 0 0 ...\n##  $ Comapalu: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Eleopalu: num  0 0 0 0 0 0 0 4 0 0 ...\n##  $ Elymrepe: num  4 4 4 4 4 0 0 0 6 0 ...\n##  $ Empenigr: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Hyporadi: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Juncarti: num  0 0 0 0 0 0 0 4 4 0 ...\n##  $ Juncbufo: num  0 0 0 0 0 0 2 0 4 0 ...\n##  $ Lolipere: num  7 5 6 5 2 6 6 4 2 6 ...\n##  $ Planlanc: num  0 0 0 0 5 5 5 0 0 3 ...\n##  $ Poaprat : num  4 4 5 4 2 3 4 4 4 4 ...\n##  $ Poatriv : num  2 7 6 5 6 4 5 4 5 4 ...\n##  $ Ranuflam: num  0 0 0 0 0 0 0 2 0 0 ...\n##  $ Rumeacet: num  0 0 0 0 5 6 3 0 2 0 ...\n##  $ Sagiproc: num  0 0 0 5 0 0 0 2 2 0 ...\n##  $ Salirepe: num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Scorautu: num  0 5 2 2 3 3 3 3 2 3 ...\n##  $ Trifprat: num  0 0 0 0 2 5 2 0 0 0 ...\n##  $ Trifrepe: num  0 5 2 1 2 5 2 2 3 6 ...\n##  $ Vicilath: num  0 0 0 0 0 0 0 0 0 1 ...\n##  $ Bracruta: num  0 0 2 2 2 6 2 2 2 2 ...\n##  $ Callcusp: num  0 0 0 0 0 0 0 0 0 0 ...\ndata(\"dune.env\")\nstr(dune.env) # environmental variables\n## 'data.frame':    20 obs. of  5 variables:\n##  $ A1        : num  2.8 3.5 4.3 4.2 6.3 4.3 2.8 4.2 3.7 3.3 ...\n##  $ Moisture  : Ord.factor w/ 4 levels \"1\"<\"2\"<\"4\"<\"5\": 1 1 2 2 1 1 1 4 3 2 ...\n##  $ Management: Factor w/ 4 levels \"BF\",\"HF\",\"NM\",..: 4 1 4 4 2 2 2 2 2 1 ...\n##  $ Use       : Ord.factor w/ 3 levels \"Hayfield\"<\"Haypastu\"<..: 2 2 2 2 1 2 3 3 1 1 ...\n##  $ Manure    : Ord.factor w/ 5 levels \"0\"<\"1\"<\"2\"<\"3\"<..: 5 3 5 5 3 3 4 4 2 2 ...\n\nRDA = rda(dune ~ as.numeric(Manure) + as.numeric(Moisture), \n         data = dune.env)\nplot(RDA)\n\n\n\n\nsummary(RDA)\n## \n## Call:\n## rda(formula = dune ~ as.numeric(Manure) + as.numeric(Moisture),      data = dune.env) \n## \n## Partitioning of variance:\n##               Inertia Proportion\n## Total           84.12     1.0000\n## Constrained     31.20     0.3709\n## Unconstrained   52.92     0.6291\n## \n## Eigenvalues, and their contribution to the variance \n## \n## Importance of components:\n##                          RDA1    RDA2     PC1     PC2     PC3    PC4     PC5\n## Eigenvalue            19.0185 12.1864 10.7889 7.85393 6.67657 6.2084 5.12380\n## Proportion Explained   0.2261  0.1449  0.1283 0.09336 0.07937 0.0738 0.06091\n## Cumulative Proportion  0.2261  0.3709  0.4992 0.59255 0.67192 0.7457 0.80663\n##                           PC6     PC7     PC8     PC9    PC10    PC11    PC12\n## Eigenvalue            3.50320 3.10988 2.47760 1.83130 1.63276 1.03305 0.87265\n## Proportion Explained  0.04164 0.03697 0.02945 0.02177 0.01941 0.01228 0.01037\n## Cumulative Proportion 0.84827 0.88524 0.91469 0.93646 0.95587 0.96815 0.97852\n##                           PC13     PC14     PC15     PC16     PC17\n## Eigenvalue            0.642862 0.469072 0.301387 0.215853 0.177610\n## Proportion Explained  0.007642 0.005576 0.003583 0.002566 0.002111\n## Cumulative Proportion 0.986164 0.991740 0.995323 0.997889 1.000000\n## \n## Accumulated constrained eigenvalues\n## Importance of components:\n##                          RDA1    RDA2\n## Eigenvalue            19.0185 12.1864\n## Proportion Explained   0.6095  0.3905\n## Cumulative Proportion  0.6095  1.0000\n## \n## Scaling 2 for species and site scores\n## * Species are scaled proportional to eigenvalues\n## * Sites are unscaled: weighted dispersion equal on all dimensions\n## * General scaling constant of scores:  6.322924 \n## \n## \n## Species scores\n## \n##              RDA1     RDA2       PC1        PC2        PC3      PC4\n## Achimill -0.47851  0.09403 -0.235988 -0.3197353  0.3103983 -0.28427\n## Agrostol  1.23296 -0.81601  0.315190  0.4660974 -0.5315336 -0.32248\n## Airaprae  0.09460  0.19968 -0.076754 -0.0507504  0.2022189  0.36738\n## Alopgeni  0.62089 -1.06991 -0.607808  0.4085202 -0.7675817  0.14890\n## Anthodor -0.30489  0.37796 -0.285364 -0.7314654  0.3530155  0.28189\n## Bellpere -0.35166 -0.06186 -0.191560  0.2159228  0.0656089 -0.21926\n## Bromhord -0.40174 -0.11438 -0.328850  0.0020272  0.3311781 -0.38313\n## Chenalbu  0.04451 -0.03993 -0.058797 -0.0133695 -0.0062794  0.01141\n## Cirsarve -0.02004 -0.10831  0.015724  0.0909001  0.0063718  0.02241\n## Comapalu  0.16471  0.14225  0.075038  0.0042984  0.0452944 -0.15780\n## Eleopalu  1.06943 -0.01682  0.956570 -0.1633735 -0.0681172 -0.57630\n## Elymrepe -0.46520 -0.51974 -0.379373  0.6613239 -0.0042671 -0.34204\n## Empenigr  0.08235  0.07112 -0.057452 -0.0408164  0.1097807  0.19686\n## Hyporadi  0.09461  0.32440 -0.082603 -0.0367101  0.3251675  0.62435\n## Juncarti  0.65768  0.03291  0.308049  0.1056579 -0.2072807 -0.34406\n## Juncbufo  0.17582 -0.14056 -0.532656  0.0587424 -0.4920012  0.08088\n## Lolipere -1.37550 -0.76492  0.066647  0.0244836  0.5992907 -0.19415\n## Planlanc -0.91473  0.29200  0.034350 -0.8292224 -0.1507798 -0.02191\n## Poaprat  -0.79237 -0.54275 -0.258875  0.2664459  0.2614175 -0.18998\n## Poatriv  -0.31279 -1.25438 -1.230700 -0.1907205 -0.2493570 -0.49014\n## Ranuflam  0.59648  0.04493  0.299763 -0.0445985 -0.0309806 -0.25792\n## Rumeacet -0.49744 -0.03945 -0.219023 -0.8330211 -0.5462675 -0.10570\n## Sagiproc  0.26038 -0.27567 -0.367494  0.2918690 -0.1864142  0.53458\n## Salirepe  0.20256  0.44038  0.242168  0.1056696 -0.0521776  0.24572\n## Scorautu -0.30711  0.52183 -0.365094 -0.0004242  0.1648603  0.30468\n## Trifprat -0.35833 -0.03560  0.006885 -0.6508879 -0.2073384 -0.06693\n## Trifrepe -0.23813  0.30248 -0.700492 -0.2593107  0.1604908 -0.54832\n## Vicilath -0.13798  0.12689  0.043562  0.0653852  0.0278102  0.03656\n## Bracruta -0.07231  0.35720  0.477998 -0.2964003 -0.7549203  0.23978\n## Callcusp  0.42177  0.12915  0.374847 -0.0305725  0.0002153 -0.33309\n## \n## \n## Site scores (weighted sums of species scores)\n## \n##        RDA1     RDA2     PC1       PC2     PC3     PC4\n## 1  -1.20433 -0.66993  1.9859  1.074660  2.0108  0.8005\n## 2  -1.63761 -1.22512 -1.8071  1.724159  1.1211 -2.1875\n## 3  -0.41325 -2.98426 -0.3938  1.380869 -0.1922  0.1239\n## 4   0.09600 -2.56866  0.3143  1.817064  0.1274  0.4481\n## 5  -1.79219  0.23179 -0.7176 -1.888869 -0.8907 -0.7317\n## 6  -2.29807  0.77565  0.2429 -3.590960 -1.4197 -0.2869\n## 7  -2.04152 -0.05247  0.2480 -2.144774  0.2954  0.1111\n## 8   1.11426 -1.42188  0.1640 -0.499303  0.9258 -0.0638\n## 9   0.17152 -1.55487 -2.1156  1.667963 -1.4793 -1.2478\n## 10 -2.05972  0.47751 -1.4807 -1.049874  2.3911 -1.6675\n## 11 -1.41282  1.17178  1.0315  1.096567  0.2632  1.2017\n## 12  1.40040 -1.21998 -1.5692  0.392422 -3.3976  1.6584\n## 13  1.21063 -2.23426 -2.3507 -0.534503 -0.2510  0.4563\n## 14  1.68764  1.47758  0.1848 -0.008859  1.1639 -2.1575\n## 15  1.93732  1.38809  1.3152  0.094782 -0.2584 -0.9969\n## 16  3.10493 -0.43010  2.8191 -0.847596 -0.8985 -0.7305\n## 17 -0.01732  2.06719  0.1884  0.209376  0.7506  1.4412\n## 18 -0.62775  2.16502  1.1592  1.470799 -1.8057  0.7257\n## 19  0.37978  2.98990 -1.1484 -0.815907  2.1945  3.9351\n## 20  2.40211  1.61704  1.9299  0.451985 -0.6505 -0.8317\n## \n## \n## Site constraints (linear combinations of constraining variables)\n## \n##       RDA1    RDA2     PC1       PC2     PC3     PC4\n## 1  -1.5128 -1.9466  1.9859  1.074660  2.0108  0.8005\n## 2  -1.6016  0.0654 -1.8071  1.724159  1.1211 -2.1875\n## 3  -0.4006 -2.1652 -0.3938  1.380869 -0.1922  0.1239\n## 4  -0.4006 -2.1652  0.3143  1.817064  0.1274  0.4481\n## 5  -1.6016  0.0654 -0.7176 -1.888869 -0.8907 -0.7317\n## 6  -1.6016  0.0654  0.2429 -3.590960 -1.4197 -0.2869\n## 7  -1.5572 -0.9406  0.2480 -2.144774  0.2954  0.1111\n## 8   1.7795 -1.5963  0.1640 -0.499303  0.9258 -0.0638\n## 9   0.5784  0.6343 -2.1156  1.667963 -1.4793 -1.2478\n## 10 -0.5338  0.8529 -1.4807 -1.049874  2.3911 -1.6675\n## 11 -1.6461  1.0714  1.0315  1.096567  0.2632  1.2017\n## 12  0.6228 -0.3717 -1.5692  0.392422 -3.3976  1.6584\n## 13  1.7795 -1.5963 -2.3507 -0.534503 -0.2510  0.4563\n## 14  1.6462  1.4218  0.1848 -0.008859  1.1639 -2.1575\n## 15  1.6462  1.4218  1.3152  0.094782 -0.2584 -0.9969\n## 16  1.7795 -1.5963  2.8191 -0.847596 -0.8985 -0.7305\n## 17 -0.5783  1.8589  0.1884  0.209376  0.7506  1.4412\n## 18 -1.6905  2.0774  1.1592  1.470799 -1.8057  0.7257\n## 19  1.6462  1.4218 -1.1484 -0.815907  2.1945  3.9351\n## 20  1.6462  1.4218  1.9299  0.451985 -0.6505 -0.8317\n## \n## \n## Biplot scores for constraining variables\n## \n##                         RDA1     RDA2 PC1 PC2 PC3 PC4\n## as.numeric(Manure)   -0.1928 -0.98124   0   0   0   0\n## as.numeric(Moisture)  0.9990  0.04412   0   0   0   0\n# important part at the top\n# variance explained by the two variables = prop constrained = 37.09%\n# how much is explained by each RDA = see importance of components prop explained\n# PCs are the unconstrained axes\n\n# species scores = coordinates of species in the plot\n# site scores = coordinates of sites in the plot\n# biplot scores = coordinates of environmental variable vectors\n\n\nbarplot(summary(RDA)$cont$importance[2, ],  las = 2,\n        col = c(rep ('red', 2), \n                rep ('black', length(summary(RDA)$cont$importance[2, ])-2)),\n        ylab=\"proportion of variance explained\") # displays % of variance explained by PCs"
  },
  {
    "objectID": "7-Multivariate.html#clustering",
    "href": "7-Multivariate.html#clustering",
    "title": "7  Multivariate Statistics",
    "section": "7.3 Clustering",
    "text": "7.3 Clustering\n\n# hierarchical clustering: \n\nlibrary(cluster) # clustering\nlibrary(ape) # phylogenetic analyses, here to get pretty dendrogram\n\n# example for distance matrix\ndist(iris[1:3, 1:4]) # creates a distance matrix (comparison of all possible sample pairs), \n##           1         2\n## 2 0.5385165          \n## 3 0.5099020 0.3000000\n# default method =\"euclidean\", but can be changed\n\n# get Hierarchical Clustering\nhc = hclust(dist(iris[, 1:4]))\nplot(hc)\n\n\n\n\n# for colors use package ape\nplot(as.phylo(hc), tip.color = as.numeric(iris$Species)) # as.phylo converts object type \"hcclust\" into object type \"phylo\"\n\n\n\n# change plotting type:\nplot(as.phylo(hc), tip.color = as.numeric(iris$Species), type = \"fan\")\n\n\n\n\n\n\n# lets try another clustering algorithm\ndata(animals)\nstr(animals)\n## 'data.frame':    20 obs. of  6 variables:\n##  $ war: int  1 1 2 1 2 2 2 2 2 1 ...\n##  $ fly: int  1 2 1 1 1 1 2 2 1 2 ...\n##  $ ver: int  1 1 2 1 2 2 2 2 2 1 ...\n##  $ end: int  1 1 1 1 2 1 1 2 2 1 ...\n##  $ gro: int  2 2 1 1 2 2 2 1 2 1 ...\n##  $ hai: int  1 2 2 2 2 2 1 1 1 1 ...\n\n# Agglomerative Nesting \naa <- agnes(animals)\nplot(aa, which.plots = 2) #which.plots: plots only plot 2\n\n\n\n# first is banner plot...\n\n\n\n### non-hierarchical\n# kmeans \n\nset.seed(123) # choice of first k centers is random and depends on (random) seed\ncl = kmeans(iris[, 1:4], centers = 3) # centers = number of clusters to be generated\n\ncl$cluster\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [75] 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3\n## [112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3\n## [149] 3 2\nas.numeric(iris$Species)\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [149] 3 3\n\n\n# without the same seed you may have to exchange cluster 2 and 3 to be able to compare assignment to species with real species in the plot\ntemp = cl$cluster\n# temp[cl$cluster==2] = 3\n# temp[cl$cluster==3] = 2\n\ntemp\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [75] 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3\n## [112] 3 3 2 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3\n## [149] 3 2\nas.numeric(iris$Species)\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [149] 3 3\n\n# original species\npairs(iris[, 1:4], col = as.numeric(iris$Species)) \n\n\n\n\n# species found by cluster\npairs(iris[, 1:4], col = temp)\n\n\n\n\n# display samples that have been assigned to the wrong species\nsame = as.numeric(iris$Species) == temp\npairs(iris[, 1:4], col = as.numeric(same) + 1) \n\n\n\npalette()\n## [1] \"black\"   \"#DF536B\" \"#61D04F\" \"#2297E6\" \"#28E2E5\" \"#CD0BBC\" \"#F5C710\"\n## [8] \"gray62\""
  },
  {
    "objectID": "7-Multivariate.html#exercises",
    "href": "7-Multivariate.html#exercises",
    "title": "7  Multivariate Statistics",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\n\n\n\nIn this exercise you will practice analyzing multivariate datasets using:\n\nordination methods which explore higher-order correlations among variables and similarities between observations\nclustering methods which aggregate individual observations into clusters.\n\n\n7.4.0.1 Useful functions for multivariate data analyses\npairs() - create pairplot (plot all variables against each other)\nprcomp() - calculate PCA\nplot(pca_object) - plot variance explained by PC axes\nbiplot() - plot PCA\nmetaMDS() - calculate NMDS\nordiplot(nmds_object) - plot NMDS\nrda() - calculate RDA\nplot(rda_object) - plot RDA\ndist() - create distance matrix\nhclust() - perform hierarchical clustering; apply to distance matrix\nplot(as.phylo(hc_object)) - plot dendrogram from hclust output\nagnes() - another clustering algorithm\nkmeans() - perform kmeans non-hierarchical clustering; access cluster assignments using kmeans_object$cluster\n\n\n7.4.0.2 Background on the dataset\nMarine coastal ecosystems harbor the most productive and diverse communities on Earth. However, they are extremely vulnerable to climate change and human activities (such as landclaim, pollution, recreation purposes, …) As a consequence, the performance of this ecosystem has decreased considerably. To better understand these ecosystems, the dutch governmental institute (RIKZ: Rijksinstituut voor Kust en Zee) started a research project on sandy beaches to investigate the relationship between environmental factors and how they might affect benthic fauna. (from the LMU and Zuur, Ieno, Smith (2007), Chapter 12.8-12.10 and 27)\nQuestion\nWe now want to use ordination methods to explore\n\nthe variability in environmental conditions\nthe variability in species richness\n\nWe also want to use clustering methods to define 3 environmental types and hierarchically cluster the samples with respect to their species richness.\nDataset\nRead in the dataset as follows:\n\ndat = read.table(file = \"http://evol.bio.lmu.de/_statgen/Multivariate/11SS/RIKZGroups.txt\", header = T)\nhead(dat)\n\nYou already know the functions str() and summary() to get an overview of a dataset and to see which variables the dataset contains.\n\ncolumns 2:5 is species richness within four larger faunal groups\ncolumns 7:16 are different environmental variables\n\nNot important for us are:\n\ncolumn 6 (week), a time stamp\n\ncolumn 17 (sorting1), variable from the observational design\n\nLet’s get into the analysis!\n\n\n7.4.1 Unconstrained ordination (PCA)\nCarry out the following analyses:\n\nMake two PCAs, one for the environmental and one for the species richness data (see columns above).\nName the results pca_environment and pca_species.\nCreate a biplot for each PCA.\nCreate a barplot for the proportion of variance explained by each component.\n\nFor example, the result for species richness should look like this:\n\n\n\n\n\nYou need the following functions and the package vegan:\n\nprcomp()\nbiplot()\n\nbarplot()\nsummary()\n\n\nlibrary(vegan)\n\nHints\n\nDon’t forget to scale the variables in the PCA.\nIn order to get the proportion of the explained variance, have a look at the summary of your analysis. str() shows you what the summary contains. You want to specifically look at the importance, here the second row contains the proportion of variance of all components. So what we want to plot is: summary(pca_species)$importance[2,].\nDon’t forget to give your plots a title.\n\nNow, use your results to answer the questions on elearning-extern (Q 1-3) (“07_Test for Exercise in R”).\n\nIn which multivariate dataset do the first and second components explain more variance?\nWhat are the two samples that have the highest score on the respective third PCA axes?\nFrom looking at the biplot: Which environmental variables would you choose, if you have only the resource to measure three variables?\n\nNAP, angle 1 and chalk\nsalinity, penetrability and grain size\ngrain size, chalk and exposure\n\nDescribe the following correlations:\n\nsalinity and humus\nangle1 and exposure\nPolychaeta and Mollusca\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## 1.) Conduct the principle component analyses (PCAs):\n\n# a) PCA of environmental data\npca_environment = prcomp(dat[,7:16], scale = T)\nsummary(pca_environment)\n## Importance of components:\n##                           PC1    PC2    PC3    PC4     PC5    PC6     PC7\n## Standard deviation     1.8476 1.3300 1.0919 1.0297 0.99521 0.7746 0.72784\n## Proportion of Variance 0.3414 0.1769 0.1192 0.1060 0.09904 0.0600 0.05298\n## Cumulative Proportion  0.3414 0.5182 0.6375 0.7435 0.84254 0.9025 0.95551\n##                            PC8     PC9    PC10\n## Standard deviation     0.52299 0.34727 0.22541\n## Proportion of Variance 0.02735 0.01206 0.00508\n## Cumulative Proportion  0.98286 0.99492 1.00000\n\nop <- par(mfrow = c(1,2))\nbiplot(pca_environment, main = \"PCA\")            # plot the results of the PCA as a rotation matrix\nbarplot(summary(pca_environment)$importance[2,], # get the importance measure\n     main = \"Components of environment\",\n     ylab = \"Proportion of variance explained\")\n\n\n\n\n# b) PCA of species richness data\npca_species = prcomp(dat[,2:5], scale = T)\nsummary(pca_species)\n## Importance of components:\n##                           PC1    PC2    PC3    PC4\n## Standard deviation     1.1177 1.0251 1.0095 0.8251\n## Proportion of Variance 0.3123 0.2627 0.2548 0.1702\n## Cumulative Proportion  0.3123 0.5750 0.8298 1.0000\n\nbiplot(pca_species, main = \"PCA\")            # plot the results of the PCA as a rotation matrix\nbarplot(summary(pca_species)$importance[2,], # get the importance measure\n     main = \"Components of species\",\n     ylab = \"Proportion of variance explained\")\n\n\n\npar(op)\n\n# From the *summary()* output we can see that the first and second components explain more variance in the species PCA.\n\n\n##  2.) What are the two samples that have the highest score on the third PCA axis?\n\n# -> order the samples by their PC3 coordinate:\norder(pca_environment$x[,'PC3'], decreasing = T) # -> 24 is highest\n##  [1] 24 21 23 28 25 16 22 40 36 19  8 12  6 30  9  7 39 32 44  1 15  4 31 13 14\n## [26]  5 10 35 37 45 27 20 18 34 11 17 26 29 38  3 41 33  2 42 43\norder(pca_species$x[,'PC3'], decreasing = T) # -> 7 is highest\n##  [1]  9 10 22 24 40 44 32 25 12 16 33 31 19 21 41 30 23 15 20 18 36 27 17 34 26\n## [26] 45  2 43 28 39 29 42 11 38 14  4 35  3 13  5 37  1  6  8  7\n\n\n## 3.) Which environmental variables would you choose, if you have only the resource to measure three variables? \n\n# From looking at the biplot, we choose 3 variables that describe a lot of variation (i.e. have a large length in the biplot) and have little collinearity. \n# -> For example, an appropriate choice would be salinity, penetrability and grain size.\n\n\n## 4.) We can get information on the correlations of variables by looking at their representation in the biplot: \n  #  a) salinity and humus:  same direction  ->  positively correlated\n  #  b) angle1 and exposure:  opposite directions  ->  negatively correlated\n  #  c) Polychaeta and Mollusca:  almost perpendicular  ->  uncorrelated\n\n\n\n\n\n\n7.4.2 Clustering\n\n7.4.2.1 K-means\nWe want to use clustering methods to define 3 environmental types. Use the function kmeans() with centers = 3 (number of clusters to be generated). Remember to set a seed; the choice of the first k centers is random.\n\nset.seed(467456)\ncl = #...\n\nCompare the three clusters with the result of the PCA using the following code that uses another plotting framework called ggplot2:\n\nlibrary(ggfortify)\nautoplot(pca_environment, colour = cl$cluster, size = 2, loadings = TRUE, loadings.colour = 'black',\n         loadings.label = TRUE, loadings.label.size = 3, loadings.label.colour = \"black\")\n\nThe colors of the points represent the three clusters. Answer the following question on elearning-extern (Q 4-5):\n\nHow is it possible that four observations in the middle (in red - if you have used the same seed) belong to a different cluster than the observations around them (in black)?\nWhich environmental variables are on average high within the black cluster (cluster 1)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n# Create clusters using all environmental variables\nset.seed(467456)\ncl <- kmeans(dat[,7:16], centers = 3) # 3 clusters to be generated\n\n# Plot\nlibrary(ggfortify)\nautoplot(pca_environment, colour = cl$cluster, size = 2, loadings = TRUE, loadings.colour = 'black',\n         loadings.label = TRUE, loadings.label.size = 3, loadings.label.colour = \"black\")\n\n\n\n\n# To understand why the four observations in the middle belong to a different cluster than the observations around them, we must take into account, that the biplot is only a 2-dimensional representation of a more-than-2 dimensional space. Therefore, the four points in the middle will be dissimilar to the points around them with respect to a variable that is not well represented by the first two PCA axes.\n\n# Environmental variables that are on average high within cluster 1:\n# -> looking at the plot we find that exposure and grain size are high on average within cluster 1\n\n\n\n\n\n\n7.4.2.2 Hierarchical clustering\nNow we want to hierarchically cluster the samples with respect to their species richness, as shown in the following plot:\n\n\n\n\n\nCreate the same plot using the functions:\n\nhclust()\n\nplot()\nas.phylo()\n\nLoad the package ape. Have a look at the help for hclust() to read what the function does and look at the examples for further help on how to use the function. Then have a look at what the function as.phylo() does. Now, color the labels using the variable week. You can do this using the argument “tip.color =”.\nChoose the correct statement(s) about the species richness and its sampling on elearning-extern(Q6). To be able to read the plot more easily, you can click Zoom in the top pane of the Plots window in RStudio.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(ape)\n\nhc = hclust(dist(dat[, 2:5]))\nplot(as.phylo(hc), tip.color = dat$week)"
  },
  {
    "objectID": "8-MachineLearning.html#classroom-demo",
    "href": "8-MachineLearning.html#classroom-demo",
    "title": "8  Machine Learning",
    "section": "8.1 Classroom demo",
    "text": "8.1 Classroom demo\n\n# Machine Learning --------------------------------------------------------\n\nlibrary(tree)\nlibrary(randomForest)\n## randomForest 4.7-1.1\n## Type rfNews() to see new features/changes/bug fixes.\n#library(forestFloor)\n\n\n# 1. Regression with random forest:------------------------\n# We want to compare performance (%variability explained by the model) of lm \n# with performance of randomforest\n\nozone.lm <- lm(Ozone ~ ., data=airquality, na.action=na.omit)\nsummary(ozone.lm)\n## \n## Call:\n## lm(formula = Ozone ~ ., data = airquality, na.action = na.omit)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -37.014 -12.284  -3.302   8.454  95.348 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -64.11632   23.48249  -2.730  0.00742 ** \n## Solar.R       0.05027    0.02342   2.147  0.03411 *  \n## Wind         -3.31844    0.64451  -5.149 1.23e-06 ***\n## Temp          1.89579    0.27389   6.922 3.66e-10 ***\n## Month        -3.03996    1.51346  -2.009  0.04714 *  \n## Day           0.27388    0.22967   1.192  0.23576    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 20.86 on 105 degrees of freedom\n##   (42 observations deleted due to missingness)\n## Multiple R-squared:  0.6249, Adjusted R-squared:  0.6071 \n## F-statistic: 34.99 on 5 and 105 DF,  p-value: < 2.2e-16\n# lm explains about 62% of the variance in the data\n\n \n# Let's look now at how a decision tree looks like:\n\n#install.packages(\"tree\") # not available for R version 3.5.3\n#devtools::install_version('tree', version = '1.0-39')\n# we use this, because newest version of tree does not work with R version\n\nozone.tree = tree(Ozone ~ ., data=airquality[complete.cases(airquality),])\nplot(ozone.tree)\ntext(ozone.tree)\n\n\n\n# this is based on all variables and all observations\n#### explain how decision trees are produced with 2-variable-example on the blackboard:\n#### correlation of variable A with Ozone > split at a point so that variance left and right of the split is minimized, remember outcome \n#### correlate Ozone with variable B > split > minimize variance >> compare this with performance of variable A >> if A removes more variability, choose split of A for first node\n#### then split dataset according to the split  and redo the whole thing for the two groups that were generated >>> leads to 1 decision tree for the dataset\n\n# plot:\n# split-points are given at nodes of the tree\n# importance of different variables related to how often they appear at these nodes\n\n\n\n# Now, let's look at the random forest\n# Difference here: for each iteration, only a random subset \n# (about 66%) of observations) and a random subset of the variables is used\n# > we get a large number of different trees whose prediction ability is worse \n# than a tree derived from the whole data\n# > then: average over these trees (similar to model averaging)\n# >> get 1 final tree which predicts better than each single tree \n\n\nozone.rf <- randomForest(Ozone ~ ., data=airquality, mtry=3,\n                         importance=TRUE, na.action=na.omit, keep.inbag=T)\n# mtry = number of predictors used in each split\nprint(ozone.rf) # explains about 72% of the variance >> = better than lm!\n## \n## Call:\n##  randomForest(formula = Ozone ~ ., data = airquality, mtry = 3,      importance = TRUE, keep.inbag = T, na.action = na.omit) \n##                Type of random forest: regression\n##                      Number of trees: 500\n## No. of variables tried at each split: 3\n## \n##           Mean of squared residuals: 308.604\n##                     % Var explained: 71.88\n\n# random forest does not give p-values for explanatory variables, but \n# you can get the variables importance for predictions (no causal assumptions here!):\n \nimportance(ozone.rf)\n##           %IncMSE IncNodePurity\n## Solar.R  9.832887     10903.699\n## Wind    22.007222     41648.054\n## Temp    43.631946     56064.801\n## Month    2.044271      1597.533\n## Day      1.022438      6334.784\n# the same as a plot:\nvarImpPlot(ozone.rf)\n\n\n\n\n\n\n# 2. Classification with random forest:-------------------\n#Let's do a decision tree on the whole data first (all observations, all predictors):\niris.tree <- tree(Species ~ ., data=iris)\nplot(iris.tree)\ntext(iris.tree)\n\n\n\n\n# then try randomforest:\nmodel <- randomForest(Species ~ ., data=iris, importance=TRUE, ntree=500, mtry = 2, do.trace=100,  keep.inbag=T)\n## ntree      OOB      1      2      3\n##   100:   4.00%  0.00%  6.00%  6.00%\n##   200:   4.00%  0.00%  6.00%  6.00%\n##   300:   4.00%  0.00%  6.00%  6.00%\n##   400:   4.00%  0.00%  6.00%  6.00%\n##   500:   4.00%  0.00%  6.00%  6.00%\n# two random steps in RF:\n# 1. bootstrap for each tree\n# 2. mtry: random subset of predictors in each split\n\n#plot importance of predictors:\nvarImpPlot(model)\n\n\n\nimportance(model)\n##                 setosa  versicolor virginica MeanDecreaseAccuracy\n## Sepal.Length  5.648370  6.80421839  7.960420            10.692420\n## Sepal.Width   4.090753 -0.05394849  4.868079             4.709399\n## Petal.Length 23.149126 34.86959187 29.210133            35.454773\n## Petal.Width  21.444086 33.68284448 30.343499            33.669313\n##              MeanDecreaseGini\n## Sepal.Length         9.267761\n## Sepal.Width          2.080905\n## Petal.Length        44.127351\n## Petal.Width         43.836169\n#how many samples were distributed to the wrong species?\nprint(model)\n## \n## Call:\n##  randomForest(formula = Species ~ ., data = iris, importance = TRUE,      ntree = 500, mtry = 2, do.trace = 100, keep.inbag = T) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 2\n## \n##         OOB estimate of  error rate: 4%\n## Confusion matrix:\n##            setosa versicolor virginica class.error\n## setosa         50          0         0        0.00\n## versicolor      0         47         3        0.06\n## virginica       0          3        47        0.06\n#> 4% average error rate"
  },
  {
    "objectID": "9-DataScienceSkills.html#exercise",
    "href": "9-DataScienceSkills.html#exercise",
    "title": "9  Data handling",
    "section": "9.1 Exercise",
    "text": "9.1 Exercise\nIn this exercise you will practice common steps of the data preparation procedure using a dataset that was gathered in a forest in Switzerland.\nThe main steps are as follows (to solve the tasks, please carefully read the detailed instructions and hints below):\n\nRead the provided datasets treedata.csv and species.csv to R.\nHave a look at the dataset properties.\nFind values that prevent a column you expect to be numeric to do so.\nDoes the dataset contain NA-values in height? How many?\nHave a look at the data: Check for implausible values. For now, remove these values.\nAdd the species names to the dataset.\nCreate a new dataset containing only trees with both, height and dbh measurements.\nAre there any correlations within the new dataset?\nRemove all trees with rem = F4 from the dataset\nCalculate mean dbh by species.\n\n\nTo thoroughly check the dataset and perform the operations, you will need the following functions:\n\nread.csv(): Check the different options using ?read.csv\nstr(): Structure of an object\ntable(): Check the optional arguments!\nmerge(): Combine to data.frames\nas.character(): Change a vector’s class to character\nas.numeric(): Change a vector’s class to numeric.\n%in%\nis.na()\nmax()\nsummary()\ncomplete.cases()\ncor.test()\n%>% and group_by() and summarize() from the dlyr package (check demonstration Part 2)\n\nRegarding the solutions, note we don’t expect you to come up with exactly this code - there are many ways to solve the problem. The solutions just give you an idea of a possible solution.\n\n9.1.1 1. Read data\nRead the provided datasets treedata.csv and species.csv to R. Use the option stringsAsFactors = FALSE in the function read.csv.\nRead the dataset treedata.csv and call it treedata. It has the following columns:\n\nspecies_code: tree species code\ndbh: diameter at breast height (1.3 m) [cm]\nheight: total height of the tree [m]. Measured only on a subset of trees.\nrem: coded values, containing remarks\n\nRead the dataset species.csv and call it species. The dataset consists of the following columns:\n\nspecies_code: tree species code (matching the one used in treedata.csv)\nspecies_scientific: Scientific species name\nspecies_english: English species name\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, you read in the file using read.csv. You have to specify the correct separator for your dataset, here this is “;”.\n\ntreedata <- read.csv('Where/ever/you/put/it/treedata.csv', sep = \";\")\nspecies <- read.csv('Hopefully/in/the/same/folder/species.csv', sep = \";\")\n\n\n\n\n\n\n9.1.2 2. Dataset properties\nHave a look at the properties of the dataset:\n\nWhich classes do the columns have?\nDid you expect these classes?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe data.frame dat contains 4 columns: species, dbh (diameter at breast height [cm]), height [m] and rem, a remark. We expect the following formats:\n\n\n\ncolumn\nformat\n\n\n\n\nspecies\ncharacter\n\n\ndbh\nnumeric\n\n\nheight\nnumeric\n\n\nrem\nfactor\n\n\n\nUsing str we get an overview of the structure of a dataset:\n\nstr(treedata)\n## 'data.frame':    287 obs. of  4 variables:\n##  $ species_code: int  121 121 411 411 411 431 411 411 411 121 ...\n##  $ dbh         : chr  \"19.3\" \"21.3\" \"43\" \"25.8\" ...\n##  $ height      : num  NA NA 37.7 NA 34.4 44.4 NA NA NA NA ...\n##  $ rem         : chr  \"\" \"\" \"P7\" \"F2\" ...\n\nColumn dbh is a character, although we would have expected this one to be class numeric. This indicates, that a letter or special characters are in that column (we do not want these to be in there at all!).\n\n\n\n\n\n9.1.3 3. Turn character to numeric\nOne column, which we expect to be numeric, is of class character. Find the value causing this column to be character, set it to NA and turn the column into class numeric.\nNote that using ‘is.numeric()’ is not enough, if the column is a factor. This may be the case if you have used the option stringAsFactor = T in read.csv or an older version of R. Use a combination of ‘as.character()’ and ‘as.numeric()’ in that case.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe suspect dbh to contain a character and we want to remove this. With the function ‘table()’, we can check all existing values in the column. There seems to be an ‘X’ in the data.\n\ntable(treedata$dbh)\n## \n## 10.1 10.2 10.4 10.6 10.7 10.9 11.2 11.4 11.5 11.9 12.3 12.5   13 13.2 13.3 13.4 \n##    1    2    2    1    1    1    2    1    1    1    1    2    1    1    1    1 \n## 13.8 13.9   14 14.4 14.8 14.9 15.2 15.4 15.5 15.8   16 16.2 16.4 16.6 16.8 17.1 \n##    3    1    1    6    1    1    3    1    1    1    1    1    1    1    4    1 \n## 17.2 17.4 17.5 17.6 17.8 17.9 18.1 18.2 18.3 18.7 18.8   19 19.1 19.2 19.3 19.4 \n##    2    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n## 19.5 19.8   20 20.2 20.4 20.7 20.8 20.9 2030 21.3 21.6 21.8 21.9   22 22.2 22.3 \n##    3    3    1    2    1    1    2    2    1    1    4    1    1    1    2    1 \n## 22.6 22.7 22.9 23.2 23.3 23.6 23.7 23.8   24 24.2 24.3 24.4 24.9   25 25.2 25.3 \n##    2    1    2    1    1    2    1    1    2    2    2    1    2    1    1    1 \n## 25.4 25.8   26 26.2 26.3 26.4 26.6 27.4 27.5 27.6 27.8 28.2 28.4 28.6 28.8 29.2 \n##    4    3    1    2    1    1    1    2    2    1    1    1    2    1    1    1 \n## 29.4 29.6 29.7 29.8 30.1 30.2 30.4 30.8 31.2 31.3 31.4 31.9   32 32.2 32.3 32.4 \n##    2    1    1    2    1    1    1    2    1    1    2    1    1    3    1    1 \n## 32.9   33 33.2 33.3 33.4 33.8 33.9 34.2 34.4 34.6 34.7 34.8   35 35.8 36.5 36.6 \n##    1    1    1    1    1    3    1    5    1    1    1    1    1    1    1    1 \n## 36.8 37.1 37.2 37.4 37.8 38.1 38.2 38.3 38.5 39.2 39.4 39.7 39.8    4  4.2  4.6 \n##    2    1    1    1    2    1    1    1    1    1    1    1    1    3    3    1 \n##  4.7   40 40.6 40.8 41.3 41.4 41.5 42.3 42.4   43 43.2 43.4 43.6 44.2 44.5 44.8 \n##    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 45.2 45.7 47.8   48 48.7 48.8   49 49.7 49.8  5.1  5.2  5.4  5.5  5.6  5.7  5.8 \n##    1    1    1    1    1    1    1    1    1    1    3    3    1    1    1    2 \n##  5.9 51.2  6.2  6.3  6.4  6.5  6.6  6.8  6.9    7  7.2  7.3  7.4  7.6  7.8  8.1 \n##    2    1    4    2    1    1    2    1    2    1    1    1    2    1    2    1 \n##  8.2  8.3  8.5  8.8  8.9    9  9.4  9.5  9.6  9.9    X \n##    3    2    1    1    2    1    1    2    2    1    1\n\nTo automatically search for characters, we can check if dbh contains a character that is part of LETTERS (capital letters) or letters:\n\ntreedata[treedata$dbh %in% LETTERS | treedata$dbh %in% letters,]\n##     species_code dbh height rem\n## 159          411   X   27.2  F4\n\nA more advanced option would be to use grepl. If we are using the solution above, we will only find the value if it is exactly one character. Things get a bit more complicated, if we have special characters, e.g, a *.\n\n\nx <- rep(c(1, 3, 5, '*', 'AA', ',', 9), 2)\nx[grepl(\"^[A-Za-z]+$\", x, perl = T)]\n## [1] \"AA\" \"AA\"\nx[!grepl('[^[:punct:]]', x, perl =T)]\n## [1] \"*\" \",\" \"*\" \",\"\n\nWe want to set the X in dbh to NA (probably, this is a transcription error, so one could also have a look at the field forms…).\n\n\ntreedata$dbh[treedata$dbh == 'X'] <- NA\nstr(treedata)\n## 'data.frame':    287 obs. of  4 variables:\n##  $ species_code: int  121 121 411 411 411 431 411 411 411 121 ...\n##  $ dbh         : chr  \"19.3\" \"21.3\" \"43\" \"25.8\" ...\n##  $ height      : num  NA NA 37.7 NA 34.4 44.4 NA NA NA NA ...\n##  $ rem         : chr  \"\" \"\" \"P7\" \"F2\" ...\n\nJust removing the ‘X’ does not turn a character to numeric! R provides the function as.numeric, which might be of use in this case.\n\ntreedata$dbh <- as.numeric(treedata$dbh)\nhead(treedata$dbh)\n## [1] 19.3 21.3 43.0 25.8 38.5 36.8\n\n\n\n\n\n\n9.1.4 4. NA- values in height\nCheck for NA’s in the column height:\n\nHow many NA’s do appear in this column?\nDid you expect this column to contain NA’s? Why?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(treedata)\n##   species_code        dbh              height           rem           \n##  Min.   :101.0   Min.   :   4.00   Min.   :  4.60   Length:287        \n##  1st Qu.:121.0   1st Qu.:  11.60   1st Qu.: 21.75   Class :character  \n##  Median :411.0   Median :  21.60   Median : 29.10   Mode  :character  \n##  Mean   :329.8   Mean   :  29.22   Mean   : 26.30                     \n##  3rd Qu.:411.0   3rd Qu.:  31.40   3rd Qu.: 32.62                     \n##  Max.   :920.0   Max.   :2030.00   Max.   :110.88                     \n##                  NA's   :1         NA's   :221\n\nsum(is.na(treedata$height))\n## [1] 221\ntable(is.na(treedata$height))\n## \n## FALSE  TRUE \n##    66   221\n\nnrow(treedata[is.na(treedata$height),])\n## [1] 221\ntable(treedata$height, useNA = 'ifany')\n## \n##    4.6    4.7    4.8    5.7    5.8    6.1    6.2    6.8    7.3    7.9   10.8 \n##      1      2      1      1      1      1      1      1      1      1      1 \n##   11.1   11.3   11.5   14.2   21.7   21.9     22   22.4   23.3   23.8   24.5 \n##      1      1      1      1      1      1      1      2      1      2      1 \n##   24.8   26.1   27.2   27.5   27.8   28.6     29   29.2   29.4   29.8   29.9 \n##      1      1      1      2      1      1      1      1      1      1      1 \n##   30.2   30.3   30.4   31.3   31.5   31.9     32   32.1   32.3   32.4   32.7 \n##      1      2      1      1      1      1      2      1      1      1      1 \n##   32.8   33.2   33.5   33.8   34.2   34.4   35.4   35.8   37.7   38.2   38.6 \n##      1      1      1      1      1      1      1      1      2      1      1 \n##     39   40.8   44.4 110.88   <NA> \n##      1      1      1      1    221\n\n\nThe dataset contains 221 NA.\nSince height has been measured only on a subset of the trees, we expect this column to contain NA-values.\n\n\n\n\n\n\n9.1.5 5. Implausible values\nThe dataset contains some implausible values (completely out of range!). Find and replace these values with NA.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWhat values are implausible? The dataset contains different species. A value which is plausible for species A might be implausible for species B. However, for now, we will not go into the details here.\nDo a visual check of the dataset\n\nboxplot(treedata$height, main = 'Height')\n\n\n\nboxplot(treedata$dbh, main ='DBH')\n\n\n\n\nmax(treedata$dbh, na.rm = T)\n## [1] 2030\nmax(treedata$height, na.rm = T)\n## [1] 110.88\n\ntreedata[treedata$dbh > 500 & !is.na(treedata$dbh),]\n##     species_code  dbh height rem\n## 114          411 2030     NA  F2\ntreedata[treedata$height > 50 & !is.na(treedata$height),]\n##     species_code  dbh height rem\n## 234          411 36.6 110.88\n\nThere seems to be one outlier in both datasets which can be seen as implausible: No tree is more than 100 m of height and no tree has a diameter > 20 m (These values can be considered implausible for trees in Switzerland).\nWe will now remove these values from our dataset by setting them to NA - this might not always be the best option, there are also statistical models that can account for such errors!\n\ntreedata$dbh[treedata$dbh > 500] <- NA\ntreedata$height[treedata$height > 50] <- NA\n\nboxplot(treedata$dbh, main = 'DBH')\n\n\n\nboxplot(treedata$height, main = 'Height')\n\n\n\n\n\n\n\n\n\n9.1.6 6. Add species names\nAdd the species names from the species dataset to the treedata dataset.\nHint: ?merge\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse merge to add species names to the dataset treedata. For adding only one column, match is a helpful function.\n\ntreedata = merge(treedata, species, by = \"species_code\")\nhead(treedata)\n##   species_code  dbh height rem         species_scientific species_english\n## 1          101  8.1    5.7     Picea abies (L.) H. Karst.   Norway Spruce\n## 2          101 10.4     NA     Picea abies (L.) H. Karst.   Norway Spruce\n## 3          101  5.2     NA  S0 Picea abies (L.) H. Karst.   Norway Spruce\n## 4          101  9.6     NA     Picea abies (L.) H. Karst.   Norway Spruce\n## 5          101  8.2    6.2     Picea abies (L.) H. Karst.   Norway Spruce\n## 6          101  7.4    6.1     Picea abies (L.) H. Karst.   Norway Spruce\n\n# Alternative using match():\n# treedata$species_english <- species$species_english[match(treedata$species_code, species$species_code)]\n\n\n\n\n\n\n9.1.7 7. Remove F4\nThe remark F4 indicates, that the dbh-measurments might be flawed. Remove these trees from treedata.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntreedata <- treedata[!treedata$rem %in% 'F4',]\n\n\n\n\n\n\n9.1.8 8. Select trees that contain both dbh and height measurements\nSubset treedata to trees where both height and dbh measurements were carried out. Call the result dbh_height.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be done using complete.cases using\n\ndbh_height <- treedata[complete.cases(treedata$dbh, treedata$height),]\n\n\n\n\n\n\n9.1.9 9. Correlations?\nCheck for correlations in the dataset dbh_height. Use a test and plot height vs dbh.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncor.test(dbh_height$dbh\n         , dbh_height$height)\n## \n##  Pearson's product-moment correlation\n## \n## data:  dbh_height$dbh and dbh_height$height\n## t = 13.147, df = 47, p-value < 2.2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.8066074 0.9348068\n## sample estimates:\n##       cor \n## 0.8866893\n\nplot(dbh_height$dbh\n     , dbh_height$height\n     , ylab = 'Height'\n     , xlab = 'DBH')\n\n\n\n\n\n\n\n\n\n9.1.10 10. Calculate mean dbh per species.\nFor calculating summary statistics, the dplyr package is really helpful. It is part of the tidyverse environment, which was designed for data science. If you work with large and complex datasets or if you have to derive many new variables, I really recommend that you have a look at this. Also, the syntax for dplyr is quite intuitive.\nFor this exercise, use the dplyr package to calculate mean dbh per species. If you need help on this, check the demonstration of Part 2 where we calculated summary statistics for groups using the dplyr package!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\ntreedata %>% \n  group_by(species_english) %>% \n  summarize(N = n(),\n            meanDBH = mean(dbh, na.rm = T), \n            sdDBH = sd(dbh, na.rm = T))\n## # A tibble: 10 × 4\n##    species_english         N meanDBH  sdDBH\n##    <chr>               <int>   <dbl>  <dbl>\n##  1 Beech                 131   29.6  10.1  \n##  2 Commom Yew              2   18.8   0.212\n##  3 European Ash           12   30.6   4.14 \n##  4 European Silver Fir    75   11.8   5.55 \n##  5 Ivy                     6    5.13  0.927\n##  6 Little Leaf Linden      4   13.3   9.88 \n##  7 Norway Maple            9   28.2  11.1  \n##  8 Norway Spruce          15    7.77  1.89 \n##  9 Scotch Elm              1   17.9  NA    \n## 10 Sycamore               12   24.0   3.91"
  }
]