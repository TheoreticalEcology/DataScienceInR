[
  {
    "path": "index.html",
    "id": "webexercises",
    "chapter": "Webexercises",
    "heading": "Webexercises",
    "text": "Web Exercise template created #PsyTeachR team University Glasgow, based ideas Software Carpentry. template shows instructors can easily create interactive web documents students can use self-guided learning.webexercises package provides number functions use inline R code code chunk options create HTML widgets (text boxes, pull menus, buttons reveal hidden content). Examples given next. Render book see work.NOTE: use widgets compiled HTML files, need JavaScript-enabled browser.",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "example-questions",
    "chapter": "Webexercises",
    "heading": "0.1 Example Questions",
    "text": "functions optimised used inline r code, can also use code chunks setting chunk option results = 'asis' using cat() display result widget.function loads package already computer? install.packageinstall.packageslibrarylibraries",
    "code": "\n# echo = FALSE, results = 'asis'\nopts <- c(\"install.package\", \n            \"install.packages\", \n            answer = \"library\", \n            \"libraries\")\n\nq1 <- mcq(opts)\n\ncat(\"What function loads a package that is already on your computer?\", q1)"
  },
  {
    "path": "index.html",
    "id": "fill-in-the-blanks-fitb",
    "chapter": "Webexercises",
    "heading": "0.1.1 Fill-In-The-Blanks (fitb())",
    "text": "Create fill---blank questions using fitb(), providing answer first argument.2 + 2 can also create questions dynamically, using variables R session.square root 16 : blanks case-sensitive; care case, use argument ignore_case = TRUE.letter D? want ignore differences whitespace use, use argument ignore_ws = TRUE (default) include spaces answer anywhere acceptable.load tidyverse package? can set one possible correct answer setting answers vector.Type vowel: can use regular expressions test answers complex rules.Type 3 letters: ",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "multiple-choice-mcq",
    "chapter": "Webexercises",
    "heading": "0.1.2 Multiple Choice (mcq())",
    "text": "\"Never gonna give , never gonna: let goturn downrun awaylet \"\"bless rainsguess rainssense rain Africa\" -Toto",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "true-or-false-torf",
    "chapter": "Webexercises",
    "heading": "0.1.3 True or False (torf())",
    "text": "True False? can permute values vector using sample(). TRUEFALSE",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "longer-mcqs-longmcq",
    "chapter": "Webexercises",
    "heading": "0.1.4 Longer MCQs (longmcq())",
    "text": "answers long, sometimes drop-select box gets formatted oddly. can use longmcq() deal . Since answers long, probably best set options inside R chunk echo=FALSE.p-value?true 95% confidence interval mean?",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "checked-sections",
    "chapter": "Webexercises",
    "heading": "0.2 Checked sections",
    "text": "Create sections class webex-check add button hides feedback pressed. Add class webex-box draw box around section (use styles).going learn lot: TRUEFALSE",
    "code": ""
  },
  {
    "path": "index.html",
    "id": "hidden-solutions-and-hints",
    "chapter": "Webexercises",
    "heading": "0.3 Hidden solutions and hints",
    "text": "can fence solution area hidden behind button using hide() solution unhide() , inline R code. Pass text want appear button hide() function.solution RMarkdown code chunk, instead using hide() unhide(), simply set webex.hide chunk option TRUE, set string wish display button.Recreate scatterplot , using built-cars dataset.See documentation plot() (?plot)",
    "code": "\nplot(cars$speed, cars$dist)"
  },
  {
    "objectID": "4A-Exercise.html#analyzing-the-regrowth-dataset",
    "href": "4A-Exercise.html#analyzing-the-regrowth-dataset",
    "title": "Exercise - Simple linear regression",
    "section": "Analyzing the “regrowth” dataset",
    "text": "Analyzing the “regrowth” dataset\n\n\n\n\n\n\nWarning\n\n\n\nImagine you have a garden with some fruit trees and you were thinking of adding some berry bushes between them. However, you don’t want them to suffer from malnutrition so you want to estimate the volume of root biomass as a function of the fruit biomass.\nCarry out the following tasks\n\nPerform a simple linear regression for the influence of fruit biomass on root biomass.\nVisualize the data and add the regression line to the plot.\n\nYou will need the following functions:\n\nlm()\nsummary()\nplot()\nabline()\n\n\nQuestion\nYou have performed a simple linear regression for the influence of fruit biomass on root biomass.\nWhich of the following statements are correct? (More than one are correct)\n\n Root biomass is not significantly affected by fruit biomass. Fruit biomass explains most of the variance (&gt;50%) in the root biomass. At a fruit biomass of 70, the model would predict root biomass of about 4.18 + 0.05*70. At a fruit biomass of 0, the model predicts a root biomass of about 4.18.\n\n\n\n\n\n\nClick here to see the solution\n\n### Solution\n\nRoot biomass is not significantly affected by fruit biomass. WRONG: Look at the p-value for the slope (2nd row in the regression table below Pr(&gt;|t|))\nFruit biomass explains most of the variance (&gt;50%) in the root biomass. CORRECT: The proportion of variance explained by the model is given by R2.\nAt a fruit biomass of 70, the model would predict root biomass of about \\(4.18 + 0.05*70\\). CORRECT: The linear equation for the model is: \\(y = a*x + b\\) that is \\(Root = slope*Fruit + intercept\\)\nAt a fruit biomass of 0, the model predicts a root biomass of about 4.18. CORRECT: This is the intercept (1st row in the regression table below Estimate)\n\nThis is the code that you need to interpret the results.\n\nlibrary(EcoData)\n# simple linear regression\nfit &lt;- lm(Root ~ Fruit, data = regrowth)\n\n# check summary for regression coefficient and p-value\nsummary(fit)\n## \n## Call:\n## lm(formula = Root ~ Fruit, data = regrowth)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.25105 -0.69970 -0.01755  0.66982  1.63933 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 4.184256   0.337987  12.380  6.6e-15 ***\n## Fruit       0.050444   0.005264   9.584  1.1e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8111 on 38 degrees of freedom\n## Multiple R-squared:  0.7073, Adjusted R-squared:  0.6996 \n## F-statistic: 91.84 on 1 and 38 DF,  p-value: 1.099e-11\n\n# plot root explained by fruit biomass\nplot(Root ~ Fruit, data = regrowth, \n     ylab = \"Root biomass in cubic meters\",\n     xlab = \"Fruit biomass in g\")\n\nabline(fit) # add regression line\nabline(v = 70, col = \"purple\") # add line at x value (here fruit biomass of 70g)\nabline(h = 4.184256 + 0.050444*70, col = \"brown\") # add line at y value according to x = 70 using the intercept and regression coefficient of x"
  },
  {
    "objectID": "4A-Exercise.html#analyzing-the-birdabundance-dataset",
    "href": "4A-Exercise.html#analyzing-the-birdabundance-dataset",
    "title": "Exercise - Simple linear regression",
    "section": "Analyzing the “birdabundance” dataset",
    "text": "Analyzing the “birdabundance” dataset\nThe dataset provides bird abundances in forest fragments with different characteristics in Australia. We want to look at the relationship of the variables “abundance”, “distance” and “grazing”.\n\n\n\n\n\n\nQuestions\n\n\n\nFirst, answer the following questions?:\n\nWhat is the most reasonable research question regarding these variables?\n\n\n How is grazing influenced by distance / abundance? How is distance influenced by grazing / abundance? How is abundance influenced by distance / grazing?\n\n\nWhat is the response variable?\n\n\n abundance distance grazing\n\n\nWhat is the predictor variable?\n\n\n either grazing or distance either abundance or distance either abundance or grazing\n\nThen, perform the following tasks:\n\nFit a simple linear regression relating the response variable to the categorical predictor (that is the one with five levels, make sure that it is indeed a factor using as.factor())\nApply an ANOVA to your model.\n\nYou may need the following functions:\n\nlm()\nsummary()\nanova()\n\nUse your results to chose the correct statement(s):\nYou have now fitted a simple linear regression with a categorical predictor and analyzed it. Which of the following statements are correct? (several statements are correct)\n\n The maximum likelihood estimate of bird abundance for grazing intensity 1 is 28.623. We can see in the regression table that the difference between grazing intensity 3 and 4 is significant. The non-significant p-value for grazing intensity 2 indicates that the data are compatible with the null hypothesis “H0: the bird abundance at grazing intensity 2 is on average 0.” The confidence interval for the estimate of the intercept is the smallest. The difference between grazing intensity 1 and 3 is significant. Grazing intensity in general has a highly significant effect (&lt; 0.001) on bird abundance.\n\n\n\n\n\nClick here to see the solution\n\n\nSolution\n\nThe maximum likelihood estimate of bird abundance for grazing intensity 1 is 28.623. CORRECT: When the predictor is a factor, the intercept equals the first factor level (by default, this follows an alphabetical order).\nWe can see in the regression table that the difference between grazing intensity 3 and 4 is significant. WRONG: Comparisons are always related to the intercept, i.e. to the first factor level. For comparisons among other factor levels we need post-hoc tests.\nThe non-significant p-value for grazing intensity 2 indicates that the data are compatible with the null hypothesis “H0: the bird abundance at grazing intensity 2 is on average 0.” WRONG: Comparisons are always related to the intercept, i.e. to the first factor level. Only the test for the intercept has H0: mean = 0.\n\nA reasonable research question is how abundance is influenced by distance and/or grazing. Here, the response variable is abundance, while the predictors are distance and/or grazing.\nThis is the code that you need to interpret the results.\n\n# change variable from integer to factor\nbirdabundance$GRAZE &lt;- as.factor(birdabundance$GRAZE) \nfit &lt;- lm(ABUND ~ GRAZE, data = birdabundance)\nsummary(fit)\n## \n## Call:\n## lm(formula = ABUND ~ GRAZE, data = birdabundance)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -18.3867  -4.1159   0.0269   5.1484  16.4133 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   28.623      2.086  13.723  &lt; 2e-16 ***\n## GRAZE2        -6.673      3.379  -1.975   0.0537 .  \n## GRAZE3        -7.336      2.850  -2.574   0.0130 *  \n## GRAZE4        -8.052      3.526  -2.284   0.0266 *  \n## GRAZE5       -22.331      2.950  -7.571 6.85e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.52 on 51 degrees of freedom\n## Multiple R-squared:  0.5449, Adjusted R-squared:  0.5092 \n## F-statistic: 15.27 on 4 and 51 DF,  p-value: 2.846e-08\n\n# anova to check global effect of the factor grazing intensity\nanova(fit)\n## Analysis of Variance Table\n## \n## Response: ABUND\n##           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \n## GRAZE      4 3453.7  863.42  15.267 2.846e-08 ***\n## Residuals 51 2884.2   56.55                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# boxplot\nplot(ABUND ~ GRAZE, data = birdabundance)"
  },
  {
    "objectID": "4A-Exercise.html#model-validation-residual-checks",
    "href": "4A-Exercise.html#model-validation-residual-checks",
    "title": "Exercise - Simple linear regression",
    "section": "Model validation: Residual checks",
    "text": "Model validation: Residual checks\nNow, we will have a closer look at model diagnostics and residual checks in particular. Of course, we should have done this for all models above as well (we simply didn’t do this because of time restrictions). So remember that you always have to validate your model, if you want to be sure that your conclusions are correct.\nFor this exercise, you can prepare a dataset yourself called “dat” with the variables “x” and “y”. Simply copy the following code to generate the data:\n\nset.seed(234)\nx = rnorm(40, mean = 10, sd = 5)\ny = 10 - 2*x + 0.9 * x^2 + rnorm(40, mean=5, sd = 20)\ndat &lt;- data.frame(x, y)\nhead(dat)\n##           x          y\n## 1 13.303849 152.093910\n## 2 -0.264915   6.831275\n## 3  2.503970  45.207691\n## 4 17.356166 240.274237\n## 5 17.295693 240.917066\n## 6 10.700695 117.691234\n\nPerform the following tasks:\n\n\n\n\n\n\nWarning\n\n\n\n\nFit a simple linear regression.\nCheck the residuals.\nPerform another simple linear regression with a modified formula, if needed.\nCreate a scatter plot of the data and add a regression line for the first fit in black and one for the second fit in red. The second model cannot be plotted with the abline() function. Use the following code instead:\n\n\nlines(sort(x), predict(fit2, newdata = data.frame(x = sort(x))), col = \"red\")\n\nYou may also need the following functions:\n\nlm()\nsummary()\npar(mfrow = c(2, 2))\nplot()\nabline()\n\nUse your results to answer the following questions:\n\nWhat pattern do the residuals of the first regression model show when plotted against the fitted values?\nWhat do you have to do to improve your first regression model?\nIdentify the correct statement(s) about the residuals of the modified model.\n\n\n\n\n\nClick here to see the solution\n\n\nset.seed(234)\nx = rnorm(40, mean = 10, sd = 5)\ny = 10 - 2*x + 0.9 * x^2 + rnorm(40, mean=5, sd = 20)\ndat &lt;- data.frame(x, y)\n\n# simple linear regression\nfit &lt;- lm(y ~ x, dat)\n\n# check residuals\nop = par(mfrow=c(2,2))\nplot(fit) # residuals show a parabolic relationship (see first plot)  -&gt; to improve, fit a quadratic relationship\n\n\n\npar(op)\n\n# scatter plot\nplot(y ~ x, data = dat)\nabline(fit)\n\n\n\n\nsummary(fit) # significantly positively correlated, but this doesn't tell the full story because the residuals are not okay\n## \n## Call:\n## lm(formula = y ~ x, data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -39.884 -22.208  -4.948  10.602 118.164 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   -8.459     10.973  -0.771    0.446    \n## x             11.465      1.019  11.248 1.18e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 32.11 on 38 degrees of freedom\n## Multiple R-squared:  0.769,  Adjusted R-squared:  0.763 \n## F-statistic: 126.5 on 1 and 38 DF,  p-value: 1.176e-13\n\n# improved regression model\nfit2 = lm(y ~ x + I(x^2), dat)\n\n# check residuals\nop = par(mfrow=c(2,2))\nplot(fit2) # no pattern in residuals anymore (first plot) -&gt; fit is fine\n\n\n\npar(op)\n\n# scatter plot\nplot(y ~ x, data = dat)\nabline(fit)\nlines(sort(x), predict(fit2, newdata = data.frame(x = sort(x))), col = \"red\")\n\n\n\n\n\nsummary(fit2) # significantly negatively correlated, trustworthy now, because residuals are sufficiently uniformly distributed (first plot in plot(fit2))\n## \n## Call:\n## lm(formula = y ~ x + I(x^2), data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -33.174 -11.444   0.938  10.164  40.666 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 17.87505    6.00812   2.975  0.00513 ** \n## x           -1.10100    1.27706  -0.862  0.39417    \n## I(x^2)       0.80752    0.07526  10.730 6.49e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 16.05 on 37 degrees of freedom\n## Multiple R-squared:  0.9438, Adjusted R-squared:  0.9408 \n## F-statistic: 310.9 on 2 and 37 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "4B-Exercise.html#analyzing-the-mtcars-dataset",
    "href": "4B-Exercise.html#analyzing-the-mtcars-dataset",
    "title": "Exercise - Multiple Linear Regression",
    "section": "Analyzing the mtcars dataset",
    "text": "Analyzing the mtcars dataset\nImagine a start up company wants to rebuild a car with a nice retro look from the 70ies. The car should be modern though, meaning the fuel consumption should be as low as possible. They’ve discovered the mtcars dataset with all the necessary measurements and they’ve somehow heard about you and your R skills and asked you for help. And of course you promised to help, kind as you are.\nThe company wants you to find out which of the following characteristics affects the fuel consumption measured in miles per gallon (mpg). Lower values for mpg thus reflect a higher fuel consumption. The company wants you to include the following variables into your analysis:\n\nnumber of cylinders (cyl)\nweight (wt)\nhorsepower (hp)\nwhether the car is driven manually or with automatic (am)\n\nIn addition, Pawl, one of the founders of the company suggested that the effect of weight (wt) might be irrelevant for powerful cars (high hp values). You are thus asked to test for this interaction in your analysis as well.\n\n\n\n\n\n\nQuestion\n\n\n\nCarry out the following tasks:\n\nPerform a multiple linear regression (change class for cyl and am to factor)\nCheck the model residuals\nInterpret and plot all effects\n\nYou may need the following functions:\n\nas.factor()\nlm()\nsummary()\nanova()\nplot()\nallEffects()\n\nUse your results to answer the questions:\nWhich of the following statements are correct? (Several are correct).\n\n Cars with 6 cylinders do not differ significantly in their fuel consumption as compared to cars with 4 cylinders. Stronger cars (hp) use less fuel (mpg). Overall, heavier cars (wt) use significantly more fuel (their range is smaller; mpg)\n\nConcerning the interaction between weight (wt) and horsepower (hp), which of the following statements is correct?\n\n Pawl was wrong. There is a significant interaction between weight and horsepower, but the direction is opposite to what Pawl thought: The effect of weight is stronger for stronger cars. Pawl was wrong, the effect of weight is independent of horsepower. Pawl was right. The effect of weight is strong for weaker cars, but becomes indeed irrelevant for stronger cars.\n\n\n\n\n\nClick here to see the solution\n\nThis is the code that you need to interpret the results.\n\n# change am and cyl from numeric to factor\nmtcars$am &lt;- as.factor(mtcars$am)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\n# multiple linear regression and results:\n# (we need to scale (standardize) the predictors wt and hp, since we include their interaction)\ncarsfit &lt;- lm(mpg ~ am + cyl + scale(wt) * scale(hp), dat = mtcars)\n# weight is included as the first predictor in order to have\n# it as the grouping factor in the allEffects plot\n\nsummary(carsfit)\n## \n## Call:\n## lm(formula = mpg ~ am + cyl + scale(wt) * scale(hp), data = mtcars)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.4121 -1.6789 -0.4446  1.3752  4.4338 \n## \n## Coefficients:\n##                     Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)          19.9064     1.5362  12.958 1.36e-12 ***\n## am1                   0.1898     1.4909   0.127 0.899740    \n## cyl6                 -1.2818     1.5291  -0.838 0.409813    \n## cyl8                 -1.3942     2.1563  -0.647 0.523803    \n## scale(wt)            -3.6248     0.9665  -3.750 0.000938 ***\n## scale(hp)            -1.8602     0.8881  -2.095 0.046503 *  \n## scale(wt):scale(hp)   1.5631     0.7027   2.224 0.035383 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.246 on 25 degrees of freedom\n## Multiple R-squared:  0.888,  Adjusted R-squared:  0.8612 \n## F-statistic: 33.05 on 6 and 25 DF,  p-value: 1.021e-10\n# The first level of each factor is used as a reference, i.e. in this case a manual gear shift with 4 gears.\n# From the coefficient cyl6 we see that there is no significant difference in fuel consumption (= our response) between 4 gears (the reference) and 6 gears.\n# In contrast, the predictors weight (wt) and horsepower (hp) have a significant negative effect on the range (mpg), so that they both increase fuel consumption.\n\n# check residuals\nold.par = par(mfrow = c(2, 2))\nplot(carsfit)\n\n\n\npar(old.par)\n\n# plot effects\nplot(allEffects(carsfit))\n## Warning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\n## predictors scale(wt), scale(hp) are one-column matrices that were converted to\n## vectors\n\n## Warning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\n## predictors scale(wt), scale(hp) are one-column matrices that were converted to\n## vectors\n\n## Warning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\n## predictors scale(wt), scale(hp) are one-column matrices that were converted to\n## vectors\n\n\n\n# We can see in the wt*hp plot, that for high values of hp wt has no effect on the response mpg. We conclude that Pawl was right.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat is the meaning of “An effect is not significant”?\nIs an effect with three *** more significant / certain than an effect with one *?\n\n\n\n\n\nClick here to see the solution\n\n\nYou should NOT say that the effect is zero, or that the null hypothesis has been accepted. Official language is “there is no significant evidence for an effect(p = XXX)”. If we would like to assess what that means, some people do a post-hoc power analysis (which effect size could have been estimated), but better is typically just to discuss the confidence interval, i.e. look at the confidence interval and say: if there is an effect, we are relatively certain that it is smaller than X, given the confidence interval of XYZ.\nMany people view it that way, and some even write “highly significant” for *** . It is probably true that we should have a slightly higher confidence in a very small p-value, but strictly speaking, however, there is only significant, or not significant. Interpreting the p-value as a measure of certainty is a slight misinterpretation. Again, if we want to say how certain we are about the effect, it is better to look again at the confidence interval, i.e. the standard error and use this to discuss the precision of the estimate (small confidence interval / standard error = high precision / certainty)."
  },
  {
    "objectID": "4B-Exercise.html#model-selection-with-the-cement-dataset",
    "href": "4B-Exercise.html#model-selection-with-the-cement-dataset",
    "title": "Exercise - Multiple Linear Regression",
    "section": "Model-selection with the Cement dataset",
    "text": "Model-selection with the Cement dataset\nThe process of cement hardening involves exogenous chemical reactions and thus produces heat. The amount of heat produced by the cement depends on the mixture of its constituents. The Cement dataset includes heat measurements for different types of cement that consist of different relative amounts of calcium aluminate (X1), tricalcium silicate (X2), tetracalcium alumino ferrite (X3) and dicalcium silicate (X4). A cement producing company wants to optimize the composition of its product and wants to know, which of these compounds are mainly responsible for heat production.\n\n\n\n\n\n\nNote\n\n\n\nWe only do a model selection here for educational reasons. For your analysis, and if your goal is not a predictive model, think about the model structure before you do the analysis and then stick to it! See here the section about p-hacking (and also consider that AIC selection will/can remove confounders which will violate causality and can lead to spurious correlations!\n\n\n\n\n\n\n\n\nQuestions\n\n\n\nCarry out the following tasks:\n\nPerform a multiple linear regression including all predictor variables and all two-way interactions (remember the notation (var1 + var2 + var3)^2.\nPerform forward, backward, and global model selection and compare the results\nFit the model considered optimal by global model selection and compare it with the full model based on AIC (or AICc) and LRT.\n\nYou may need the following functions:\n\nlm()\nsummary()\nstepAIC() from the MuMIn package (library(MuMIn))\noptions()\ndredge()\nAIC() or AICc() (for small datasets)\nanova()\n\nUse your results to answer the following questions:\n1. You tested 3 different model selection methods: forward stepwise AIC selection, backward stepwise AIC selection and global model selection. How many terms ( = intercept + predictor effects + interactions) did each of the reduced models include?\n\nForward selection \nBackward selection \nglobal model selection \n\n2. You compared the full model with the reduced model from global model selection based on AIC and LRT (using the anova() function). Which of the two models would you choose based on their AIC? And which would you choose based on the LRT?\n\nAIC The full modelI don’t know. Both models fit equally well.Also the full modelThe reduced model\nLRT The full modelI don’t know. Both models fit equally well.Also the full modelThe reduced model\n\n3. Here’s a quote from Wikipedia on the AIC: “When the sample size is small, there is a substantial probability that AIC will select models that have too many parameters, i.e. that AIC will overfit.” Check the sample size of the Cement dataset. How do you now interpret the AIC values for the full model as compared to the reduced model from global model selection? (Several are correct)\n\n The AIC for the full model is smaller. The full model thus fits better. I would not trust AIC model selection in this case, because the sample size is too small to fit the number of parameters necessary for the full model. Instead of using the AIC for model comparison, I would now prefer the AICc, which corrects for small sample sizes.\n\n\n\n\n\nClick here to see the solution\n\nThis is the code that you need to obtain the results.\n\nlibrary(MuMIn)\nlibrary(MASS)\n\n# full model -&gt;  has 11 coefficients\nfull = lm(y ~ (X1 + X2 + X3 + X4)^2, data = Cement)\nsummary(full)\n\n# forward model selection\nms_forw = stepAIC(full, direction = \"forward\")\nsummary(ms_forw)\n# lists 11 coefficients (i.e. selects full model)\n\n# backward model selection\nms_back = stepAIC(full, direction = \"backward\")\nsummary(ms_back)\n# lists 10 coefficients\n\n# global model selection\noptions(na.action = \"na.fail\")\ndd = dredge(full)\nhead(dd)\n# The first row lists the best performing model: it includes only the intercept and effects for X1 and X2 (= 3 coefficients).\n\n# Fit the model considered optimal by global model selection and compare it with the full model based on AIC (or AICc) and LRT:\nopt = lm(y ~ X1 + X2, data = Cement)\nsummary(opt)\n\nAIC(opt,full) # full model is better according to AIC (lower AIC)\nanova(opt, full) # -&gt; LRT: no significant difference between the models\n\n# sample size in the Cement dataset:\nstr(Cement)  # or\nnrow(Cement)\n\n# If the sample size is low, a corrected version of the AIC is recommended to avoid overfitting:\nAICc(opt,full) # This is inf! -&gt; optimal model is better according to AICc"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science with R",
    "section": "",
    "text": "Preface\nMaterial for a introductory 1-week course in Data Science, taught at the University of Regensburg.\nDay 1: Getting started, descriptive statistics\nDay 2: Graphics, Hypothesis tests\nDay 3: Linear regression (simple, multiple)\nDay 4: Generalised linear models (GLM), Multivariate statistics\nDay 5: Machine Learning, Data and project organisation\nLink to the UR GRIPS course here (requires UR account)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "4-SimpleRegression.html#maximum-likelihood-estimator",
    "href": "4-SimpleRegression.html#maximum-likelihood-estimator",
    "title": "7  Simple linear regression",
    "section": "7.1 Maximum Likelihood Estimator",
    "text": "7.1 Maximum Likelihood Estimator\n\\(likelihood = P(D|model, parameter)\\)\nThe likelihood is the probability to observe the Data given a certain model (which is described by its parameter).\nIt is an approach to optimize a model/parameter to find the set of parameters that describes best the observed data.\nA simple example, we want to estimate the average of random vectors and we assume that our model is a normal distribution (so we assume that the data originated from a normal distribution). We want to optimize the two parameters that describe a normal distribution: the mean, and the sd:\n\nXobs = rnorm(100, sd = 1.0)\n# Now we assume that mean = 0, and sd = 0.2 are unknown but we want to find them, let's write the likelihood function:\nlikelihood = function(par) { # we give two parameters, mean and sd\n  lls = dnorm(Xobs, mean = par[1], sd = par[2], log = TRUE) # calculate for each observation to observe the data given our model\n  # we use the logLikilihood for numerical reasons\n  return(sum(lls))\n}\n\nlikelihood(c(0, 0.2))\n## [1] -1274.247\n# let's try all values of sd:\nlikelihood_mean = function(p) likelihood(c(p, 1.0))\nplot(seq(-5, 5.0, length.out = 100), sapply(seq(-5, 5.0, length.out = 100), likelihood_mean), xlab = 'Different mean values', ylab = \"negative logLikelihood\")\n\n\n\n\n# The optimum is at 0, which corresponds to our mean we used to sample Xobs\n\nHowever it is tedious to try all values manually to find the best value, especially if we have to optimize several values. For that we can use an optimizer in R which finds for us the best set of parameters:\n\nopt = optim(c(0.0, 1.0), fn = function(par) -likelihood(par), hessian = TRUE )\n\nWe can use the shape of the likelihood to calculate standard errors for our estimates:\n\nst_errors = sqrt(diag(solve(opt$hessian)))\n\nWith that we can calculate the confidence interval for our estimates. When the estimator is repeatedly used, 95% of the calculated confidence intervals will include the true value!\n\ncbind(opt$par-1.96*st_errors, opt$par+1.96*st_errors)\n##            [,1]      [,2]\n## [1,] -0.1706372 0.2355346\n## [2,]  0.8925465 1.1797585\n\nIn short, if we would do a t.test for our Xobs (to test whether the mean is stat. significant different from zero), the test would be non significant, and a strong indicator for that is when the 0 is within the confidence interval. Let’s compare our CI to the one calculated by the t-test:\n\nt.test(Xobs)\n## \n##  One Sample t-test\n## \n## data:  Xobs\n## t = 0.31224, df = 99, p-value = 0.7555\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  -0.1741130  0.2391426\n## sample estimates:\n##  mean of x \n## 0.03251482\n\nAlmost the same! The t-test also calculates the MLE to get the standard error and the confidence interval."
  },
  {
    "objectID": "4-SimpleRegression.html#the-theory-of-linear-regression",
    "href": "4-SimpleRegression.html#the-theory-of-linear-regression",
    "title": "7  Simple linear regression",
    "section": "7.2 The theory of linear regression",
    "text": "7.2 The theory of linear regression\nIf we want to test for an association between two continuous variables, we can calculate the correlation between the two - and with the cor.test function we can test even for significance. But, the correlation doesn’t report the magnitude, the strength, of the effect:\n\nX = runif(100)\npar(mfrow = c(1,1))\nplot(X, 0.5*X, ylim = c(0, 1), type = \"p\", pch = 15, col = \"red\", xlab = \"X\", ylab = \"Y\")\npoints(X, 1.0*X, ylim = c(0, 1), type = \"p\", pch = 15, col = \"blue\", xlab = \"X\", ylab = \"Y\")\n\n\n\ncor(X, 0.5*X)\n## [1] 1\ncor(X, 1.0*X)\n## [1] 1\n\nBoth have a correlation factor of 1.0! But we see clearly that the blue line has an stronger effect on Y then the red line.\nSolution: Linear regression models\nThey describe the relationship between a dependent variable and one or more explanatory variables:\n\\[\ny = a_0 +a_1*x\n\\]\n(x = explanatory variable; y = dependent variable; a0=intercept; a1= slope)\nExamples:\n\nplot(X, 0.5*X, ylim = c(0, 1), type = \"p\", pch = 16, col = \"black\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\npoints(X, 0.5*X, col = \"red\", type = \"l\", lwd = 1.5)\npoints(X, 1.0*X, ylim = c(0, 1), type = \"p\", pch = 16, col = \"black\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\npoints(X, 1.0*X, ylim = c(0, 1), type = \"l\", pch = 16, col = \"blue\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\nlegend(\"topleft\", col = c(\"red\", \"blue\"), lty = 1,legend = c('Y = 0.5*X+0', 'Y = 1.0**X+0'))\n\n\n\n\nWe can get the parameters (slope and intercept) with the MLE. However, we need first to make another assumptions, usually the model line doesn’t perfectly the data because there is an observational error on Y, so the points scatter around the line:\n\nplot(X, 0.5*X+rnorm(100, sd = 0.05), ylim = c(0, 1), type = \"p\", pch = 16, col = \"black\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\npoints(X, 0.5*X, col = \"red\", type = \"l\", lwd = 1.5)\npoints(X, 1.0*X+rnorm(100, sd = 0.05), ylim = c(0, 1), type = \"p\", pch = 16, col = \"black\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\npoints(X, 1.0*X, ylim = c(0, 1), type = \"l\", pch = 16, col = \"blue\", xlab = \"X\", ylab = \"Y\", lwd = 1.5)\nlegend(\"topleft\", col = c(\"red\", \"blue\"), lty = 1,legend = c('Y = 0.5*X+0', 'Y = 1.0**X+0'))\n\n\n\n\nAnd the model extends to:\n\\[\ny = a_0 + a_1*x +\\epsilon, \\epsilon \\sim N(0, sd)\n\\]\nWhich we can also rewrite into:\n\\[\ny = N(a_0 + a_1*x, sd)\n\\]\nWhich is very similar to our previous MLE, right? The only difference is now that the mean depends now on x, let’s optimize it again:\n\nXobs = rnorm(100, sd = 1.0)\nY = Xobs + rnorm(100,sd = 0.2)\nlikelihood = function(par) { # three parameters now\n  lls = dnorm(Y, mean = Xobs*par[2]+par[1], sd = par[3], log = TRUE) # calculate for each observation the probability to observe the data given our model\n  # we use the logLikilihood because of numerical reasons\n  return(sum(lls))\n}\n\nlikelihood(c(0, 0, 0.2))\n## [1] -1162.229\nopt = optim(c(0.0, 0.0, 1.0), fn = function(par) -likelihood(par), hessian = TRUE )\n\nopt$par\n## [1] 0.002927292 0.997608527 0.216189328\n\nOur true parameters are 0.0 for the intercept, 1.0 for the slope, and 0.22 for the sd of the observational error.\nNow, we want to test whether the effect (slope) is statistically significant different from 0:\n\ncalculate standard error\ncalculate t-statistic\ncalculate p-value\n\n\nst_errors = sqrt(diag(solve(opt$hessian)))\nst_errors[2]\n## [1] 0.02226489\nt_statistic = opt$par[2] / st_errors[2]\npt(t_statistic, df = 100-3, lower.tail = FALSE)*2\n## [1] 1.264962e-66\n\nThe p-value is smaller than \\(\\alpha\\), so the effect is significant! However, it would be tedious to do that always by hand, and because it is probably one of the most used analysis, there’s a function for it in R:\n\nmodel = lm(Y~Xobs) # 1. Get estimates, MLE\nmodel\n## \n## Call:\n## lm(formula = Y ~ Xobs)\n## \n## Coefficients:\n## (Intercept)         Xobs  \n##    0.002927     0.997569\nsummary(model) # 2. Calculate standard errors, CI, and p-values\n## \n## Call:\n## lm(formula = Y ~ Xobs)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.49919 -0.13197 -0.01336  0.14239  0.64505 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 0.002927   0.021838   0.134    0.894    \n## Xobs        0.997569   0.022490  44.355   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2184 on 98 degrees of freedom\n## Multiple R-squared:  0.9526, Adjusted R-squared:  0.9521 \n## F-statistic:  1967 on 1 and 98 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "4-SimpleRegression.html#sec-lm",
    "href": "4-SimpleRegression.html#sec-lm",
    "title": "7  Simple linear regression",
    "section": "7.3 Understanding the linear regression",
    "text": "7.3 Understanding the linear regression\n\nBesides the MLE, there are also several tests in a regression. The most important are\n\nsignificance of each parameter. \u000bt-test: H0 = variable has no effect, that means the estimator for the parameter is 0\u000b\nsignificance of the model. \u000bF-test: H0 = none of the explanatory variables has an effect, that means all estimators are 0\n\nExample:\n\npairs(airquality)\n\n\n\n# first think about what is explanatory / predictor \n# and what is the dependent variable (e.g. in Ozone and Temp)\n\n# par(mfrow = c(1, 1))\nplot(Ozone ~ Temp, data = airquality)\n\n\n\nfit1 = lm(Ozone ~ Temp, data = airquality)\nsummary(fit1)\n## \n## Call:\n## lm(formula = Ozone ~ Temp, data = airquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -40.729 -17.409  -0.587  11.306 118.271 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\n## Temp           2.4287     0.2331  10.418  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 23.71 on 114 degrees of freedom\n##   (37 observations deleted due to missingness)\n## Multiple R-squared:  0.4877, Adjusted R-squared:  0.4832 \n## F-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n# gives a negative values for the intercept = negative Ozone levels when Temp = 0\n# this does not make sense (&gt;extrapolation)\n\n# we can also fit a model without intercept, \n# without means: intercept = 0; y = a*x \n# although this doesn't make much sense here\nfit2 = lm(Ozone ~ Temp - 1, data = airquality)\nsummary(fit2)\n## \n## Call:\n## lm(formula = Ozone ~ Temp - 1, data = airquality)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -38.47 -23.26 -12.46  15.15 121.96 \n## \n## Coefficients:\n##      Estimate Std. Error t value Pr(&gt;|t|)    \n## Temp  0.56838    0.03498   16.25   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 29.55 on 115 degrees of freedom\n##   (37 observations deleted due to missingness)\n## Multiple R-squared:  0.6966, Adjusted R-squared:  0.6939 \n## F-statistic:   264 on 1 and 115 DF,  p-value: &lt; 2.2e-16\n\nplot(Ozone ~ Temp, data = airquality, xlim = c(0,100), ylim = c(-150, 150))\nabline(fit1, col = \"green\")\nabline(fit2, col = \"red\", lty = 2)\n\n\n\n\n# there is no need to check normality of Ozone\nhist(airquality$Ozone) # this is not normal, and that's no problem !\n\n\n\n\n\n7.3.1 Model diagnostics\nThe regression optimizes the parameters under the condition that the model is correct (e.g. there is really a linear relationship). If the model is not specified correctly, the parameter values are still estimated - to the best of the model’s ability, but the result will be misleading, e.g. p-values and effect sizes\nWhat could be wrong:\n\nthe distribution (e.g. error not normal)\nthe shape of the relationship between explanatory variable and dependent variable (e.g., could be non-linear)\n\nThe model’s assumptions must always be checked!\nWe can check the model by looking at the residuals (which are predicted - observed values) which should be normally distributed and should show no patterns:\n\nX = runif(50)\nY = X + rnorm(50, sd = 0.2)\nfit = lm(Y~X)\npar(mfrow = c(1, 3))\nplot(X, Y)\nabline(fit, col = \"red\")\nplot(X, predict(fit) - Y, ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\nhist(predict(fit) - Y, main = \"\", xlab = \"Residuals\")\n\n\n\npar(mfrow = c(1,1))\n\nThe residuals should match the model assumptions. For linear regression:\n\nnormal distribution\nconstant variance\nindependence of the data points\n\nExample:\n\nfit1 = lm(Ozone~Temp, data = airquality)\nresiduals(fit1)\n##           1           2           3           4           6           7 \n##  25.2723695   8.1288530 -20.7285536  14.4158861  14.7010729  12.1297762 \n##           8           9          11          12          13          14 \n##  22.7019960   6.8445894 -25.7285536  -4.5850371  -2.2989271  -4.1563338 \n##          15          16          17          18          19          20 \n##  24.1306993   5.5584795  20.7010729  14.5594026  11.8436662   7.4158861 \n##          21          22          23          24          28          29 \n##   4.7019960 -19.2998503   2.8445894  30.8445894   7.2723695  -4.7294767 \n##          30          31          38          40          41          44 \n##  70.1279299  -0.5859602 -23.1581800  -0.5878065 -25.3016966 -29.1581800 \n##          47          48          49          50          51          62 \n## -19.0146635   9.1288530   9.1297762 -18.2998503 -24.5859602  77.9844134 \n##          63          64          66          67          68          69 \n## -10.4442899 -17.7294767   9.4131167 -14.5868833  10.2696001  20.5547869 \n##          70          71          73          74          76          77 \n##  20.5547869  15.8408968 -20.2998503 -22.7294767 -40.3007734  -1.7294767 \n##          78          79          80          81          82          85 \n## -17.1581800   3.9844134  14.6983034   3.5557101 -16.7285536  18.1270068 \n##          86          87          88          89          90          91 \n##  48.5557101 -32.1581800  -9.8729932  15.2696001 -11.8729932   9.4131167 \n##          92          93          94          95          96          97 \n##   9.2705233 -10.7294767 -40.7294767 -36.1581800  16.1270068 -24.4442899 \n##          98          99         100         101         104         105 \n##   1.6983034  52.8408968  17.4121935  38.4121935 -17.8729932 -24.1581800 \n##         106         108         109         110         111         112 \n##  17.6992266 -18.0146635  14.1279299 -14.5859602 -11.4433668   1.5566332 \n##         113         114         116         117         118         120 \n## -19.0146635 -18.8711470   0.1279299 118.2705233  11.1270068 -12.5887296 \n##         121         122         123         124         125         126 \n##  36.6973803  -2.1600263   3.6973803  21.9834902   1.5547869  -5.8739164 \n##         127         128         129         130         131         132 \n##  12.1260836 -17.3016966 -25.0155866 -27.3007734 -19.4433668 -14.1572569 \n##         133         134         135         136         137         138 \n##  -6.2998503  -5.7294767 -16.5859602 -12.0146635 -16.4424437 -12.4424437 \n##         139         140         141         142         143         144 \n##   3.5566332   2.2723695 -24.5859602   5.8436662 -36.1581800   4.5584795 \n##         145         146         147         148         149         151 \n##  -2.4424437 -13.7294767 -13.5850371   7.9871828   6.9862596 -21.1572569 \n##         152         153 \n## -19.5859602   1.8436662\nhist(residuals(fit1))\n\n\n\n# residuals are not normally distributed\n# we do not use a test for this, but instead look at the residuals visually\n\n# let's plot residuals versus predictor\nplot(airquality$Temp[!is.na(airquality$Ozone)], residuals(fit1))\n\n\n\n\n# model checking plots\noldpar= par(mfrow = c(2,2))\nplot(fit1)\n\n\n\npar(oldpar)\n#&gt; there's a pattern in the residuals &gt; the model does not fit very well!\n\n\n\n7.3.2 Linear regression with a categorical variable\nWe can also use categorical variables as an explanatory variable:\n\nm = lm(weight~group, data = PlantGrowth)\nsummary(m)\n## \n## Call:\n## lm(formula = weight ~ group, data = PlantGrowth)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.0710 -0.4180 -0.0060  0.2627  1.3690 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   5.0320     0.1971  25.527   &lt;2e-16 ***\n## grouptrt1    -0.3710     0.2788  -1.331   0.1944    \n## grouptrt2     0.4940     0.2788   1.772   0.0877 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6234 on 27 degrees of freedom\n## Multiple R-squared:  0.2641, Adjusted R-squared:  0.2096 \n## F-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\nThe lm estimates an effect/intercept for each level in the categorical variable. The first level of the categorical variable is used as a reference, i.e. the true effect for grouptrt1 is Intercept+grouptrt1 = 4.661 and grouptrt2 is 5.5242. Moreover, the lm tests for a difference of the reference to the other levels. So with this model we know whether the control is significant different from treatment 1 and 2 but we cannot say anything about the difference between trt1 and trt2.\nIf we are interested in testing trt1 vs trt2 we can, for example, change the reference level of our variable:\n\ntmp = PlantGrowth\ntmp$group = relevel(tmp$group, ref = \"trt1\")\nm = lm(weight~group, data = tmp)\nsummary(m)\n## \n## Call:\n## lm(formula = weight ~ group, data = tmp)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.0710 -0.4180 -0.0060  0.2627  1.3690 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   4.6610     0.1971  23.644  &lt; 2e-16 ***\n## groupctrl     0.3710     0.2788   1.331  0.19439    \n## grouptrt2     0.8650     0.2788   3.103  0.00446 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6234 on 27 degrees of freedom\n## Multiple R-squared:  0.2641, Adjusted R-squared:  0.2096 \n## F-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\nAnother example:\n\nlibrary(effects)\n## Loading required package: carData\n## lattice theme set by effectsTheme()\n## See ?effectsTheme for details.\nlibrary(jtools)\n\nsummary(chickwts)\n##      weight             feed   \n##  Min.   :108.0   casein   :12  \n##  1st Qu.:204.5   horsebean:10  \n##  Median :258.0   linseed  :12  \n##  Mean   :261.3   meatmeal :11  \n##  3rd Qu.:323.5   soybean  :14  \n##  Max.   :423.0   sunflower:12\n\nplot(weight ~ feed, chickwts)\n\n\n\nfit4 = lm(weight ~ feed, chickwts)\n\nsummary(fit4)\n## \n## Call:\n## lm(formula = weight ~ feed, data = chickwts)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -123.909  -34.413    1.571   38.170  103.091 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)    323.583     15.834  20.436  &lt; 2e-16 ***\n## feedhorsebean -163.383     23.485  -6.957 2.07e-09 ***\n## feedlinseed   -104.833     22.393  -4.682 1.49e-05 ***\n## feedmeatmeal   -46.674     22.896  -2.039 0.045567 *  \n## feedsoybean    -77.155     21.578  -3.576 0.000665 ***\n## feedsunflower    5.333     22.393   0.238 0.812495    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 54.85 on 65 degrees of freedom\n## Multiple R-squared:  0.5417, Adjusted R-squared:  0.5064 \n## F-statistic: 15.36 on 5 and 65 DF,  p-value: 5.936e-10\nanova(fit4) #get overall effect of feeding treatment\n## Analysis of Variance Table\n## \n## Response: weight\n##           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \n## feed       5 231129   46226  15.365 5.936e-10 ***\n## Residuals 65 195556    3009                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(allEffects(fit4))\n\n\n\nplot(allEffects(fit4, partial.residuals = T))\n\n\n\neffect_plot(fit4, pred = feed, interval = TRUE, plot.points = F)\n\n\n\n\nold.par = par(mfrow = c(2, 2))\nplot(fit4)\n\n\n\npar(old.par)\n\nboxplot(residuals(fit4) ~ chickwts$feed)\n\n\n\n\n\n\n7.3.3 Linear regression with a quadratic term\n\n## what does simple linear regression mean?\n# simple = one predictor!\n# linear = linear in the parameters\n# a0 + a1 * x + a2 * x^2 \n# even if we add a quadratic term, this is a linear combination\n# this is called polynomial\n\nfit3 = lm(Ozone ~ Temp + I(Temp^2), data = airquality)\nsummary(fit3)\n## \n## Call:\n## lm(formula = Ozone ~ Temp + I(Temp^2), data = airquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -37.619 -12.513  -2.736   9.676 123.909 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 305.48577  122.12182   2.501 0.013800 *  \n## Temp         -9.55060    3.20805  -2.977 0.003561 ** \n## I(Temp^2)     0.07807    0.02086   3.743 0.000288 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 22.47 on 113 degrees of freedom\n##   (37 observations deleted due to missingness)\n## Multiple R-squared:  0.5442, Adjusted R-squared:  0.5362 \n## F-statistic: 67.46 on 2 and 113 DF,  p-value: &lt; 2.2e-16\n\noldpar= par(mfrow = c(2,2))\nplot(fit3)\n\n\n\npar(oldpar)\n\n\n# Residual vs. fitted looks okay, but Outliers are still there, and additionally\n# too wide. But for now, let's plot prediction with uncertainty (plot line plus confidence interval)\n\nplot(Ozone ~ Temp, data = airquality)\n\n# if the relationship between x and y is not linear, we cannot use abline\n# instead we predict values of x for different values of y based on the model \nnewDat = data.frame(Temp = 55:100)\npredictions = predict(fit3, newdata = newDat, se.fit = T)\n# and plot these into our figure:\nlines(newDat$Temp, predictions$fit, col= \"red\")\n# let's also plot the confidence intervals:\nlines(newDat$Temp, predictions$fit + 1.96*predictions$se.fit, col= \"red\", lty = 2)\nlines(newDat$Temp, predictions$fit - 1.96*predictions$se.fit, col= \"red\", lty = 2)\n\n# add a polygon (shading for confidence interval)\nx = c(newDat$Temp, rev(newDat$Temp))\ny = c(predictions$fit - 1.96*predictions$se.fit, \n      rev(predictions$fit + 1.96*predictions$se.fit))\n\npolygon(x,y, col=\"#99009922\", border = F )\n\n\n\n\n\n# alternative: use package effects\n#install.packages(\"effects\")\nlibrary(effects)\nplot(allEffects(fit3))\n\n\n\nplot(allEffects(fit3, partial.residuals = T)) \n\n\n\n#to check patterns in residuals (plots measurements and partial residuals)\n\n# or jtools package\nlibrary(jtools)\neffect_plot(fit3, pred = Temp, interval = TRUE, plot.points = TRUE)"
  },
  {
    "objectID": "6-GLM.html#sec-logistic",
    "href": "6-GLM.html#sec-logistic",
    "title": "9  Generalized linear models",
    "section": "9.1 Logistic Regression",
    "text": "9.1 Logistic Regression\nFor the binomial model we can use the logit link:\n\\[\nlogit(y) = a_0 +a_1*x\n\\]\nAnd with the inverse link:\n\\[\ny = \\frac{1}{1+e^{-(a_0 + a_1) }}\n\\]\nYou can interpret the glm outputs basically like lm outputs.\nBUT: To get absolute response values, you have to transform the output with the inverse link function. For the logit, e.g. an intercept of 0 means a predicted value of 0.5. Different overall statistics: no R2 instead Pseudo R2 = 1 - Residual deviance / Null deviance\u000b(deviance is based on the likelihood):\n\n# logistic regression with categorical predictor\nm1 = glm(survived ~ sex, data = titanic, family = \"binomial\")\nsummary(m1)\n## \n## Call:\n## glm(formula = survived ~ sex, family = \"binomial\", data = titanic)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)   0.9818     0.1040   9.437   &lt;2e-16 ***\n## sexmale      -2.4254     0.1360 -17.832   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1741.0  on 1308  degrees of freedom\n## Residual deviance: 1368.1  on 1307  degrees of freedom\n## AIC: 1372.1\n## \n## Number of Fisher Scoring iterations: 4\n\n# 2 groups: sexmale = difference of mean for male from mean for female\n# intercept = linear term for female: \n0.98 \n## [1] 0.98\n# but: this has to be transformed back to original scale before being interpreted!!!\n# survival probability for females\nplogis(0.98)\n## [1] 0.7271082\n# applies inverse logit function\n\n\n# linear term for male\n0.98 - 2.43\n## [1] -1.45\n# survival probability\nplogis(0.98 - 2.43)\n## [1] 0.1900016\n\n# predicted linear term\ntable(predict(m1))\n## \n##  -1.4436252928589 0.981813020919314 \n##               843               466\n# predicted response\ntable(predict(m1, type = \"response\"))\n## \n##  0.19098457888495 0.727467811158294 \n##               843               466\n\n\nplot(allEffects(m1))\n\n\n\n\n# more predictors\nm2 = glm(survived ~ sex + age, titanic, family = binomial)\nsummary(m2)\n## \n## Call:\n## glm(formula = survived ~ sex + age, family = binomial, data = titanic)\n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  1.235414   0.192032   6.433 1.25e-10 ***\n## sexmale     -2.460689   0.152315 -16.155  &lt; 2e-16 ***\n## age         -0.004254   0.005207  -0.817    0.414    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1414.6  on 1045  degrees of freedom\n## Residual deviance: 1101.3  on 1043  degrees of freedom\n##   (263 observations deleted due to missingness)\n## AIC: 1107.3\n## \n## Number of Fisher Scoring iterations: 4\n\n\n# Calculate Pseudo R2: 1 - Residual deviance / Null deviance\n1 - 1101.3/1414.6 # Pseudo R2 of model\n## [1] 0.221476\n\n# Anova\nanova(m2, test = \"Chisq\")\n## Analysis of Deviance Table\n## \n## Model: binomial, link: logit\n## \n## Response: survived\n## \n## Terms added sequentially (first to last)\n## \n## \n##      Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    \n## NULL                  1045     1414.6             \n## sex   1  312.612      1044     1102.0   &lt;2e-16 ***\n## age   1    0.669      1043     1101.3   0.4133    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nplot(allEffects(m2))\n\n\n\n\nResidual check:\n\n# Model diagnostics\n# do not use the plot(model) residual checks\n# use DHARMa package\nlibrary(DHARMa)\nres = simulateResiduals(m2)\nplot(res)"
  },
  {
    "objectID": "6-GLM.html#sec-poisson",
    "href": "6-GLM.html#sec-poisson",
    "title": "9  Generalized linear models",
    "section": "9.2 Poisson Regression",
    "text": "9.2 Poisson Regression\nPoisson regression is used for count data. Properties of count data are: no negative values, only integers, y ~ Poisson(lambda); where lambda = mean = variance, log link function (lambda must be positive).\nExample:\n\nhead(birdfeeding)\n##   feeding attractiveness\n## 1       3              1\n## 2       6              1\n## 3       8              1\n## 4       4              1\n## 5       2              1\n## 6       7              2\n\nplot(feeding ~ attractiveness, birdfeeding)\n\n\n\n\nfit = glm(feeding ~ attractiveness, birdfeeding, family = \"poisson\")\nsummary(fit)\n## \n## Call:\n## glm(formula = feeding ~ attractiveness, family = \"poisson\", data = birdfeeding)\n## \n## Coefficients:\n##                Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)     1.47459    0.19443   7.584 3.34e-14 ***\n## attractiveness  0.14794    0.05437   2.721  0.00651 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 25.829  on 24  degrees of freedom\n## Residual deviance: 18.320  on 23  degrees of freedom\n## AIC: 115.42\n## \n## Number of Fisher Scoring iterations: 4\n\n# feeding for a bird with attractiveness 3\n# linear term\n1.47 + 0.148 * 3\n## [1] 1.914\n# pieces of food, using inverse of the link function, log --&gt; exp\nexp(1.47 + 0.148 * 3)\n## [1] 6.780155\n\n\nplot(allEffects(fit))\n\n\n\n\n\n# checking residuals\nres = simulateResiduals(fit)\nplot(res, quantreg = F)\n## Warning in smooth.spline(pred, res, df = 10): not using invalid df; must have 1\n## &lt; df &lt;= n := #{unique x} = 5\n\n\n\n# the warning is because of a recent change in DHARMa \n# qgam requires more data points\n\nNormal versus Poisson distribution:\n\nN(mean, sd)\u000bThis means that fitting a normal distribution estimates a parameter for the variance (sd)\nPoisson(lambda)\u000blambda = mean = variance\u000bThis means that a Poisson regression does not fit a separate parameter for the variance\n\nSo in the glm always assume that the variance is the mean, which is a strong assumption. In reality it can often occur that the variance is greater than expected (Overdispersion) or smaller than expected (Underdispersion). (this cannot happen for the lm because there we estimate a variance parameter (residual variance)). Overdispersion can have a HUGE influence on the MLEs and particularly on the p-values!\nWe can use the DHARMa package to check for Over or Underdispersion:\n\n# test for overdispersion\ntestDispersion(fit)\n\n\n\n## \n##  DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n##  simulated\n## \n## data:  simulationOutput\n## dispersion = 0.74488, p-value = 0.384\n## alternative hypothesis: two.sided\n\n# Dispersion test is necessary for all poisson or binomial models with k/n \n# if positive, you can chose family = quasi-poisson or quasi-binomial\n# or use negative binomial distribution instead of poisson"
  },
  {
    "objectID": "5A-Exercise.html#unconstrained-ordination-pca",
    "href": "5A-Exercise.html#unconstrained-ordination-pca",
    "title": "Exercise - Multivariate statistics",
    "section": "Unconstrained ordination (PCA)",
    "text": "Unconstrained ordination (PCA)\nCarry out the following analyses:\n\nMake two PCAs, one for the environmental and one for the species richness data (see columns above).\nName the results pca_environment and pca_species.\nCreate a biplot for each PCA.\nCreate a barplot for the proportion of variance explained by each component.\n\nFor example, the result for species richness should look like this:\n\n\n\n\n\nYou need the following functions and the package vegan:\n\nprcomp()\nbiplot()\n\nbarplot()\nsummary()\n\n\nlibrary(vegan)\n\nHints\n\nDon’t forget to scale the variables in the PCA.\nIn order to get the proportion of the explained variance, have a look at the summary of your analysis. str() shows you what the summary contains. You want to specifically look at the importance, here the second row contains the proportion of variance of all components. So what we want to plot is: summary(pca_species)$importance[2,].\nDon’t forget to give your plots a title.\n\nNow, use your results to answer the questions on elearning-extern (Q 1-3) (“07_Test for Exercise in R”).\n\nIn which multivariate dataset do the first and second components explain more variance?\nWhat are the two samples that have the highest score on the respective third PCA axes?\nFrom looking at the biplot: Which environmental variables would you choose, if you have only the resource to measure three variables?\n\nNAP, angle 1 and chalk\nsalinity, penetrability and grain size\ngrain size, chalk and exposure\n\nDescribe the following correlations:\n\nsalinity and humus\nangle1 and exposure\nPolychaeta and Mollusca\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## 1.) Conduct the principle component analyses (PCAs):\n\n# a) PCA of environmental data\npca_environment = prcomp(dat[,7:16], scale = T)\nsummary(pca_environment)\n## Importance of components:\n##                           PC1    PC2    PC3    PC4     PC5    PC6     PC7\n## Standard deviation     1.8476 1.3300 1.0919 1.0297 0.99521 0.7746 0.72784\n## Proportion of Variance 0.3414 0.1769 0.1192 0.1060 0.09904 0.0600 0.05298\n## Cumulative Proportion  0.3414 0.5182 0.6375 0.7435 0.84254 0.9025 0.95551\n##                            PC8     PC9    PC10\n## Standard deviation     0.52299 0.34727 0.22541\n## Proportion of Variance 0.02735 0.01206 0.00508\n## Cumulative Proportion  0.98286 0.99492 1.00000\n\nop &lt;- par(mfrow = c(1,2))\nbiplot(pca_environment, main = \"PCA\")            # plot the results of the PCA as a rotation matrix\nbarplot(summary(pca_environment)$importance[2,], # get the importance measure\n     main = \"Components of environment\",\n     ylab = \"Proportion of variance explained\")\n\n\n\n\n# b) PCA of species richness data\npca_species = prcomp(dat[,2:5], scale = T)\nsummary(pca_species)\n## Importance of components:\n##                           PC1    PC2    PC3    PC4\n## Standard deviation     1.1177 1.0251 1.0095 0.8251\n## Proportion of Variance 0.3123 0.2627 0.2548 0.1702\n## Cumulative Proportion  0.3123 0.5750 0.8298 1.0000\n\nbiplot(pca_species, main = \"PCA\")            # plot the results of the PCA as a rotation matrix\nbarplot(summary(pca_species)$importance[2,], # get the importance measure\n     main = \"Components of species\",\n     ylab = \"Proportion of variance explained\")\n\n\n\npar(op)\n\n# From the *summary()* output we can see that the first and second components explain more variance in the species PCA.\n\n\n##  2.) What are the two samples that have the highest score on the third PCA axis?\n\n# -&gt; order the samples by their PC3 coordinate:\norder(pca_environment$x[,'PC3'], decreasing = T) # -&gt; 24 is highest\n##  [1] 24 21 23 28 25 16 22 40 36 19  8 12  6 30  9  7 39 32 44  1 15  4 31 13 14\n## [26]  5 10 35 37 45 27 20 18 34 11 17 26 29 38  3 41 33  2 42 43\norder(pca_species$x[,'PC3'], decreasing = T) # -&gt; 7 is highest\n##  [1]  7  8  6  1 37  5 13  3 35  4 14 38 11 42 29 39 28 43  2 45 26 34 17 27 36\n## [26] 18 20 15 23 30 41 21 19 31 12 16 33 25 32 24 40 44 22 10  9\n\n\n## 3.) Which environmental variables would you choose, if you have only the resource to measure three variables? \n\n# From looking at the biplot, we choose 3 variables that describe a lot of variation (i.e. have a large length in the biplot) and have little collinearity. \n# -&gt; For example, an appropriate choice would be salinity, penetrability and grain size.\n\n\n## 4.) We can get information on the correlations of variables by looking at their representation in the biplot: \n  #  a) salinity and humus:  same direction  -&gt;  positively correlated\n  #  b) angle1 and exposure:  opposite directions  -&gt;  negatively correlated\n  #  c) Polychaeta and Mollusca:  almost perpendicular  -&gt;  uncorrelated"
  },
  {
    "objectID": "5A-Exercise.html#clustering",
    "href": "5A-Exercise.html#clustering",
    "title": "Exercise - Multivariate statistics",
    "section": "Clustering",
    "text": "Clustering\n\nK-means\nWe want to use clustering methods to define 3 environmental types. Use the function kmeans() with centers = 3 (number of clusters to be generated). Remember to set a seed; the choice of the first k centers is random.\n\nset.seed(467456)\ncl = #...\n\nCompare the three clusters with the result of the PCA using the following code that uses another plotting framework called ggplot2:\n\nlibrary(ggfortify)\nautoplot(pca_environment, colour = cl$cluster, size = 2, loadings = TRUE, loadings.colour = 'black',\n         loadings.label = TRUE, loadings.label.size = 3, loadings.label.colour = \"black\")\n\nThe colors of the points represent the three clusters. Answer the following question on elearning-extern (Q 4-5):\n\nHow is it possible that four observations in the middle (in red - if you have used the same seed) belong to a different cluster than the observations around them (in black)?\nWhich environmental variables are on average high within the black cluster (cluster 1)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n# Create clusters using all environmental variables\nset.seed(467456)\ncl &lt;- kmeans(dat[,7:16], centers = 3) # 3 clusters to be generated\n\n# Plot\nlibrary(ggfortify)\nautoplot(pca_environment, colour = cl$cluster, size = 2, loadings = TRUE, loadings.colour = 'black',\n         loadings.label = TRUE, loadings.label.size = 3, loadings.label.colour = \"black\")\n\n\n\n\n# To understand why the four observations in the middle belong to a different cluster than the observations around them, we must take into account, that the biplot is only a 2-dimensional representation of a more-than-2 dimensional space. Therefore, the four points in the middle will be dissimilar to the points around them with respect to a variable that is not well represented by the first two PCA axes.\n\n# Environmental variables that are on average high within cluster 1:\n# -&gt; looking at the plot we find that exposure and grain size are high on average within cluster 1\n\n\n\n\n\n\nHierarchical clustering\nNow we want to hierarchically cluster the samples with respect to their species richness, as shown in the following plot:\n\n\n\n\n\nCreate the same plot using the functions:\n\nhclust()\n\nplot()\nas.phylo()\n\nLoad the package ape. Have a look at the help for hclust() to read what the function does and look at the examples for further help on how to use the function. Then have a look at what the function as.phylo() does. Now, color the labels using the variable week. You can do this using the argument “tip.color =”.\nChoose the correct statement(s) about the species richness and its sampling on elearning-extern(Q6). To be able to read the plot more easily, you can click Zoom in the top pane of the Plots window in RStudio.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(ape)\n\nhc = hclust(dist(dat[, 2:5]))\nplot(as.phylo(hc), tip.color = dat$week)"
  },
  {
    "objectID": "4C-Exercise.html#analyzing-the-nitrofen-dataset",
    "href": "4C-Exercise.html#analyzing-the-nitrofen-dataset",
    "title": "Exercise - GLM",
    "section": "Analyzing the nitrofen dataset",
    "text": "Analyzing the nitrofen dataset\nThe Ministry of Agriculture has appointed your university to investigate the toxicity of the herbicide nitrofen on the fertility of the waterflea species Ceriodaphnia dubia in order to asses the implications of the herbicide for ecosystems.\nIn an experiment conducted by your fellow researchers, the offspring of the waterflea species Ceriodaphnia dubia were counted as a function of different concentrations of the herbicide. Your job is to do the analysis.\n\n\n\n\n\n\nQuestion\n\n\n\nCarry out the following tasks:\n\nConvert the variable conc into a factor.\nFit a suitable model for the relationship between total amount of offspring (total) and nitrofen concentration (conc).\nTest for overdispersion.\nInterpret and plot the effect.\nTest for the overall effect of nitrofen concentration.\n\nYou may need the following functions:\n\nstr()\nas.factor()\nglm()\nsummary()\ntestDispersion()\nplot()\nallEffects()\nanova(..., test = \"Chisq\")\n\nUse your results to answer the following questions:\n1.You analyzed the response of Ceriodaphnia dubia to different concentrations of the herbicide nitrofen. Looking at your results, which of the following statements is correct? (Several anwers are correct)\n\n The MLE for the mean number of offspring at a nitrofen concentration of 0 is 31.4 (Don't forget to apply the (inverse) link function to the estimate). At a nitrofen concentration of 160, Ceriodaphnia dubia produced significantly less offspring than in the control treatment (conc = 0). At a nitrofen concentration of 235, Ceriodaphnia dubia produced significantly less offspring than in the control treatment (conc = 0). The residuals show significantly less variance than would be expected from a Poisson distribution (underdispersion).\n\n2. How do you test for the overall significance of concentration on the total amount of offspring? Perform the respective test to check whether the variable conc as a whole has an effect on the total amount of offspring. How many asteriks are displayed for significance?\n\n none = not significant * = p &lt; 0.05 ** = p &lt; 0.01 *** = p &lt; 0.001\n\n\n\n\n\nClick here to see the solution\n\n\n# prepare data\nnitrofen$conc &lt;- as.factor(nitrofen$conc) # change variable conc to a factor\n\n# plot the relation that we want to fit\nplot(total ~ conc, nitrofen)\n\n\n\n\n# Fit a suitable model for the relationship between total amount of offspring (total) and nitrofen concentration (conc):\nfit &lt;- glm(total ~ conc, family = \"poisson\", data=nitrofen)\nsummary(fit)\n## \n## Call:\n## glm(formula = total ~ conc, family = \"poisson\", data = nitrofen)\n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  3.44681    0.05643  61.078  &lt; 2e-16 ***\n## conc80       0.00318    0.07974   0.040    0.968    \n## conc160     -0.10395    0.08196  -1.268    0.205    \n## conc235     -0.60190    0.09486  -6.345 2.22e-10 ***\n## conc310     -1.65505    0.14089 -11.747  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 312.484  on 49  degrees of freedom\n## Residual deviance:  50.719  on 45  degrees of freedom\n## AIC: 297.81\n## \n## Number of Fisher Scoring iterations: 5\n\n# test for overdispersion\ntestDispersion(fit)\n\n\n\n## \n##  DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n##  simulated\n## \n## data:  simulationOutput\n## dispersion = 0.62249, p-value = 0.048\n## alternative hypothesis: two.sided\n\n# plot effect\nplot(allEffects(fit))\n\n\n\n\n# log link to calculate predicted values at the response scale:\n# predicted response = exp(Intercept + Estimate * predictor)\nexp(3.44681) # or\n## [1] 31.40007\nexp(coef(fit)[1])\n## (Intercept) \n##        31.4\n\n# Test for the overall effect of *conc* on the total number of offspring\nanova(fit, test = \"Chisq\")\n## Analysis of Deviance Table\n## \n## Model: poisson, link: log\n## \n## Response: total\n## \n## Terms added sequentially (first to last)\n## \n## \n##      Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \n## NULL                    49    312.484              \n## conc  4   261.76        45     50.719 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4C-Exercise.html#analyzing-the-melanoma-dataset",
    "href": "4C-Exercise.html#analyzing-the-melanoma-dataset",
    "title": "Exercise - GLM",
    "section": "Analyzing the melanoma dataset",
    "text": "Analyzing the melanoma dataset\nIn the following, you will investigate the melanoma dataset provided by the University Hospital of Odense, Denmark. The data are of patients with malignant melanomas. You can find a more detailed description in the help of melanoma. The question you want to investigate is whether the occurrence of ulceration (ulcer, yes = 1, no = 0) is related to the thickness of the tumor (thickness in mm) and the sex of the patient (sex, male = 1, female = 0) and the interaction of the two.\n\n\n\n\n\n\nQuestion\n\n\n\nCarry out the following tasks:\n\nFit an appropriate model to answer the research question.\nCheck the model residuals.\nInterpret and plot all effects including an analysis of the deviance.\n\nYou may need the following functions:\n\nstr()\nglm()\nsummary()\nanova()\nsimulateResiduals()\nplot()\nallEffects()\n\nUse your results to answer the following questions:\n1. Let’s move to the melanoma dataset: Assuming you chose the correct distribution, which type of regression did you use for the analysis?\n\n Linear regression Logistic regression Poisson regression\n\n3. You have checked the residuals of your model. Which of the following patterns and conclusions apply? (Several answers are correct)\n\n There remains a significant pattern between the residuals and the fitted values. An additional quadratic term for thickness could help to improve the model. The residuals deviate from the expected distribution. The model is overdispersed. The model assumptions are not met yet, and I don't trust this model without further improvements.\n\n\n\n\n\nClick here to see the solution\n\n\nThere remains a significant pattern between the residuals and the fitted values. –&gt; You can see this in the right part of the DHARMa residual plot.\nAn additional quadratic term for thickness could help to improve the model. –&gt; Try it out!\nThe residuals deviate from the expected distribution. –&gt; You can see this in the left part of the DHARMa residual plot.\nThe model is overdispersed. –&gt; Only poisson or binomial k/n models can be overdispersed!\n\n\n# get the data\ndat &lt;- melanoma\n\n# Fit an appropriate model to answer the research question.\ndat$sex &lt;- as.factor(dat$sex) # change variable sex to factor (this is optional, not necessary with binary values 0/1)\nfit &lt;- glm(ulcer ~ thickness * sex, family = \"binomial\", data=dat)\n\n# Check residuals\nres &lt;- simulateResiduals(fit, n = 500)\nplot(res)\n\n\n\n\n# model interpretation\nsummary(fit)\n## \n## Call:\n## glm(formula = ulcer ~ thickness * sex, family = \"binomial\", data = dat)\n## \n## Coefficients:\n##                Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)    -1.40642    0.31902  -4.409 1.04e-05 ***\n## thickness       0.36927    0.11368   3.248  0.00116 ** \n## sex1           -0.02579    0.55535  -0.046  0.96296    \n## thickness:sex1  0.14527    0.17656   0.823  0.41064    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 281.13  on 204  degrees of freedom\n## Residual deviance: 234.04  on 201  degrees of freedom\n## AIC: 242.04\n## \n## Number of Fisher Scoring iterations: 5\nanova(fit, test = \"Chisq\")\n## Analysis of Deviance Table\n## \n## Model: binomial, link: logit\n## \n## Response: ulcer\n## \n## Terms added sequentially (first to last)\n## \n## \n##               Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \n## NULL                            204     281.13              \n## thickness      1   45.374       203     235.76 1.628e-11 ***\n## sex            1    1.039       202     234.72    0.3080    \n## thickness:sex  1    0.681       201     234.04    0.4094    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Calculate Pseudo R2: 1 - Residual deviance / Null deviance\n1 - 234.04/281.13 # Pseudo R2 of model\n## [1] 0.1675026\n\n# plot effects\nplot(allEffects(fit))\n\n\n\n\nAs the residuals look quite suspicious and all quantile regressions significantly deviate, we can try to improve the model with a quadratic term for thickness.\n\nfit &lt;- glm(ulcer ~ thickness * sex + I(thickness^2), family = \"binomial\", data=dat)\nsummary(fit)\n## \n## Call:\n## glm(formula = ulcer ~ thickness * sex + I(thickness^2), family = \"binomial\", \n##     data = dat)\n## \n## Coefficients:\n##                 Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)    -2.466019   0.411511  -5.993 2.07e-09 ***\n## thickness       1.066836   0.193715   5.507 3.65e-08 ***\n## sex1            0.236510   0.518545   0.456    0.648    \n## I(thickness^2) -0.057746   0.012766  -4.523 6.09e-06 ***\n## thickness:sex1 -0.009476   0.133618  -0.071    0.943    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 281.13  on 204  degrees of freedom\n## Residual deviance: 214.05  on 200  degrees of freedom\n## AIC: 224.05\n## \n## Number of Fisher Scoring iterations: 4\nres &lt;- simulateResiduals(fit)\nplot(res)\n\n\n\n\n# plot effects\nplot(allEffects(fit))\n\n\n\n\nThe quadratic term solves the problem of the residuals. The effects plots look quite different. There seems to be a maximum of ulcer around a thickness of 10 that we would have missed without the quadratic term."
  },
  {
    "objectID": "4B-Exercise.html#useful-functions",
    "href": "4B-Exercise.html#useful-functions",
    "title": "Exercise - Multiple Linear Regression",
    "section": "Useful functions",
    "text": "Useful functions\nfor multiple linear regression\nlm() - fit linear model\nsummary(fit) - apply to fitted model object to display regression table\nplot(fit) - plot residuals for model validation\nanova(fit) - apply type I ANOVA (variables included sequentially) to model to test for effects all levels of a factor\nAnova(fit) - car package; use type II ANOVA (effects for predictors when all other predictors are already included) for overall effects\nscale() - scale variable\nsqrt() - square-root\nlog() - calculates natural logarithm\nplot(allEffects(fit)) - apply to fitted model object to plot marginal effect; effects package\npar() - change graphical parameters\nuse oldpar \\&lt;- par(mfrow = c(number_rows, number_cols)) to change figure layout including more than 1 plot per figure\nuse par(oldpar) to reset graphic parameter\nfor model selection\nstepAIC(fullModel) - perform stepwise AIC model selection; apply to full model object, MASS package\ndredge(fullModel) - perform global model selection, MuMIn package\nmodel.avg() - perform model averaging\nAIC(fit) - get AIC for a fitted model\nanova(fit1, fit2) - compare two fitted models via Likelihood Ratio Test (LRT)"
  },
  {
    "objectID": "4B-Exercise.html#interactions-with-the-plantheight-dataset",
    "href": "4B-Exercise.html#interactions-with-the-plantheight-dataset",
    "title": "Exercise - Multiple Linear Regression",
    "section": "Interactions with the plantHeight dataset",
    "text": "Interactions with the plantHeight dataset\n\n\n\n\n\n\nPlant Height revisited\n\n\n\nRevisit exercise our previous analysis of EcoData::plantHeight\n\nlibrary(EcoData)\nmodel = lm(loght ~ temp, data = plantHeight)\n\nUse (separate) multiple regressions to test if:\n\nIf temp or NPP (net primary productivity) is a more important predictor (importance == absolute effect size).\nIf growth forms (variable growthform) differ in their temperature effects. (use an interaction)\nIf the effect of temp remains significant if we include latitude and an interaction of latitude with temp. If not, why? Tip: plot temp ~ lat.\n\n\n\n\n\nClick here to see the solution\n\n\nplantHeight$sTemp = scale(plantHeight$temp)\nplantHeight$sLat = scale(plantHeight$lat)\nplantHeight$sNPP = scale(plantHeight$NPP)\n\n# relevel \nplantHeight$growthform2 = relevel(as.factor(plantHeight$growthform), \"Herb\")\n\n\nNPP or Temp?\n\n\nfit = lm(loght ~ sTemp + sNPP, data = plantHeight)\nsummary(fit)\n## \n## Call:\n## lm(formula = loght ~ sTemp + sNPP, data = plantHeight)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.69726 -0.47935  0.04285  0.39812  1.77919 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.44692    0.05119   8.731 2.36e-15 ***\n## sTemp        0.20846    0.07170   2.907 0.004134 ** \n## sNPP         0.24734    0.07164   3.452 0.000702 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6711 on 169 degrees of freedom\n##   (6 observations deleted due to missingness)\n## Multiple R-squared:  0.2839, Adjusted R-squared:  0.2754 \n## F-statistic:  33.5 on 2 and 169 DF,  p-value: 5.553e-13\n\nNPP is slightly more important\n\nInteraction with growth form\n\n\nfit = lm(loght ~ growthform2 *  sTemp , data = plantHeight)\nsummary(fit)\n## \n## Call:\n## lm(formula = loght ~ growthform2 * sTemp, data = plantHeight)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.19634 -0.21217 -0.00997  0.22750  1.62398 \n## \n## Coefficients: (2 not defined because of singularities)\n##                              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                 -0.310748   0.062150  -5.000 1.51e-06 ***\n## growthform2Fern              0.624160   0.375650   1.662 0.098586 .  \n## growthform2Herb/Shrub        0.456394   0.377088   1.210 0.227967    \n## growthform2Shrub             0.562799   0.083100   6.773 2.36e-10 ***\n## growthform2Shrub/Tree        0.957088   0.486858   1.966 0.051069 .  \n## growthform2Tree              1.586005   0.080756  19.640  &lt; 2e-16 ***\n## sTemp                        0.203808   0.053231   3.829 0.000185 ***\n## growthform2Fern:sTemp              NA         NA      NA       NA    \n## growthform2Herb/Shrub:sTemp        NA         NA      NA       NA    \n## growthform2Shrub:sTemp       0.103357   0.076860   1.345 0.180636    \n## growthform2Shrub/Tree:sTemp -0.004614   0.526866  -0.009 0.993024    \n## growthform2Tree:sTemp       -0.244410   0.077661  -3.147 0.001971 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3713 on 158 degrees of freedom\n##   (10 observations deleted due to missingness)\n## Multiple R-squared:  0.796,  Adjusted R-squared:  0.7844 \n## F-statistic: 68.51 on 9 and 158 DF,  p-value: &lt; 2.2e-16\n\nYes, because (some) interactions are significant.\nNote that the n.s. effect of sTemp is the first growth form (Ferns), for which we had only one observation. In a standard multiple regression, you don’t have p-values for the significance of the temperature effect against 0 for the other growth forms, because you test against the reference. What one usually does is to run an ANOVA (see chapter on ANOVA) to see if temp is overall significant.\n\nanova(lm(loght ~ growthform *  sTemp , data = plantHeight))\n## Analysis of Variance Table\n## \n## Response: loght\n##                   Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \n## growthform         5 78.654 15.7309 114.1241 &lt; 2.2e-16 ***\n## sTemp              1  3.543  3.5426  25.7006 1.104e-06 ***\n## growthform:sTemp   3  2.800  0.9333   6.7707 0.0002524 ***\n## Residuals        158 21.779  0.1378                       \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAlternatively, if you want to test if a specific growth form has a significant temperature effect, you could either extract the p-value from a multiple regression (a bit complicated) or just run a univariate regression for this growth form\n\nfit = lm(loght ~ sTemp + 0, data = plantHeight[plantHeight$growthform == \"Tree\",])\nsummary(fit)\n## \n## Call:\n## lm(formula = loght ~ sTemp + 0, data = plantHeight[plantHeight$growthform == \n##     \"Tree\", ])\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## 0.2636 0.7198 0.9672 1.3503 2.3914 \n## \n## Coefficients:\n##       Estimate Std. Error t value Pr(&gt;|t|)   \n## sTemp   0.5013     0.1699    2.95  0.00452 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.21 on 60 degrees of freedom\n##   (10 observations deleted due to missingness)\n## Multiple R-squared:  0.1267, Adjusted R-squared:  0.1121 \n## F-statistic: 8.704 on 1 and 60 DF,  p-value: 0.004522\n\nOr you could fit the interaction but turn-off the intercept (by saying +0 or -1) and remove the plantHeight intercepts:\n\nfit = lm(loght ~ sTemp:growthform + 0, data = plantHeight[,])\nsummary(fit)\n## \n## Call:\n## lm(formula = loght ~ sTemp:growthform + 0, data = plantHeight[, \n##     ])\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.5156 -0.1396  0.3488  0.8103  2.3914 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(&gt;|t|)    \n## sTemp:growthformFern        -0.8949     2.9233  -0.306 0.759911    \n## sTemp:growthformHerb         0.3195     0.1077   2.967 0.003460 ** \n## sTemp:growthformHerb/Shrub   1.1788     5.5825   0.211 0.833026    \n## sTemp:growthformShrub        0.2375     0.1197   1.984 0.048974 *  \n## sTemp:growthformShrub/Tree   0.8833     0.2613   3.380 0.000908 ***\n## sTemp:growthformTree         0.5013     0.1171   4.281 3.17e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8339 on 162 degrees of freedom\n##   (10 observations deleted due to missingness)\n## Multiple R-squared:  0.2083, Adjusted R-squared:  0.179 \n## F-statistic: 7.106 on 6 and 162 DF,  p-value: 9.796e-07\n\n\nInteraction with lat\n\n\nfit = lm(loght ~ sTemp * sLat, data = plantHeight)\nsummary(fit)\n## \n## Call:\n## lm(formula = loght ~ sTemp * sLat, data = plantHeight)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.97905 -0.45112  0.01062  0.42852  1.74054 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.46939    0.06771   6.932 7.78e-11 ***\n## sTemp        0.26120    0.14200   1.839   0.0676 .  \n## sLat        -0.13072    0.13616  -0.960   0.3383    \n## sTemp:sLat   0.01209    0.04782   0.253   0.8007    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6869 on 174 degrees of freedom\n## Multiple R-squared:  0.2504, Adjusted R-squared:  0.2375 \n## F-statistic: 19.37 on 3 and 174 DF,  p-value: 6.95e-11\n\nAll is n.s. … how did this happen? If we check the correlation between temp and lat, we see that the two predictors are highly collinear.\n\ncor(plantHeight$temp, plantHeight$lat)\n## [1] -0.9249304\n\nIn principle, the regression model should be able to still separate them, but the higher the collinearity, the more difficult it becomes for the regression to infer if the effect is caused by one or the other predictor."
  },
  {
    "objectID": "4C-Exercise.html#analyzing-elk-data",
    "href": "4C-Exercise.html#analyzing-elk-data",
    "title": "Exercise - GLM",
    "section": "Analyzing Elk data",
    "text": "Analyzing Elk data\nDownload the elk_data from GRIPS and import the file using the load( ) function.\n\n\nRData is a R specific data type. You can save any R object by running save(object, file \"filename.RData\")\n\n\n\n\n\n\nExample - Elk Data\n\n\n\nYou will be given a data set of habitat use of Elks in Canada. Measured is the presence of Elks (0/1), and a number of other predictors. Description of variables:\n\ndist_roads - distance of the location to the next road\nNDVI - normalized difference vegetation index, essentially greeness of vegetation on the site\nruggedness of the terrain\ndem - digital eleveation model = elevation above sea level\npresence - presence of the elk\nhabitat - open or forest\n\nPerform either:\n\nA predictive analysis, i.e. a model to predict where Elks can be found.\nA causal analysis, trying to understand the effect of roads on Elk presence.\n\nHints:\n\nHypothesis: presence ~ dist_roads\nWhat are potential confounders? How can you see if a variable is a confounder?\n\n\n\n\n\nClick here to see the solution\n\nA. Predictive analysis\n\nload(file = \"elk_data.RData\")\n\nlibrary(MASS)\nfit &lt;- glm(presence ~ dist_roads  + dem + ruggedness, data = elk_data, family = \"binomial\")\npredictive_model = MASS::stepAIC(fit, direction = \"both\")\n## Start:  AIC=5109.03\n## presence ~ dist_roads + dem + ruggedness\n## \n##              Df Deviance    AIC\n## - dist_roads  1   5101.9 5107.9\n## &lt;none&gt;            5101.0 5109.0\n## - ruggedness  1   5171.4 5177.4\n## - dem         1   5241.3 5247.3\n## \n## Step:  AIC=5107.94\n## presence ~ dem + ruggedness\n## \n##              Df Deviance    AIC\n## &lt;none&gt;            5101.9 5107.9\n## + dist_roads  1   5101.0 5109.0\n## - ruggedness  1   5172.0 5176.0\n## - dem         1   5324.8 5328.8\nsummary(predictive_model)\n## \n## Call:\n## glm(formula = presence ~ dem + ruggedness, family = \"binomial\", \n##     data = elk_data)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -6.7890658  0.4917287 -13.807   &lt;2e-16 ***\n## dem          0.0042951  0.0002994  14.343   &lt;2e-16 ***\n## ruggedness  -0.0289100  0.0035076  -8.242   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 5334.5  on 3847  degrees of freedom\n## Residual deviance: 5101.9  on 3845  degrees of freedom\n## AIC: 5107.9\n## \n## Number of Fisher Scoring iterations: 4\n\nB. Causal analysis\nThe predictive model has actually dropped the variable of interest (distance to roads) which shows the risks of tools that select for the best predictive model such as AIC selection: Collinear variables that we need to adjust our effects, are often dropped.\nFor the causal model, we really need to think about the causal relationships between the variables:\nWe are interested in the effect of dist_roads on presence:\n\nsummary(glm(presence ~ dist_roads, data = elk_data, family = \"binomial\"))\n## \n## Call:\n## glm(formula = presence ~ dist_roads, family = \"binomial\", data = elk_data)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -4.101e-01  6.026e-02  -6.806  1.0e-11 ***\n## dist_roads   3.204e-04  3.977e-05   8.056  7.9e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 5334.5  on 3847  degrees of freedom\n## Residual deviance: 5268.1  on 3846  degrees of freedom\n## AIC: 5272.1\n## \n## Number of Fisher Scoring iterations: 4\n\nPositive effect of dist_roads on elk, or in other words, more elks closer to the roads? Does that make sense? No, we expect a negative effect!\nAltitude (dem) and the ruggedness probably affect both variables, presence and dist_roads, and thus they should be considered as confounders:\n\nfit = glm(presence ~ dist_roads+ dem + ruggedness, data = elk_data, family = \"binomial\")\n\nThe effect of dist_roads is now negative!\nLet’s check the residuals:\n\nlibrary(DHARMa)\nres &lt;- simulateResiduals(fit, plot = TRUE)\n\n\n\nplot(res, quantreg = TRUE)\n\n\n\nplotResiduals(res, form = elk_data$dem, quantreg = TRUE)\n\n\n\nplotResiduals(res, form = elk_data$ruggedness, quantreg = TRUE)\n\n\n\n\nThe functional forms of our confounders are not perfect.\nSince we are not really interested in them, a cool trick is to use a GAM (generalized addictive model) which automatically adjusts the functional for of the fitted curve to flexibly take care of the confounders. Our main predictor dist_roads is still modelled as a linear effect.\n\nlibrary(mgcv)\n## Loading required package: nlme\n## This is mgcv 1.8-42. For overview type 'help(\"mgcv-package\")'.\nfit2 &lt;- gam(presence ~ dist_roads + s(dem) + s(ruggedness), data = elk_data, family = \"binomial\")\nsummary(fit2)\n## \n## Family: binomial \n## Link function: logit \n## \n## Formula:\n## presence ~ dist_roads + s(dem) + s(ruggedness)\n## \n## Parametric coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept)  1.783e-01  8.229e-02   2.167  0.03025 * \n## dist_roads  -1.798e-04  5.771e-05  -3.115  0.00184 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Approximate significance of smooth terms:\n##                 edf Ref.df Chi.sq p-value    \n## s(dem)        8.283  8.845  220.3  &lt;2e-16 ***\n## s(ruggedness) 8.510  8.918  128.3  &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## R-sq.(adj) =  0.114   Deviance explained = 9.27%\n## UBRE = 0.26754  Scale est. = 1         n = 3848\nplot(fit2)\n\n\n\n\n\n\n\nLet’s take another look at the residual plots, in particular for the confounders.\n\nres &lt;- simulateResiduals(fit2, plot = TRUE)\n## Registered S3 method overwritten by 'GGally':\n##   method from   \n##   +.gg   ggplot2\n## Registered S3 method overwritten by 'mgcViz':\n##   method from  \n##   +.gg   GGally\n\n\n\nplot(res, quantreg = TRUE)\n\n\n\nplotResiduals(res, form = elk_data$dem, quantreg = TRUE)\n\n\n\nplotResiduals(res, form = elk_data$ruggedness, quantreg = TRUE)\n\n\n\n\nNow, everything looks perfect"
  },
  {
    "objectID": "8-MachineLearning.html#regression",
    "href": "8-MachineLearning.html#regression",
    "title": "11  Machine Learning",
    "section": "11.1 Regression",
    "text": "11.1 Regression\nWe call task with a numerical response variable a regression task:\n\nindices = sample.int(nrow(airquality), 50)\ntrain = airquality[-indices,]\ntest = airquality[indices,]\n\n# 1. Fit model on train data:\nmodel = randomForest(Ozone~., data = train)\n\n# 2. Make Predictions\npredictions = predict(model, newdata = test)\n\n# 3. Compare predictions with observed values:\n## the root mean squared error is commonly used as an error statistic:\nsqrt(mean((predictions-test$Ozone)**2))\n## [1] 20.41785\n# Or use a correlationf actor\ncor(predictions, test$Ozone)\n## [1] 0.9017803\n# Or Rsquared\ncor(predictions, test$Ozone)**2\n## [1] 0.8132077"
  },
  {
    "objectID": "8-MachineLearning.html#classification",
    "href": "8-MachineLearning.html#classification",
    "title": "11  Machine Learning",
    "section": "11.2 Classification",
    "text": "11.2 Classification\nWe call a task with a categorical response variable a classification task (see also multi-class and multi-label classification):\n\nindices = sample.int(nrow(iris), 50)\ntrain = iris[-indices,]\ntest = iris[indices,]\n\n# 1. Fit model on train data:\nmodel = randomForest(Species~., data = train)\n\n# 2. Make Predictions\npredictions = predict(model, newdata = test)\n\n# 3. Compare predictions with observed values:\nmean(predictions == test$Species) # accuracy\n## [1] 0.94\n\n96% accuracy, which means only 4% of the observations were wrongly classified by our random forest!\nVariable importance:\n\nvarImpPlot(model)\n\n\n\n\nPetal.Width and Petal.Length were the most important predictors!"
  },
  {
    "objectID": "8-MachineLearning.html#exercise",
    "href": "8-MachineLearning.html#exercise",
    "title": "11  Machine Learning",
    "section": "11.3 Exercise",
    "text": "11.3 Exercise\n\n11.3.1 birdabundance dataset\n\nlibrary(EcoData)\nlibrary(randomForest)\nset.seed(42)\nindices = sample.int(nrow(birdabundance), 30)\ntrain = birdabundance[-indices,]\ntest = birdabundance[indices,]\n# ABUND is the response variable\n\nTask:\n\nFit random forest on train data\nPredict for test data\nCalculate R2\nDo the same with a lm and compare the predictive performance of both models\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nrf = randomForest(ABUND~., data = train)\nm = lm(ABUND~., data = train)\n\npred1 = predict(rf, newdata = test)\npred2 = predict(m, newdata = test)\n\ncor(pred1, test$ABUND)**2\n## [1] 0.6596678\ncor(pred2, test$ABUND)**2\n## [1] 0.1983452\n\nRF clearly outperforms the linear regression model!\n\n\n\n\n\n11.3.2 titantic dataset\n\nlibrary(EcoData)\nlibrary(randomForest)\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following object is masked from 'package:randomForest':\n## \n##     combine\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nset.seed(42)\ntitanic_sub = titanic %&gt;% select(survived, age, pclass, sex, fare)\ntitanic_sub = titanic_sub[complete.cases(titanic_sub),]\n\nindices = sample.int(nrow(titanic_sub), 200)\ntrain = titanic_sub[-indices,]\ntest = titanic_sub[indices,]\n\nTask:\n\nFit random forest on train data\nPredict for test data\nCalculate Accuracy\nDo the same with a glm (binomial) and compare the predictive performance of both models\nWhat is the most important variable?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nrf = randomForest(as.factor(survived)~., data = train)\nm = glm(survived~., data = train, family = binomial)\n\npred1 = predict(rf, newdata = test)\npred2 = predict(m, newdata = test, type = \"response\")\n\n# pred2 are probabilities, we have to change them to levels\npred2 = ifelse(pred2 &lt; 0.5, 0, 1)\n\nmean(pred1 == test$survived) # RF\n## [1] 0.82\nmean(pred2 == test$survived) # glm\n## [1] 0.765\n\nRF is better than the glm!\n\nvarImpPlot(rf)\n\n\n\n\nSex is the most important variable!\n\n\n\n\n\n11.3.3 Bias-variance tradeoff\nAn important concept of statistics and, in particular, ML is the concept of the bias-variance tradeoff - or in other words, finding the right complexity of the model. So how flexible should our model be so that it generalizes well to other/new observations. Many ML algorithms have complexity parameters (e.g. nodesize or mtry in RF) that control their complexity. Have a look at the following youtube video about the bias-variance tradeoff:\n\nLet’s see how we can control the complexity in the Random Forest algorithm:\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\n\npred = predict(rf, data)\nimportance(rf)\n##         IncNodePurity\n## Solar.R      17969.59\n## Wind         31978.36\n## Temp         34176.71\n## Month        10753.73\n## Day          15436.47\n#&gt;         IncNodePurity\n#&gt; Solar.R      17969.59\n#&gt; Wind         31978.36\n#&gt; Temp         34176.71\n#&gt; Month        10753.73\n#&gt; Day          15436.47\ncat(\"RMSE: \", sqrt(mean((data$Ozone - pred)^2)), \"\\n\")\n## RMSE:  9.507848\n#&gt; RMSE:  9.507848\n\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n\n\n\n\nTry different values for the nodesize and mtry and describe how the predictions depend on these parameters. (randomForest(…, nodesize = …, mtry = …) (the exercise was taken from the ML course book)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\n\nfor(nodesize in c(1, 5, 15, 50, 100)){\n  for(mtry in c(1, 3, 5)){\n    rf = randomForest(Ozone~., data = data, mtry = mtry, nodesize = nodesize)\n    \n    pred = predict(rf, data)\n    \n    plot(data$Temp, data$Ozone, main = paste0(\n        \"mtry: \", mtry, \"    nodesize: \", nodesize,\n        \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n    )\n    lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11.3.4 ML pipeline\nIf you want to know more about a typical ML pipeline/workflow, read this chapter from the ML course!"
  },
  {
    "objectID": "5-MultipleRegression.html#confounder",
    "href": "5-MultipleRegression.html#confounder",
    "title": "8  Multiple regression",
    "section": "8.1 Confounder",
    "text": "8.1 Confounder\nConfounders have effects on the response and another predictor.\n\nClimate = runif(100)\nTemp = Climate + rnorm(100, sd = 0.2)\nGrowth = 0.5*Temp - 1.0*Climate + rnorm(100, sd = 0.2)\n\nsummary(lm(Growth~Temp))\n## \n## Call:\n## lm(formula = Growth ~ Temp)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.55719 -0.18748 -0.01354  0.18858  0.59337 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.16604    0.04228  -3.927  0.00016 ***\n## Temp        -0.19311    0.06602  -2.925  0.00428 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2472 on 98 degrees of freedom\n## Multiple R-squared:  0.0803, Adjusted R-squared:  0.07091 \n## F-statistic: 8.556 on 1 and 98 DF,  p-value: 0.004279\nsummary(lm(Growth~Temp+Climate)) # correct effects!!\n## \n## Call:\n## lm(formula = Growth ~ Temp + Climate)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.41912 -0.13228 -0.00661  0.12988  0.41630 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.009234   0.038203   0.242     0.81    \n## Temp         0.568083   0.102652   5.534 2.66e-07 ***\n## Climate     -1.088041   0.127964  -8.503 2.27e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.1881 on 97 degrees of freedom\n## Multiple R-squared:  0.473,  Adjusted R-squared:  0.4622 \n## F-statistic: 43.54 on 2 and 97 DF,  p-value: 3.205e-14\n\nIdentifying confounders is the most important challenge in observational studies: For example, several studies showed that overweight adults have lower mortality. However, another study shows that these earlier results might have come up due to confounding: smoking!\n\nsmokers: higher mortality and lower BMI -&gt; people with lower BMI have higher mortality rates\nWhen we correct for the confounder smoking, the correlation between BMI and mortality goes in the other direction, i.e. obese people have higher mortality!\n\nConfounders can even lead to observed correlations where in reality there is no such correlation. This is called spurious correlation.\n\n\n\n\n\n\nWarning\n\n\n\nConclusion: Confounders can cause correlations where no causal relationship exists."
  },
  {
    "objectID": "5-MultipleRegression.html#multiple-lm",
    "href": "5-MultipleRegression.html#multiple-lm",
    "title": "8  Multiple regression",
    "section": "8.2 Multiple LM",
    "text": "8.2 Multiple LM\nThe multiple linear regression can deal with confounders:\n\nUnivariate (simple) linear regression describes how y depends on x using a polynomial of x1 e.g.: \\[\ny = a_0 + a_1*x_1 + a_2*x_1^2\n\\]\nMultiple linear regression expands simple linear regression to a polynomial of several explanatory variables x1, x2… e.g.: \\[\ny = a_0 + a_1*x_1 + a_2*x_2 + a_3*x_3\n\\]\nIdea: if we jointly consider “all” variables in the model formula, the influence of confounding variables is incorporated\n\n\n## first remove observations with NA values\nnewAirquality = airquality[complete.cases(airquality),]\nsummary(newAirquality)\n##      Ozone          Solar.R           Wind            Temp      \n##  Min.   :  1.0   Min.   :  7.0   Min.   : 2.30   Min.   :57.00  \n##  1st Qu.: 18.0   1st Qu.:113.5   1st Qu.: 7.40   1st Qu.:71.00  \n##  Median : 31.0   Median :207.0   Median : 9.70   Median :79.00  \n##  Mean   : 42.1   Mean   :184.8   Mean   : 9.94   Mean   :77.79  \n##  3rd Qu.: 62.0   3rd Qu.:255.5   3rd Qu.:11.50   3rd Qu.:84.50  \n##  Max.   :168.0   Max.   :334.0   Max.   :20.70   Max.   :97.00  \n##      Month            Day       \n##  Min.   :5.000   Min.   : 1.00  \n##  1st Qu.:6.000   1st Qu.: 9.00  \n##  Median :7.000   Median :16.00  \n##  Mean   :7.216   Mean   :15.95  \n##  3rd Qu.:9.000   3rd Qu.:22.50  \n##  Max.   :9.000   Max.   :31.00\n\n# simple regression\nm0 = lm(Ozone ~ Temp , data = newAirquality)\nsummary(m0)\n## \n## Call:\n## lm(formula = Ozone ~ Temp, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -40.922 -17.459  -0.874  10.444 118.078 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -147.6461    18.7553  -7.872 2.76e-12 ***\n## Temp           2.4391     0.2393  10.192  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 23.92 on 109 degrees of freedom\n## Multiple R-squared:  0.488,  Adjusted R-squared:  0.4833 \n## F-statistic: 103.9 on 1 and 109 DF,  p-value: &lt; 2.2e-16\nplot(m0)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(Ozone ~ Temp , data = newAirquality)\nabline(m0, col = \"blue\", lwd = 3)\n\n\n\n\n# Today: multiple linear regression\nm1 = lm(Ozone ~ Temp + Wind , data = newAirquality)\n# have a look at the residuals:\nop &lt;- par(mfrow = c(2,2))\nplot(m1)\n\n\n\npar(op)\n\nsummary(m1)\n## \n## Call:\n## lm(formula = Ozone ~ Temp + Wind, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -42.156 -13.216  -3.123  10.598  98.492 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -67.3220    23.6210  -2.850  0.00524 ** \n## Temp          1.8276     0.2506   7.294 5.29e-11 ***\n## Wind         -3.2948     0.6711  -4.909 3.26e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 21.73 on 108 degrees of freedom\n## Multiple R-squared:  0.5814, Adjusted R-squared:  0.5736 \n## F-statistic: 74.99 on 2 and 108 DF,  p-value: &lt; 2.2e-16\n\n# plotting multiple regression outputs\nlibrary(effects)\n## Loading required package: carData\n## lattice theme set by effectsTheme()\n## See ?effectsTheme for details.\nplot(allEffects(m1))\n\n\n\n\n\n## Omitted variable bias\nboth = lm(Ozone ~ Wind + Temp, newAirquality)\nwind = lm(Ozone ~ Wind , newAirquality)\ntemp = lm(Ozone ~ Temp, newAirquality)\nsummary(both)\n## \n## Call:\n## lm(formula = Ozone ~ Wind + Temp, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -42.156 -13.216  -3.123  10.598  98.492 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -67.3220    23.6210  -2.850  0.00524 ** \n## Wind         -3.2948     0.6711  -4.909 3.26e-06 ***\n## Temp          1.8276     0.2506   7.294 5.29e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 21.73 on 108 degrees of freedom\n## Multiple R-squared:  0.5814, Adjusted R-squared:  0.5736 \n## F-statistic: 74.99 on 2 and 108 DF,  p-value: &lt; 2.2e-16\nsummary(wind)\n## \n## Call:\n## lm(formula = Ozone ~ Wind, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -43.513 -18.597  -5.035  15.814  88.437 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  99.0413     7.4724   13.25  &lt; 2e-16 ***\n## Wind         -5.7288     0.7082   -8.09 9.09e-13 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 26.42 on 109 degrees of freedom\n## Multiple R-squared:  0.3752, Adjusted R-squared:  0.3694 \n## F-statistic: 65.44 on 1 and 109 DF,  p-value: 9.089e-13\n\nslopes &lt;- data.frame(\n  predictor = c(\"Wind\", \"Temp\"),\n  both.pred = round(coef(both)[2:3], digits = 2),\n  only.wind = c(round(coef(wind)[2], digits = 2), \"NA\"),\n  only.temp = c(\"NA\", round(coef(temp)[2], digits = 2))\n)\nslopes\n##      predictor both.pred only.wind only.temp\n## Wind      Wind     -3.29     -5.73        NA\n## Temp      Temp      1.83        NA      2.44\n\nOmitting Wind makes the effect of Temperature larger.\nProblem: Multiple regression can separate the effect of collinear explanatory variables, but only, if collinearity is not too strong.\nSolution: If the correlation is really strong, we can omit one variable and interpret the remaining collinear variable as representing both."
  },
  {
    "objectID": "5-MultipleRegression.html#interactions-between-variables",
    "href": "5-MultipleRegression.html#interactions-between-variables",
    "title": "8  Multiple regression",
    "section": "8.3 Interactions between variables",
    "text": "8.3 Interactions between variables\nIf one predictor influences the effect of the other predictor, we can include an interaction term into our model:\n\\[\ny \\sim a + b + a:b\n\\]\nor:\n\\[\ny \\sim a*b\n\\]\n\n# Include interaction\nm2 = lm(Ozone ~  scale(Wind)* scale(Temp) , data = newAirquality)\n# if including interactions, always scale your predictor variables!\n# scale: subtracts the mean and divides by standard deviation\nsummary(m2)\n## \n## Call:\n## lm(formula = Ozone ~ scale(Wind) * scale(Temp), data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -40.930 -11.193  -3.034   8.193  97.456 \n## \n## Coefficients:\n##                         Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)               38.469      2.137  18.002  &lt; 2e-16 ***\n## scale(Wind)              -11.758      2.238  -5.253 7.68e-07 ***\n## scale(Temp)               17.544      2.239   7.837 3.62e-12 ***\n## scale(Wind):scale(Temp)   -7.367      1.848  -3.987 0.000123 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 20.37 on 107 degrees of freedom\n## Multiple R-squared:  0.6355, Adjusted R-squared:  0.6253 \n## F-statistic: 62.19 on 3 and 107 DF,  p-value: &lt; 2.2e-16\nop &lt;- par(mfrow = c(2,2))\nplot(m2)\n\n\n\npar(op)\n\nThe influence of temperature on growth depends on the amount of precipitation, or: If there’s not enough water, also higher temperatures cannot increase growth.\nExample:\n\n# How does everything change, if we have factorial predictors?\nnewAirquality$MonthFactor = as.factor(newAirquality$Month)\n\nm4 = lm(sqrt(Ozone) ~ MonthFactor + scale(Wind) * scale(Temp) * scale(Solar.R) , \n        data = newAirquality)\nsummary(m4)\n## \n## Call:\n## lm(formula = sqrt(Ozone) ~ MonthFactor + scale(Wind) * scale(Temp) * \n##     scale(Solar.R), data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.6096 -0.8869 -0.2067  0.7647  4.3191 \n## \n## Coefficients:\n##                                        Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                             6.12172    0.37148  16.479  &lt; 2e-16 ***\n## MonthFactor6                           -0.54487    0.60633  -0.899 0.371025    \n## MonthFactor7                           -0.37571    0.51347  -0.732 0.466072    \n## MonthFactor8                           -0.03770    0.52839  -0.071 0.943262    \n## MonthFactor9                           -0.74343    0.43308  -1.717 0.089179 .  \n## scale(Wind)                            -0.76983    0.16456  -4.678 9.18e-06 ***\n## scale(Temp)                             1.35350    0.20937   6.465 3.86e-09 ***\n## scale(Solar.R)                          0.65689    0.16212   4.052 0.000101 ***\n## scale(Wind):scale(Temp)                -0.30440    0.14655  -2.077 0.040379 *  \n## scale(Wind):scale(Solar.R)             -0.07695    0.17222  -0.447 0.655999    \n## scale(Temp):scale(Solar.R)              0.22985    0.15451   1.488 0.140040    \n## scale(Wind):scale(Temp):scale(Solar.R)  0.03202    0.15179   0.211 0.833366    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.328 on 99 degrees of freedom\n## Multiple R-squared:  0.7335, Adjusted R-squared:  0.7039 \n## F-statistic: 24.78 on 11 and 99 DF,  p-value: &lt; 2.2e-16\n\nm5 = lm(sqrt(Ozone) ~ MonthFactor + scale(Wind) + scale(Temp) + scale(Solar.R) \n                      + scale(Wind):scale(Temp)\n                      + scale(Wind):scale(Solar.R)\n                      + scale(Temp):scale(Solar.R), \n        data = newAirquality)\nsummary(m5)\n## \n## Call:\n## lm(formula = sqrt(Ozone) ~ MonthFactor + scale(Wind) + scale(Temp) + \n##     scale(Solar.R) + scale(Wind):scale(Temp) + scale(Wind):scale(Solar.R) + \n##     scale(Temp):scale(Solar.R), data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.6023 -0.9182 -0.2180  0.7713  4.3209 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                 6.12350    0.36960  16.568  &lt; 2e-16 ***\n## MonthFactor6               -0.54871    0.60315  -0.910   0.3652    \n## MonthFactor7               -0.39194    0.50524  -0.776   0.4397    \n## MonthFactor8               -0.04701    0.52402  -0.090   0.9287    \n## MonthFactor9               -0.74873    0.43028  -1.740   0.0849 .  \n## scale(Wind)                -0.75588    0.14997  -5.040 2.07e-06 ***\n## scale(Temp)                 1.35192    0.20823   6.492 3.29e-09 ***\n## scale(Solar.R)              0.65178    0.15953   4.086 8.88e-05 ***\n## scale(Wind):scale(Temp)    -0.31305    0.14002  -2.236   0.0276 *  \n## scale(Wind):scale(Solar.R) -0.09259    0.15469  -0.599   0.5508    \n## scale(Temp):scale(Solar.R)  0.23573    0.15126   1.558   0.1223    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.321 on 100 degrees of freedom\n## Multiple R-squared:  0.7334, Adjusted R-squared:  0.7068 \n## F-statistic: 27.51 on 10 and 100 DF,  p-value: &lt; 2.2e-16\n\n# short form for including only two-way interactions:\n\nm5 = lm(sqrt(Ozone) ~ MonthFactor + (scale(Wind) + scale(Temp) + scale(Solar.R))^2,\n        data = newAirquality)\nsummary(m5)\n## \n## Call:\n## lm(formula = sqrt(Ozone) ~ MonthFactor + (scale(Wind) + scale(Temp) + \n##     scale(Solar.R))^2, data = newAirquality)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -2.6023 -0.9182 -0.2180  0.7713  4.3209 \n## \n## Coefficients:\n##                            Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                 6.12350    0.36960  16.568  &lt; 2e-16 ***\n## MonthFactor6               -0.54871    0.60315  -0.910   0.3652    \n## MonthFactor7               -0.39194    0.50524  -0.776   0.4397    \n## MonthFactor8               -0.04701    0.52402  -0.090   0.9287    \n## MonthFactor9               -0.74873    0.43028  -1.740   0.0849 .  \n## scale(Wind)                -0.75588    0.14997  -5.040 2.07e-06 ***\n## scale(Temp)                 1.35192    0.20823   6.492 3.29e-09 ***\n## scale(Solar.R)              0.65178    0.15953   4.086 8.88e-05 ***\n## scale(Wind):scale(Temp)    -0.31305    0.14002  -2.236   0.0276 *  \n## scale(Wind):scale(Solar.R) -0.09259    0.15469  -0.599   0.5508    \n## scale(Temp):scale(Solar.R)  0.23573    0.15126   1.558   0.1223    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.321 on 100 degrees of freedom\n## Multiple R-squared:  0.7334, Adjusted R-squared:  0.7068 \n## F-statistic: 27.51 on 10 and 100 DF,  p-value: &lt; 2.2e-16\n# get overall effect of Month:\nanova(m5)\n## Analysis of Variance Table\n## \n## Response: sqrt(Ozone)\n##                             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \n## MonthFactor                  4 158.726  39.681 22.7249 2.261e-13 ***\n## scale(Wind)                  1 149.523 149.523 85.6296 4.282e-15 ***\n## scale(Temp)                  1 126.124 126.124 72.2290 1.899e-13 ***\n## scale(Solar.R)               1  19.376  19.376 11.0961 0.0012129 ** \n## scale(Wind):scale(Temp)      1  20.639  20.639 11.8198 0.0008556 ***\n## scale(Wind):scale(Solar.R)   1   1.803   1.803  1.0328 0.3119518    \n## scale(Temp):scale(Solar.R)   1   4.241   4.241  2.4288 0.1222856    \n## Residuals                  100 174.616   1.746                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# this is doing a type I ANOVA = sequential\n# order in which you include the predictors changes the estimates and p-values\n\n# If you want to do a type II ANOVA, use ANova() from the car package\nlibrary(car)\nAnova(m5) # Anova with capital A\n## Anova Table (Type II tests)\n## \n## Response: sqrt(Ozone)\n##                             Sum Sq  Df F value    Pr(&gt;F)    \n## MonthFactor                  9.557   4  1.3683 0.2503349    \n## scale(Wind)                 41.993   1 24.0488 3.641e-06 ***\n## scale(Temp)                 78.938   1 45.2067 1.112e-09 ***\n## scale(Solar.R)              23.189   1 13.2797 0.0004276 ***\n## scale(Wind):scale(Temp)      8.728   1  4.9983 0.0275955 *  \n## scale(Wind):scale(Solar.R)   0.626   1  0.3582 0.5508395    \n## scale(Temp):scale(Solar.R)   4.241   1  2.4288 0.1222856    \n## Residuals                  174.616 100                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#type II ANOVA: all other predictors have already been taken into account\n# Does an additional predictor explain some of the variance on top of that?"
  },
  {
    "objectID": "5-MultipleRegression.html#model-selection",
    "href": "5-MultipleRegression.html#model-selection",
    "title": "8  Multiple regression",
    "section": "8.4 Model selection",
    "text": "8.4 Model selection\nWe’ve learned that we should include variables in the model that are collinear, that is they correlate with other predictors, but how many and which factors should we include?\nFamous example: Female hurricanes are deadlier than male hurricanes (Jung et al., 2014)\nThey have analyzed the number of fatalities of hurricane and claimed that there is an effect of femininity of the name on the number of deads (while correcting for confounders). They recommend to give hurricans only male names because it would considerably reduce the number of deads.\n\nlibrary(DHARMa)\n## This is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\nlibrary(effects)\n?hurricanes\nstr(hurricanes)\n## Classes 'tbl_df', 'tbl' and 'data.frame':    92 obs. of  14 variables:\n##  $ Year                    : num  1950 1950 1952 1953 1953 ...\n##  $ Name                    : chr  \"Easy\" \"King\" \"Able\" \"Barbara\" ...\n##  $ MasFem                  : num  6.78 1.39 3.83 9.83 8.33 ...\n##  $ MinPressure_before      : num  958 955 985 987 985 960 954 938 962 987 ...\n##  $ Minpressure_Updated_2014: num  960 955 985 987 985 960 954 938 962 987 ...\n##  $ Gender_MF               : num  1 0 0 1 1 1 1 1 1 1 ...\n##  $ Category                : num  3 3 1 1 1 3 3 4 3 1 ...\n##  $ alldeaths               : num  2 4 3 1 0 60 20 20 0 200 ...\n##  $ NDAM                    : num  1590 5350 150 58 15 ...\n##  $ Elapsed_Yrs             : num  63 63 61 60 60 59 59 59 58 58 ...\n##  $ Source                  : chr  \"MWR\" \"MWR\" \"MWR\" \"MWR\" ...\n##  $ ZMasFem                 : num  -0.000935 -1.670758 -0.913313 0.945871 0.481075 ...\n##  $ ZMinPressure_A          : num  -0.356 -0.511 1.038 1.141 1.038 ...\n##  $ ZNDAM                   : num  -0.439 -0.148 -0.55 -0.558 -0.561 ...\n\nlibrary(glmmTMB)\n## Warning in checkMatrixPackageVersion(): Package version inconsistency detected.\n## TMB was built with Matrix version 1.5.4\n## Current Matrix version is 1.5.4.1\n## Please re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n## Warning in checkDepPackageVersion(dep_pkg = \"TMB\"): Package version inconsistency detected.\n## glmmTMB was built with TMB version 1.9.6\n## Current TMB version is 1.9.4\n## Please re-install glmmTMB from source or restore original 'TMB' package (see '?reinstalling' for more information)\n\nm1 = glmmTMB(alldeaths ~ MasFem*\n                             (Minpressure_Updated_2014 + scale(NDAM)),\n                           data = hurricanes, family = nbinom2)\nsummary(m1)\n##  Family: nbinom2  ( log )\n## Formula:          alldeaths ~ MasFem * (Minpressure_Updated_2014 + scale(NDAM))\n## Data: hurricanes\n## \n##      AIC      BIC   logLik deviance df.resid \n##    660.7    678.4   -323.4    646.7       85 \n## \n## \n## Dispersion parameter for nbinom2 family (): 0.787 \n## \n## Conditional model:\n##                                  Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)                     69.661590  23.425598   2.974 0.002942 ** \n## MasFem                          -5.855078   2.716589  -2.155 0.031138 *  \n## Minpressure_Updated_2014        -0.069870   0.024251  -2.881 0.003964 ** \n## scale(NDAM)                     -0.494094   0.455968  -1.084 0.278536    \n## MasFem:Minpressure_Updated_2014  0.006108   0.002813   2.171 0.029901 *  \n## MasFem:scale(NDAM)               0.205418   0.061956   3.316 0.000915 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nInteractions -&gt; we need to scale variables:\n\nm2 = glmmTMB(alldeaths ~ scale(MasFem)*\n                             (scale(Minpressure_Updated_2014) + scale(NDAM)+scale(sqrt(NDAM))),\n                           data = hurricanes, family = nbinom2)\nsummary(m2)\n##  Family: nbinom2  ( log )\n## Formula:          \n## alldeaths ~ scale(MasFem) * (scale(Minpressure_Updated_2014) +  \n##     scale(NDAM) + scale(sqrt(NDAM)))\n## Data: hurricanes\n## \n##      AIC      BIC   logLik deviance df.resid \n##    634.9    657.6   -308.4    616.9       83 \n## \n## \n## Dispersion parameter for nbinom2 family (): 1.12 \n## \n## Conditional model:\n##                                               Estimate Std. Error z value\n## (Intercept)                                    2.28082    0.10850  21.022\n## scale(MasFem)                                  0.05608    0.10672   0.525\n## scale(Minpressure_Updated_2014)               -0.14267    0.17804  -0.801\n## scale(NDAM)                                   -1.11104    0.28030  -3.964\n## scale(sqrt(NDAM))                              2.10764    0.36487   5.776\n## scale(MasFem):scale(Minpressure_Updated_2014)  0.07371    0.19618   0.376\n## scale(MasFem):scale(NDAM)                     -0.10159    0.27080  -0.375\n## scale(MasFem):scale(sqrt(NDAM))                0.32960    0.36594   0.901\n##                                               Pr(&gt;|z|)    \n## (Intercept)                                    &lt; 2e-16 ***\n## scale(MasFem)                                    0.599    \n## scale(Minpressure_Updated_2014)                  0.423    \n## scale(NDAM)                                   7.38e-05 ***\n## scale(sqrt(NDAM))                             7.63e-09 ***\n## scale(MasFem):scale(Minpressure_Updated_2014)    0.707    \n## scale(MasFem):scale(NDAM)                        0.708    \n## scale(MasFem):scale(sqrt(NDAM))                  0.368    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe effect of femininity is gone! Already with the scaled variables, but also with the transformation with the NDAM variable. The question was raised which of both is more reasonable, whether the relationship between damage and mortality isn’t a straight line or that the gender of the hurricane names affect deaths (Bob O’Hara and GrrlScientist). They argue that the model with the transformed variable fits the data better which brings us to the topic of this section, how to choose between different models? Answering this question if the goal of model selection.\nWhy not include all the variables we can measure in our model? Problem with the full model:\n\nIf you have more parameters than data points, the model cannot be fitted at all\nEven with n (samples) ~ k (number of parameters), model properties become very unfavorable (high p-values and uncertainties/standard errors) –&gt; Overfitting\n\nA “good model” depends on the goal of the analysis, do we want to optimize:\n\nPredictive ability – how well can we predict with the model?\nInferential ability – do we identify the true values for the parameters (true effects), are the p-values correct, can we correctly say that a variable has an effect?\n\nThe more complex a model gets, the better it fits to the data, but there’s a downside, the bias-variance tradeoff.\nExplanation bias-variance tradeoff\nExplanation LRT and AIC\nProblem of p-hacking\nExample:\n\n# Compare different competing models:\n# let's compare models m3 and m5 to decide which one explains our data better:\n# 1. LRT\nanova(m3, m5)\n# RSS = residual sum of squares = variance not explained by the model\n# smaller RSS = better model\n# p-value\n\n#2. AIC\nAIC(m3)\nAIC(m5)\n# also here, model m5 is better\n\n\n#### Demonstration: Why interpretation of effect sizes and p-values \n### after extensive model selection is not a good idea:\nlibrary(MASS)\nset.seed(1)\n#make up predictors:\ndat = data.frame(matrix(runif(20000), ncol = 100))\n# create a response variable\ndat$y = rnorm(200)\nfullModel = lm(y ~ ., data = dat)\nsum &lt;- summary(fullModel)\nmean(sum$coefficients[,4] &lt; 0.05)\n# 0.019: less than 2 % false positives = type I error rate\n\nselection = stepAIC(fullModel)\nsum.sel &lt;- summary(selection)\nmean(sum.sel$coefficients[,4] &lt; 0.05)\n# 0.48: Now almost 50 % of our results are false positives!!!"
  },
  {
    "objectID": "5-MultipleRegression.html#formula-syntax",
    "href": "5-MultipleRegression.html#formula-syntax",
    "title": "8  Multiple regression",
    "section": "8.5 Formula syntax",
    "text": "8.5 Formula syntax\n\nFormula syntax\n\n\n\n\n\n\n\nFormula\nMeaning\nDetails\n\n\n\n\ny~x_1\n\\(y=a_0 +a_1*x_1\\)\nSlope+Intercept\n\n\ny~x_1 - 1\n\\(y=a_1*x_1\\)\nSlope, no intercept\n\n\ny~I(x_1^2)\n\\(y=a_0 + a_1*(x_1^2)\\)\nQuadratic effect\n\n\ny~x_1+x_2\n\\(y=a_0+a_1*x_1+a_2*x_2\\)\nMultiple linear regression (two variables)\n\n\ny~x_1:x_2\n\\(y=a_0+a_1*(x_1*x_2)\\)\nInteraction between x1 and x2\n\n\ny~x_1*x_2\n\\(y=a_0+a_1*(x_1*x_2)+a_2*x_1+a_3*x_2\\)\nInteraction and main effects"
  },
  {
    "objectID": "6A-Exercise.html#birdabundance-dataset",
    "href": "6A-Exercise.html#birdabundance-dataset",
    "title": "12  Exercise",
    "section": "12.1 birdabundance dataset",
    "text": "12.1 birdabundance dataset\n\nlibrary(EcoData)\nlibrary(randomForest)\n## randomForest 4.7-1.1\n## Type rfNews() to see new features/changes/bug fixes.\nset.seed(42)\nindices = sample.int(nrow(birdabundance), 30)\ntrain = birdabundance[-indices,]\ntest = birdabundance[indices,]\n# ABUND is the response variable\n\n\n\n\n\n\n\nWarning\n\n\n\nTask:\n\nFit random forest on train data\nPredict for test data\nCalculate R2\nDo the same with a lm and compare the predictive performance of both models\n\n\n\n\n\nClick here to see the solution\n\n\nrf = randomForest(ABUND~., data = train)\nm = lm(ABUND~., data = train)\n\npred1 = predict(rf, newdata = test)\npred2 = predict(m, newdata = test)\n\ncor(pred1, test$ABUND)**2\n## [1] 0.6596678\ncor(pred2, test$ABUND)**2\n## [1] 0.1983452\n\nRF clearly outperforms the linear regression model!"
  },
  {
    "objectID": "6A-Exercise.html#titantic-dataset",
    "href": "6A-Exercise.html#titantic-dataset",
    "title": "12  Exercise",
    "section": "12.2 titantic dataset",
    "text": "12.2 titantic dataset\n\nlibrary(EcoData)\nlibrary(randomForest)\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following object is masked from 'package:randomForest':\n## \n##     combine\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nset.seed(42)\ntitanic_sub = titanic %&gt;% select(survived, age, pclass, sex, fare)\ntitanic_sub = titanic_sub[complete.cases(titanic_sub),]\n\nindices = sample.int(nrow(titanic_sub), 200)\ntrain = titanic_sub[-indices,]\ntest = titanic_sub[indices,]\n\n\n\n\n\n\n\nWarning\n\n\n\nTask:\n\nFit random forest on train data\nPredict for test data\nCalculate Accuracy\nDo the same with a glm (binomial) and compare the predictive performance of both models\nWhat is the most important variable?\n\n\n\n\n\nClick here to see the solution\n\n\nrf = randomForest(as.factor(survived)~., data = train)\nm = glm(survived~., data = train, family = binomial)\n\npred1 = predict(rf, newdata = test)\npred2 = predict(m, newdata = test, type = \"response\")\n\n# pred2 are probabilities, we have to change them to levels\npred2 = ifelse(pred2 &lt; 0.5, 0, 1)\n\nmean(pred1 == test$survived) # RF\n## [1] 0.82\nmean(pred2 == test$survived) # glm\n## [1] 0.765\n\nRF is better than the glm!\n\nvarImpPlot(rf)\n\n\n\n\nSex is the most important variable!"
  },
  {
    "objectID": "6A-Exercise.html#bias-variance-tradeoff",
    "href": "6A-Exercise.html#bias-variance-tradeoff",
    "title": "12  Exercise",
    "section": "12.3 Bias-variance tradeoff",
    "text": "12.3 Bias-variance tradeoff\nAn important concept of statistics and, in particular, ML is the concept of the bias-variance tradeoff - or in other words, finding the right complexity of the model. So how flexible should our model be so that it generalizes well to other/new observations. Many ML algorithms have complexity parameters (e.g. nodesize or mtry in RF) that control their complexity. Have a look at the following youtube video about the bias-variance tradeoff:\n\nLet’s see how we can control the complexity in the Random Forest algorithm:\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\nrf = randomForest(Ozone~., data = data)\n\npred = predict(rf, data)\nimportance(rf)\n##         IncNodePurity\n## Solar.R      17969.59\n## Wind         31978.36\n## Temp         34176.71\n## Month        10753.73\n## Day          15436.47\n#&gt;         IncNodePurity\n#&gt; Solar.R      17969.59\n#&gt; Wind         31978.36\n#&gt; Temp         34176.71\n#&gt; Month        10753.73\n#&gt; Day          15436.47\ncat(\"RMSE: \", sqrt(mean((data$Ozone - pred)^2)), \"\\n\")\n## RMSE:  9.507848\n#&gt; RMSE:  9.507848\n\nplot(data$Temp, data$Ozone)\nlines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n\n\n\n\n\n\n\n\n\n\nTask\n\n\n\nTry different values for the nodesize and mtry and describe how the predictions depend on these parameters. (randomForest(..., nodesize = ..., mtry = ...) (the exercise was taken from the ML course book)\n\n\n\n\nClick here to see the solution\n\n\nlibrary(randomForest)\nset.seed(123)\n\ndata = airquality[complete.cases(airquality),]\n\n\nfor(nodesize in c(1, 5, 15, 50, 100)){\n  for(mtry in c(1, 3, 5)){\n    rf = randomForest(Ozone~., data = data, mtry = mtry, nodesize = nodesize)\n    \n    pred = predict(rf, data)\n    \n    plot(data$Temp, data$Ozone, main = paste0(\n        \"mtry: \", mtry, \"    nodesize: \", nodesize,\n        \"\\nRMSE: \", round(sqrt(mean((data$Ozone - pred)^2)), 2)\n      )\n    )\n    lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = \"red\")\n  }\n}"
  },
  {
    "objectID": "6A-Exercise.html#ml-pipeline",
    "href": "6A-Exercise.html#ml-pipeline",
    "title": "12  Exercise",
    "section": "12.5 ML pipeline",
    "text": "12.5 ML pipeline\nIf you want to know more about a typical ML pipeline/workflow, read this chapter from the ML course!"
  },
  {
    "objectID": "6A-Exercise.html#deep-neural-networks",
    "href": "6A-Exercise.html#deep-neural-networks",
    "title": "12  Exercise",
    "section": "12.4 Deep Neural Networks",
    "text": "12.4 Deep Neural Networks\nTask description: Predict the spatial distribution of the African elephant. In Ecology we call such model a species distribution model.\n\nlibrary(EcoData)\n?elephant\n\nThe object elephant contains two subdatasets\n\nelephant$occurenceData contains presence / absence data as well as bioclim variables (environmental predictors) for the African elephant\nelephant$predictionData data with environmental predictors for spatial predictions\n\nThe environmental data consists of 19 environmental variables, called bio1 through bio19, which are public and globally available bioclimatic variables (see https://www.worldclim.org/data/bioclim.html for a description of the variables). For example, bio1 is the mean annual temperature. No understanding of these variables is required for the task, the only difficulty is that many of them are highly correlated because they encode similar information (e.g. there are several temperature variables).\nThe goal of this exercise is to fit a deep neural network based on the observed presence / absences, and then make new predictions of habitat suitability in space across Africa based on the fitted model. Thus, our workflow consists of two steps:\n\nbuilding and optimizing the predictive model, and\nusing the predictive model to make predictions on new data and visualizing the results.\n\nHere an example of how you could do this\nBuild predictive model:\n\n# Use subsample of data because too many observations and use the rest of data to validate our model\ntrain_indices = sample.int(nrow(elephant$occurenceData), 500)\ndf = elephant$occurenceData[train_indices, ]\n\nlibrary(cito)\nmodel = dnn(Presence~bio1, \n            data = df, \n            loss = 'binomial', \n            verbose = FALSE)\n\n\n\nplot(model)\n\n\n\n\nTo check the predictive power of the model for the observations we have not used to train the model ([-train_indices,])\n\nlibrary(pROC)\nauc(df$Presence[-train_indices], \n    predict(model, newdata = df[-train_indices,],type = \"response\"))\n## Warning in roc.default(response, predictor, auc = TRUE, ...): Deprecated use a\n## matrix as predictor. Unexpected results may be produced, please pass a numeric\n## vector.\n## Area under the curve: 0.7117\n\nThe AUC is a common measure of goodness of fit for binary classification.\n\n\n\n\n\n\nTasks\n\n\n\n\nDrop some of the highly correlated variables (don’t use all of them).\nChange architecture of the dnn (using the hidden=c(...)argument)\nChange the number of epochs and the learning rate (see documentation of dnn)\n\nMake new predictions\nThe data for making spatial predictions is in elephant$predictionData. This new dataset is not a data.frame but a raster object, which is a special data class for spatial data. You can plot one of the predictors in the following way.\n\nlibrary(sp)\nlibrary(raster)\nplot(elephant$predictionData$bio1)\n\n\n\n\nAs our new_data object is not a typical data.frame, we are not using the standard predict function for a dnn, which is ?predict.citodnn, but the predict function from the raster object (which internally transforms the new_data into a classical data.frame, pass then the data.frame to our model, and then transforms the output back to a raster object). Therefore, the syntax is slightly different to how we previously used predict().\n\npredictions =  predict(elephant$predictionData, model = model, type = \"response\")\nhead(as.data.frame(predictions))\n##        layer\n## 1 0.04989933\n## 2 0.04631172\n## 3 0.04300154\n## 4 0.04146853\n## 5 0.03866339\n## 6 0.03732348\n\nThe advantage of the raster object is that we can directly use it to create a map (the raster object has coordinates for each observation):\n\nspplot(predictions, colorkey = list(space = \"left\") )\n\n\n\n\nTask: play around with the DNN to improve predictive accuracy. You can check predictive accuracy by looking the AUC of the test data. When improving the predictive power of the model, does the map change?"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Sample, population, and the data-generating process\nThe very reason for doing statistics is that the data that we observe is somehow random. But how does this randomness arise?\nImagine that we are interested in the average growth rate of trees in Germany during two consecutive years. Ideally, we would measure them all and be done, without having to do statistics. In practice, however, we hardly ever have the resources to do so. We therefore have to make a selection of trees, and infer the growth rate of all trees from that. The statistical term for all the trees is the “population”, and the term for the trees that you have observed is the “sample”. Hence, we want to infer properties of the population from a sample.\nThe population as such is fixed and does not change, but every time we observe a random selection (sample) of the population, we may get elements with slightly different properties. As a concrete example: imagine we have the resources to only sample 1000 trees across Germany. Thus, every time we take a random selection of 1000 trees out of the population, we will get a slightly different average growth rate.\nThe process of sampling from the population does explain how randomness arises in our data. However, a slight issue with this concept is that it does not match very well with more complex random processes. Imagine, for example, that data arises from a person going to randomly selected plots to measure radiation (which varies within minutes due to cloud cover), using a measurement instrument that measures with some random error. Does it really make sense to think of the data arising from sampling from a “population” of possible observations?\nA more modern and general concept to describe how data is created is the concept of the “data-generating process”, which is pretty self-explanatory: the data-generating process describes how the observations from a random sample arise, including systematic and stochastic processes. It therefore includes the properties of what would classically be called “sampling from a population”, but it is broader and includes all other processes that would create systematic and random patterns in our data. In this picture, instead of inferring properties of the population from a sample, we would say we want to infer the properties of the data-generating process from a sample of observations created by this process.\nWhether you think in populations or data-generating processes: the important point to remember from this section is that there are two objects that we have to distinguish well: on the one hand, there is our sample. We may describe it in terms of it’s properties (mean, minimum, maximum), but the sample is not the final goal. Ultimately, we want to infer the properties of the population / data-generating process from the sample. We will explain how to do this in the next sections, in particular in the section on inferential statistics. Before we come to that, however, let us talk in a bit more detail about the representation of the sample, i.e. the data that we observe.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Introduction.html#sample-population-and-the-data-generating-process",
    "href": "Introduction.html#sample-population-and-the-data-generating-process",
    "title": "Introduction",
    "section": "",
    "text": "The population is the set of all observations that you could have made. The sample is the observations that you have actually made.\n\n\n\nSampling creates randomness.\n\n\n\nHowever, not all randomness comes from sampling from a population.\n\n\n\nA more modern concept that replaces the “population” is the “data-generating process”. The data-generating process describes how the observations from a random sample arise, including systematic and stochastic processes.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Introduction.html#representation-and-classes-of-data",
    "href": "Introduction.html#representation-and-classes-of-data",
    "title": "Introduction",
    "section": "Representation and classes of data",
    "text": "Representation and classes of data\nA typical dataset consists of multiple observations of a number of variables (e.g. temperature, precipitation, growth). You can think of this situation as a spreadsheet where the columns are the variables and the rows are the observations. Of course, there are other data structures, but this is the most common one (also known as Tabular Data).\nUsually, this data will contain one variable that is our focus, meaning that we want to understand how this variable is influenced by other variables.\n\n\nThe response variable is the variable for which we try to understand how it responds to other factors.\nWe call this variable the response variable (sometimes also the dependent variable or outcome variable), because we are interested if and how this variable of interest varies (responds, depends) when something else changes. The variables that affect the response could be environmental factors (e.g. temperature), treatments (fertilized vs. non fertilized), or anything else.\n\n\nThe predictor variables are those that affect the response.\nThose other variables that affect our response are called predictor variables (synonymous terms are explanatory variables, covariates or independent variables).\nThe most common case is that the response variable is a single variable (e.g. a single number or a categorical outcome), and we will concentrate on this case. However, there are cases when the response has more than one dimension, or when we are interested in the change of several response variables at a time. The analysis of such data is known as multivariate statistics. We will not cover these methods here; find some further links here.\n\n\nMultivariate statistics deal with response variables that have several dimensions, such as species composition.\nAnother important distinction is the type of each variables independent of whether we are speaking about the response or the predictor, we distinguish:\n\n\nVariables can be continuous, discrete or categorical. Categorical variables can be ordered, unordered, or binary.\n\nContinuous numeric variables (ordered and continuous / real), e.g. temperature\nInteger numeric variables (ordered, integer). An important special case of those are count data, i.e. 0,1,2,3, …\nCategorical variables (e.g. a fixed set of options such as red, green blue), which can further be divided into\n\nUnordered categorical variables (Nominal) such as red, green, blue\nBinary (Dichotomous) variables (dead / survived, 0/1)\nOrdered categorical variables (small, medium, large)\n\n\nIt is important that you record the variables according to their nature. And if you use a statistics software, you have to make sure that the type is properly recognized after reading in the data, because many methods treat a variable differently if it is numeric or categorical.\n\n\nCheck that your variables have the right type after reading them in in your statistics software.\nExperience shows that there is certain tendency of beginners to use categorical variables for things that are actually continuous, e.g. by coding body weight of animals into light, medium, heavy.\n\n\n\n\n\n\nDon’t use categorical variables for things that can also be recorded numerically! The justification stated is often that this avoids the measurement uncertainty. In short: it doesn’t, it just creates more problems. Don’t use categorical variables for things that can also be recorded numerically!\n\n\n\nLet’s come back to one of the first point in this script: the data. If we have to collect data ourselves, we have to answer a number of questions. Which variables should we collect? At which values of those variables should we collect data? And how many replicates do we need?",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Introduction.html#selection-of-variables",
    "href": "Introduction.html#selection-of-variables",
    "title": "Introduction",
    "section": "Selection of variables",
    "text": "Selection of variables\nIn a practical setting, we are typically interested in how a response is affected by a number of predictor variables. Clearly, we need to measure both response and this predictors of interest across a few of those predictor values to say something about the effect of the predictors. If we only wanted to know whether there is a correlation between predictors and response, our list of variables would be complete at this point. However, typically, we want to know not only if there is a correlation, but also whether we can say with some confidence that this correlation is causal. If we want to make this claim, we have to exclude that there are confounding factors, also called confounding variables.\n\n\nCorrelation is not causality. For suggesting causality, we have to exclude confounding effects.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Introduction.html#what-is-a-confounding-variable",
    "href": "Introduction.html#what-is-a-confounding-variable",
    "title": "Introduction",
    "section": "What is a confounding variable?",
    "text": "What is a confounding variable?\nImagine we are interested in a response A, and we have hypothesized that \\(A~B\\). Imagine there is a second predictor variable C that has an influence on A, but in which we are not interested in for the purpose of the question under consideration. Such a variable that is not of interest for the question is also called “extraneous variables”. So we also have A~C, but we are not interested in this relationship. If we now take data, and don’t measure C, it’s usually not a bit problem as long as C is uncorrelated with B - it might create a bit more variability in the response, but by and large the effect of C should average out and we should still be able to detect the effect of B.\n\n\nAn extraneous variable is a variable that can influence the response, but is not of interest for the experimenter\n\n\n\nDiagram illustrating a confounding variable. A key requirement for being a confounded is that the variable correlates both with the response, and with the predictor variables that form our original hypothesis. If the second link is not there, the variable is only extraneous and not confounding. If that is the case, disregarding it has usually only minor consequences on the statistical interpretation.\n\n\nThe problem of confounding appears when the extraneous variable C is for some reason correlated with the predictor variable of interest B. In that case, if we only measure B, we see both the effect of B and C. In this case, we may attribute the effect of C on A wrongly to the effect of B on A.\n\n\nA confounding variable is an extraneous variable that correlated to both the response and a predictor variable of interest.\n\nA spurious correlation is a correlation that is caused by a confounding variable.\n\nA correlation that is caused by an unmeasured confounding variable is called a spurious correlation.\nDealing with confounders is a central part of this course.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1-GettingStarted.html",
    "href": "1-GettingStarted.html",
    "title": "1  Getting Started with R",
    "section": "",
    "text": "1.1 Your R System\nIn this course, we work with the combination of R + RStudio.\nMake sure you have a recent version of R + RStudio installed on your computer. I think this page covers all possible questions about how to install this combination on the different OS.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "1-GettingStarted.html#your-r-system",
    "href": "1-GettingStarted.html#your-r-system",
    "title": "1  Getting Started with R",
    "section": "",
    "text": "R is the calculation engine that performs the computations.\nRStudio is the editor that helps you sending inputs to R and collect outputs.\n\n\n\n\nAlternatively, you could also use the Rstudio Cloud, however, we recommend local RStudio setup.\n\n1.1.1 First steps in R and Rstudio\nIf you have never used RStudio, here is a good video introducing the basic system and how R and RStudio interact. Alternatively, here is a video about R for Ecologists. The two videos give you a first orientation about the principles of working with the two programs:",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "1-GettingStarted.html#libraries-that-you-will-need",
    "href": "1-GettingStarted.html#libraries-that-you-will-need",
    "title": "1  Getting Started with R",
    "section": "1.2 Libraries that you will need",
    "text": "1.2 Libraries that you will need\nThe R engine comes with a number of base functions, but one of the great things about R is that you can extend these base functions by libraries that can be programmed by anyone. In principle, you can install libraries from any website or file. In practice, however, most commonly used libraries are distributed via two major repositories. For statistical methods, this is CRAN, and for bioinformatics, this is Bioconductor.\n\n\n\n\n\n\nClick to see more on installing libraries in R\n\n\n\n\n\nTo install a package from a library, use the command\n\ninstall.packages(LIBRARY)\n\nExchange “LIBRARY” with the name of the library you want to install. The default is to search the package in CRAN, but you can specify other repositories or file locations in the function. For Windows / Mac, R should work out of the box. For other UNIX based systems, may also need to install\nbuild-essential\ngfortran\nlibmagick++-dev\nr-base-dev\ncmake\nIf you are new to installing packages on Debian / Ubuntu, etc., type the following:\nsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev cmake\n\n\n\nIn this book, we will often use data sets from the EcoData package, which is not on CRAN, but on a GitHub page. To install the package, if you don’t have the devtools package installed already, first install devtools from CRAN by running\n\ninstall.packages(\"devtools\")\n\nThen install the EcoData package via\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\",\n                         dependencies = T, build_vignettes = T)\n\nFor your convenience, the EcoData installation also forces the installation of most of the packages needed in this book, so this may take a while. If you want to load only the EcoData package, or if you encounter problems during the install, set dependencies = F, build_vignettes = F.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "1-GettingStarted.html#sec-datamanipulation",
    "href": "1-GettingStarted.html#sec-datamanipulation",
    "title": "1  Getting Started with R",
    "section": "1.3 Data manipulation in R",
    "text": "1.3 Data manipulation in R\nR works like a calculator:\n\n2+2\n## [1] 4\n5*4\n## [1] 20\n2^2\n## [1] 4\n\nWe can also use functions that perform specific calculations:\n\nsqrt(4)\n## [1] 2\nsum(c(2,2))\n## [1] 4\n\n\n\nc(...) is a function to create a vector of scalar values (single values). Most functions in R can work on scalars, vectors, and even sometimes on matrices (two-dimensional data structures).\nWe can assign values/data to variables:\n\nobject.name &lt;- 1\n\nNote that both operators ‘&lt;-’ or “=” work. Functions in R (e.g. sum(), mean(), etc.) have arguments that control/change their behavior and are also used to pass the data to the function:\n\nmean(x = c(2, 2))\n## [1] 2\n\n\n\nA list and description of all arguments can be found in the help of a function (which can be accessed via ?mean or if you place the cursor on the function and press F1)\n\n1.3.1 Data types and data structures\nThere are four important data types in R (there are more but we focus on these 5):\n\nNumeric: 1, 2, 3, 4\nLogical: TRUE or FALSE\nCharacters: “A”, “B”,…\nFactors which are characters but we have to tell R explicitly that they are factors\nNot a number: NA, NaN (empty value)\n\nBased on the data types we can build data structures which contain either only specific data types or a mixture of data types:\n\nVector: Several values of one data type, can be created with the c function:\n\nc(5, 3, 5, 6) # numeric vector\nc(TRUE, TRUE, FALSE, TRUE) # logical vector\nc(\"A\", \"B\", \"C\") # character vector\nas.factor(c(\"A\", \"B\", \"C\")) # factor vector\n\nMatrix: two-dimensional data structure of one data type, can be created with the matrix function (we can pass a vector to the matrix function and tell it via arguments how the matrix should be constructed):\n\nmatrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2)\n##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n\nData.frame: Often our data has variables of different types which makes a matrix unsuitable data structure. Data.frames can handle different data types and is organized in columns (one column = one variables) and can be created with the data.frame function:\n\ndata.frame(A = c(1, 2, 3), B = c(\"A\", \"B\", \"C\"), C = c(TRUE, FALSE, FALSE))\n##   A B     C\n## 1 1 A  TRUE\n## 2 2 B FALSE\n## 3 3 C FALSE\n\n\n\n\n1.3.2 Data manipulation\nA vector is a one dimensional data structure and we can access the values by using [ ]:\n\nvec = c(1, 2, 3, 4, 5)\nvec[1] # access first element\n## [1] 1\nvec[5] # access last element\n## [1] 5\n\nA data.frame is a two dimensional data structure. Let’s define a data.frame from two vectors:\n\ndf = data.frame(\n  x = c(2,2,2,3,2,2,1), #add column named x with 2 elements\n  y = c(4,5,5,4,5,3,5) #add a second column named y\n)\n#Let's see how this looks like:\ndf\n##   x y\n## 1 2 4\n## 2 2 5\n## 3 2 5\n## 4 3 4\n## 5 2 5\n## 6 2 3\n## 7 1 5\n\nAccess parts of the data.frame:\n\ndf[1,2] #get element in row 1, column 1\n## [1] 4\ndf[7,1] #get element in row 7, column 1\n## [1] 1\ndf[2,] #get row 2\n##   x y\n## 2 2 5\ndf[,2] #get column 2\n## [1] 4 5 5 4 5 3 5\n#or use the $ sign to access columns:\ndf$y\n## [1] 4 5 5 4 5 3 5\ndf[2:4,1:2] #get rows 2 to 4 and only columns 1 and 2\n##   x y\n## 2 2 5\n## 3 2 5\n## 4 3 4\n\nWe can also set filters:\n\ndf[df$x &gt; 2,] # show only data where x is larger than 2\n##   x y\n## 4 3 4\ndf[df$y == 5,] #show only data where y equals 5\n##   x y\n## 2 2 5\n## 3 2 5\n## 5 2 5\n## 7 1 5\ndf[df$y == 5 & df$x == 1,] #show only data where y equals 5 AND x equals 1\n##   x y\n## 7 1 5\ndf[df$y == 5 | df$x == 3,] #show data where y equals 5 OR x equals 3\n##   x y\n## 2 2 5\n## 3 2 5\n## 4 3 4\n## 5 2 5\n## 7 1 5\n\n\n\n\n\n\n\nLogical operators\n\n\n\n\n\n\nLogical operators in R\n\n\nOperators\nMeaning\n\n\n\n\n&lt;\nLess than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;\nMore than\n\n\n&gt;=\nMore than or equal to\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n!a\nNot a\n\n\na|b\na or b\n\n\na & b\na and b\n\n\nisTRUE(a)\nTest if a is true\n\n\n\n\n\n\nAdd an additional column with NA values:\n\ndf$NAs = NA #fills up a new column named NAs with all NA values\ndf\n##   x y NAs\n## 1 2 4  NA\n## 2 2 5  NA\n## 3 2 5  NA\n## 4 3 4  NA\n## 5 2 5  NA\n## 6 2 3  NA\n## 7 1 5  NA\n\n\n\n1.3.3 Data analysis workflow\nThis is a simple version of what you’re going to learn during this course:\n\nLet’s say we measured the size of individuals in two different treatment groups\n\ngroup1 = c(2,2,2,3,2,2,1.1)\ngroup2 = c(4,5,5,4,5,3,5.1) \n\nclass(group2)\n## [1] \"numeric\"\n\nDescriptive statistics and visualization\n\nmean(group1)\n## [1] 2.014286\nmean(group2)\n## [1] 4.442857\n\nboxplot(group1, group2)\n\n\n\n\n\n\n\n\nTesting for differences. Question: Is there a difference between group1 and group2?\n\nt.test(group1, group2)\n## \n##  Welch Two Sample t-test\n## \n## data:  group1 and group2\n## t = -6.6239, df = 10.628, p-value = 4.413e-05\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.238992 -1.618151\n## sample estimates:\n## mean of x mean of y \n##  2.014286  4.442857\n\nInterpretation of the results. Individuals in Group 2 were larger than those in group 1 (t test, t = -6.62, p &lt; 0.0001)\n\nIn the course we will work a lot with datasets implemented in R or in R packages which can be accessed via their name:\n\ndat = airquality\nhead(dat)\n##   Ozone Solar.R Wind Temp Month Day\n## 1    41     190  7.4   67     5   1\n## 2    36     118  8.0   72     5   2\n## 3    12     149 12.6   74     5   3\n## 4    18     313 11.5   62     5   4\n## 5    NA      NA 14.3   56     5   5\n## 6    28      NA 14.9   66     5   6",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "9-Design.html",
    "href": "9-Design.html",
    "title": "14  Design of experiments",
    "section": "",
    "text": "14.1 Question\nIn the beginning, there is the question. Let assume for simplicity that we want to know if y depends on x\n\\[\ny \\sim x  \n\\]\nTo answer this question, we plan to vary x and then take measurements of y.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Design of experiments</span>"
    ]
  },
  {
    "objectID": "9-Design.html#quality-of-measurements",
    "href": "9-Design.html#quality-of-measurements",
    "title": "14  Design of experiments",
    "section": "14.2 Quality of measurements",
    "text": "14.2 Quality of measurements\nThe first question to ask in this context is if my measurements are reliable. In particular, I may want to know if\n\n\nThe consideration of these two questions is often referred to as construct validity.\n\nDoes my measurement process really target the quantity that I want to measure, or is it only a proxy?\nWhat is the expected statistical (stochastic) error in my measurements, and what is the possible systematic error in my measurements\n\nThe first item may seem a bit odd, because one would think that we know what we measure. However, in many cases in ecological statistics and beyond, we do not measure directly the variable that we are interested in, but rather a proxy. So, for example, we want temperature on the plot, and we use temperature from a weather station 5 km away. Or, we want to look at functional diversity, but how can we exactly express this in terms of variables that we measure in the field.\nThe second questions relates to considering how much two measurements would differ if we do them repeatedly (stochastic), and how much measurements could be off systematically (e.g. because a method or instrument is systematically wrong, or because humans show particular biases). The values can often be read of instruments etc.\n\n\n\n\n\n\nDo not categorize continuous variables\n\n\n\nA common problem in measuring variables is the inappropriate categorization of continuous variables. The motivation for this is often measurement error. As an example, imagine we have to means to exactly estimate the weight of subjects, and therefore want to write down an estimate. Experimentators then often note down weight as low, middle, high, with the idea that a more exact assessment is not possible. This is almost always bad practice. The reason is the following: even if you have a high measurement error on the variable, the error is NEVER reduced by working with categories. Rather, we increase the error and reduce signal by the categorization. If we want to put people in weight categories, we can still do this later.\n\n\nWhen designing your experiments, try to find the best solution to minimize both problems before going on in the process.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Design of experiments</span>"
    ]
  },
  {
    "objectID": "9-Design.html#selection-of-treatments-levels-for-the-independent-predictor-variables",
    "href": "9-Design.html#selection-of-treatments-levels-for-the-independent-predictor-variables",
    "title": "14  Design of experiments",
    "section": "14.3 Selection of treatments levels for the independent (predictor) variables",
    "text": "14.3 Selection of treatments levels for the independent (predictor) variables\nIn an experimental study, we usually vary predictor variables systematically for a particular entity, e.g. a plant, a pot or a plot. This entity is called the experimental unit. Also observational studies have experimental units (the entities for which measurements are taken), but it usually not possible to completely control the variables. However, one usually has the option to make particular selections. Also in observational studies, it is key to ensure sufficient sufficient variation of the predictor variables across the experimental units to allow a meaningful statistical analysis.\n\n\nThe experimental unit is the entity that can be assigned a particular variable combination (e.g. treatment or control). Example: an individual plant, or a pot.\nWhen deciding on which treatments to describe, here a few things that you want to consider:\n\n14.3.1 Vary all variables independently\nA common problem in practice is that we have two variables, but their values change in a correlated way. Imagine we test for the presence of a species, but we have only warm dry and cold wet sites. We say the two variables a collinear. In this case we don’t know whether any observed effect is due to temperature or water availability. The bottom-line: if you want to separate two effects, the correlation between them must not be perfect - ideally, it would be zero, or failing that, as low as possible.\nIn an experiment, you should try to avoid such correlations by all means, i.e. set your treatments such that variables are not correlated!\n\n\n14.3.2 Interactions\nTo be able to detect interactions between variables, it’s not enough to vary all, you also need to have certain combinations. The buzzword here is (fractional) factorial designs. Basically, if you have two predictor variables, and you want an interaction, you need at least 4 treatments for the two variables: low low, low high, high low and high high. This is called a full factorial design.\nFor more complicated situations, R or other software packages can help you to find appropriate statistical designs\n\nlibrary(designr)\n\ndesign1 &lt;- \n  fixed.factor(\"Age\", levels=c(\"young\", \"old\")) +\n  fixed.factor(\"Material\",  levels=c(\"word\", \"image\"))\ndesign1\n## Factor design with 2 factor(s):\n##  - Fixed factor `Age` with 2 level(s) (young, old) and 1 replication(s)\n##  - Fixed factor `Material` with 2 level(s) (word, image) and 1 replication(s)\n## \n## Design matrix with 4 planned observations:\n## # A tibble: 4 × 2\n##   Age   Material\n##   &lt;fct&gt; &lt;fct&gt;   \n## 1 young word    \n## 2 old   word    \n## 3 young image   \n## 4 old   image\n\n\n\n14.3.3 Nonlinear effects\nThe connection of two points is a line. If you want to see whether the response to a variable is nonlinear, you therefore need more than two values of each variable. Common recommendation for a categorical treatment (with the goal of later running an ANOVA) is to take 4, because with 3 you often do not get a good idea about the shape. An alternative is to treat the variable as continuous, in which case you can just spread your points evenly across the range of the predictor.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Design of experiments</span>"
    ]
  },
  {
    "objectID": "9-Design.html#what-do-do-about-confounding-variables",
    "href": "9-Design.html#what-do-do-about-confounding-variables",
    "title": "14  Design of experiments",
    "section": "14.4 What do do about confounding variables",
    "text": "14.4 What do do about confounding variables\nIf we think there is a factor that could be confounding, we basically have three options\n\nBest: control the value of these factors. Either fix the value (preferred if we are not interested in this factor), else vary the value in a controlled way (see below).\nSecond best: randomize and measure them\nThird best: only randomize or only measure them\n\nRandomization means that we try to ensure that the confounding factor is not systematically correlated with the variable of interest (but can still cause problems with interactions and nonlinear relationships).\nMeasuring allows us to account for the effect in a statistical analysis, but cost power (see below) and, and we can’t measure everything.\n\n\nVariables that we include but that are not interesting to us are often called nuisance variables.\n\n\n\nIllustration of a randomized block design, the probably most widely used design on (observational) experiments to randomize the effect of unknown and unmeasured confounding variables. The idea of this design is that the unknown variables are likely correlated in space. By blocking all experimentally changing variables together, we avoid that they can become confounded with the unknown spatial variables.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Design of experiments</span>"
    ]
  },
  {
    "objectID": "9-Design.html#how-many-replicates",
    "href": "9-Design.html#how-many-replicates",
    "title": "14  Design of experiments",
    "section": "14.5 How many replicates?",
    "text": "14.5 How many replicates?\nWe said before that the significance level \\(\\alpha\\) is the probability of finding false positives. This is called the type I error. There is another error we can make: failing to find significance for a true effect. This is called the type II error, and the probability of finding an effect is called power.\n\n\nPower is the probability of finding significance for an effect if it’s there.\nFor standard statistical methods, power can be calculated. You have to look it up for your particular method, but in general assume that\n\nPower goes up with increasing effect size\nPower goes down with increasing variability in the response\n\nThis means that, unlike for the type I error which is fixed, calculation of power requires knowledge about the expected effect and the variability. This sounds really bad, but in most cases you can estimate from previous experience how much variation there will be, and in most cases you also know how big the effect has to be at least to be interesting. Based on that, you can then calculate how many samples you need.",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Design of experiments</span>"
    ]
  },
  {
    "objectID": "9-Design.html#check-list-experimental-design",
    "href": "9-Design.html#check-list-experimental-design",
    "title": "14  Design of experiments",
    "section": "14.6 Check List experimental design",
    "text": "14.6 Check List experimental design\nFor each experiment, ask the following questions:\n\nClear, logically consistent question? Write it down. Read chapter about valid / good scientific questions in the lecture notes\nMake sure you have read and considered all the issues of validity discussed in the main lecture notes. Go through the checklist validity at the end of the section in the main lecture notes.\nDraft a design\n\nVary the variables that you need to measure to answer your questions. Decide if you are interested in main linear effects, or also nonlinear effects or interactions.\nWrite down potential confounding variables. Decide if they are better controlled, randomized or measured? Are you sure they are confounding (correlated to response AND one or several of the predictors)\nDefine the statistical hypothesis to be tested, including confounders. Write it down, as in \\(height \\sim age + soil * precipitation + precipitation^2\\).\nChoose how the variables will be varied in the experiment. Consider using software for this, e.g. for fractional factorial designs (in observational studies, you sometimes have limited control, but you can maybe estimate what variable combinations you will observe).\nBlocking - try to group different treatments / most different variable combinations together. The aim is that unknown / unmeasured variables are not correlated with your experimental variables (see pseudo-replication)\nDecide on the number of replicates. Make a guess for effect size and variability of the data, and either calculate or guess the number of replicates necessary to get sufficient power. What sufficient means depends on the field, but I would say you want to have a good chance to see an effect if it’s there, so a power of \\(&gt;80\\%\\) would be good.\n\nCheck design\n\nPlay through the processes of collecting your data: simulate it in your mind or in R, make up some data, write it down. Everything seems OK?\nPlay through the process of analyzing your data. Which method? Can you answer your question? Do a power analysis!\n\nRevise if necessary",
    "crumbs": [
      "Experimental Design",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Design of experiments</span>"
    ]
  },
  {
    "objectID": "1A-SettingUpTheRStudionEnvironment.html",
    "href": "1A-SettingUpTheRStudionEnvironment.html",
    "title": "2  Basic data operations with R",
    "section": "",
    "text": "2.1 Data manipulation in R\nR works like a calculator:\n2+2\n## [1] 4\n5*4\n## [1] 20\n2^2\n## [1] 4\nWe can also use functions that perform specific calculations:\nsqrt(4)\n## [1] 2\nsum(c(2,2))\n## [1] 4\nWe can assign values/data to variables:\nobject.name &lt;- 1\nNote that both operators ‘&lt;-’ or “=” work. Functions in R (e.g. sum(), mean(), etc.) have arguments that control/change their behavior and are also used to pass the data to the function:\nmean(x = c(2, 2))\n## [1] 2",
    "crumbs": [
      "Data input and manimulation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic data operations with R</span>"
    ]
  },
  {
    "objectID": "1A-SettingUpTheRStudionEnvironment.html#data",
    "href": "1A-SettingUpTheRStudionEnvironment.html#data",
    "title": "2  Setting up your R Environment",
    "section": "",
    "text": "Dependent / response variable variable of interest we want to know what it is influenced by. Typically, there is one variable of particular interest.\nExplanatory or independent variables, aka. predictors / covariates / treatments variables that potentially influence the response variable\n\n\n\n\n\n\n\nTip\n\n\n\nExample: We measure plant growth and nitrogen in the soil. Growth is the dependent variable and nitrogen is the explanatory variable\n\n\n\n\nScale of measure: nominal (unordered), ordinal (ordered), metric (differences can be interpreted)\nNumeric or metric variables Examples: Body size, conductivity\nNon-numeric = categorial variables\n\nunordered / nominal (red, green, blue) ordered /\nordinal (tiny, small, large) Attention: categories small = 1m, medium = 1.5m, large = 2m would be interpreted as numeric, because their relative differences are defined\nSpecial case: binary variable (0,1), technically it is also categorial, but with two levels only we call in binary\n\n\n\n\n\n\n\n\nRemarks on data handling\n\n\n\n\n\nTypically, data will be recorded electronically with a measurement device, or you have to enter it manually using a spreadsheet program, e.g. MS Excel. The best format for data storage is csv (comma separated values) because it is long-term compatibility with all kinds of programs / systems (Excel can export to csv).\nAfter raw data is entered, it should never be manipulated by hand! If you modify data by hand, make a copy and document all changes (additional text file). Better: Make changes using a script\nData handling in R:\n\ncreate R script “dataprep.R” or similar and import dataset\npossibly combine different datasets\nclean data (remove NAs, impossible values etc.)\nsave as Rdata (derived data)",
    "crumbs": [
      "Data input and manimulation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setting up your R Environment</span>"
    ]
  },
  {
    "objectID": "1A-SettingUpTheRStudionEnvironment.html#data-manipulation-in-r",
    "href": "1A-SettingUpTheRStudionEnvironment.html#data-manipulation-in-r",
    "title": "2  Basic data operations with R",
    "section": "",
    "text": "Help\n\n\n\nA list and description of all arguments can be found in the help of a function (which can be accessed via ?mean or if you place the cursor on the function and press F1)\n\n\n\n2.1.1 Data types and data structures\nThere are four important data types in R (there are more but we focus on these 5):\n\nNumeric: 1, 2, 3, 4\nLogical: TRUE or FALSE\nCharacters: “A”, “B”,…\nFactors which are characters but we have to tell R explicitly that they are factors\nNot a number: NA, NaN (empty value)\n\nBased on the data types we can build data structures which contain either only specific data types or a mixture of data types:\n\nVector: Several values of one data type, can be created with the c function:\n\nc(5, 3, 5, 6) # numeric vector\nc(TRUE, TRUE, FALSE, TRUE) # logical vector\nc(\"A\", \"B\", \"C\") # character vector\nas.factor(c(\"A\", \"B\", \"C\")) # factor vector\n\nMatrix: two-dimensional data structure of one data type, can be created with the matrix function (we can pass a vector to the matrix function and tell it via arguments how the matrix should be constructed):\n\nmatrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2)\n##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n\nData.frame: Often our data has variables of different types which makes a matrix unsuitable data structure. Data.frames can handle different data types and is organized in columns (one column = one variables) and can be created with the data.frame function:\n\ndata.frame(A = c(1, 2, 3), B = c(\"A\", \"B\", \"C\"), C = c(TRUE, FALSE, FALSE))\n##   A B     C\n## 1 1 A  TRUE\n## 2 2 B FALSE\n## 3 3 C FALSE\n\n\n\n\n2.1.2 Data manipulation\nA vector is a one dimensional data structure and we can access the values by using [ ]:\n\nvec = c(1, 2, 3, 4, 5)\nvec[1] # access first element\n## [1] 1\nvec[5] # access last element\n## [1] 5\n\nA data.frame is a two dimensional data structure. Let’s define a data.frame from two vectors:\n\ndf = data.frame(\n  x = c(2,2,2,3,2,2,1), #add column named x with 2 elements\n  y = c(4,5,5,4,5,3,5) #add a second column named y\n)\n#Let's see how this looks like:\ndf\n##   x y\n## 1 2 4\n## 2 2 5\n## 3 2 5\n## 4 3 4\n## 5 2 5\n## 6 2 3\n## 7 1 5\n\nAccess parts of the data.frame:\n\ndf[1,2] #get element in row 1, column 1\n## [1] 4\ndf[7,1] #get element in row 7, column 1\n## [1] 1\ndf[2,] #get row 2\n##   x y\n## 2 2 5\ndf[,2] #get column 2\n## [1] 4 5 5 4 5 3 5\n#or use the $ sign to access columns:\ndf$y\n## [1] 4 5 5 4 5 3 5\ndf[2:4,1:2] #get rows 2 to 4 and only columns 1 and 2\n##   x y\n## 2 2 5\n## 3 2 5\n## 4 3 4\n\nWe can also set filters:\n\ndf[df$x &gt; 2,] # show only data where x is larger than 2\n##   x y\n## 4 3 4\ndf[df$y == 5,] #show only data where y equals 5\n##   x y\n## 2 2 5\n## 3 2 5\n## 5 2 5\n## 7 1 5\ndf[df$y == 5 & df$x == 1,] #show only data where y equals 5 AND x equals 1\n##   x y\n## 7 1 5\ndf[df$y == 5 | df$x == 3,] #show data where y equals 5 OR x equals 3\n##   x y\n## 2 2 5\n## 3 2 5\n## 4 3 4\n## 5 2 5\n## 7 1 5\n\n\n\n\n\n\n\nLogical operators\n\n\n\n\n\n\nLogical operators in R\n\n\nOperators\nMeaning\n\n\n\n\n&lt;\nLess than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;\nMore than\n\n\n&gt;=\nMore than or equal to\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n!a\nNot a\n\n\na|b\na or b\n\n\na & b\na and b\n\n\nisTRUE(a)\nTest if a is true\n\n\n\n\n\n\nAdd an additional column with NA values:\n\ndf$NAs = NA #fills up a new column named NAs with all NA values\ndf\n##   x y NAs\n## 1 2 4  NA\n## 2 2 5  NA\n## 3 2 5  NA\n## 4 3 4  NA\n## 5 2 5  NA\n## 6 2 3  NA\n## 7 1 5  NA\n\n\n\n2.1.3 Data analysis workflow\nThis is a simple version of what you’re going to learn during this course:\n\nLet’s say we measured the size of individuals in two different treatment groups\n\ngroup1 = c(2,2,2,3,2,2,1.1)\ngroup2 = c(4,5,5,4,5,3,5.1) \n\nclass(group2)\n## [1] \"numeric\"\n\nDescriptive statistics and visualization\n\nmean(group1)\n## [1] 2.014286\nmean(group2)\n## [1] 4.442857\n\nboxplot(group1, group2)\n\n\n\n\n\n\n\n\nTesting for differences. Question: Is there a difference between group1 and group2?\n\nt.test(group1, group2)\n## \n##  Welch Two Sample t-test\n## \n## data:  group1 and group2\n## t = -6.6239, df = 10.628, p-value = 4.413e-05\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.238992 -1.618151\n## sample estimates:\n## mean of x mean of y \n##  2.014286  4.442857\n\nInterpretation of the results. Individuals in Group 2 were larger than those in group 1 (t test, t = -6.62, p &lt; 0.0001)\n\nIn the course we will work a lot with datasets implemented in R or in R packages which can be accessed via their name:\n\ndat = airquality\nhead(dat)\n##   Ozone Solar.R Wind Temp Month Day\n## 1    41     190  7.4   67     5   1\n## 2    36     118  8.0   72     5   2\n## 3    12     149 12.6   74     5   3\n## 4    18     313 11.5   62     5   4\n## 5    NA      NA 14.3   56     5   5\n## 6    28      NA 14.9   66     5   6",
    "crumbs": [
      "Data input and manimulation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic data operations with R</span>"
    ]
  },
  {
    "objectID": "1A-SettingUpTheRStudionEnvironment.html#exercises",
    "href": "1A-SettingUpTheRStudionEnvironment.html#exercises",
    "title": "2  Basic data operations with R",
    "section": "2.2 Exercises",
    "text": "2.2 Exercises\nIn this exercise you will practice:\n\nto set up your working environment (project) in RStudio\nto write R scripts and execute code\nto access data in dataframes (the most important data class in R)\nto query (filter) dataframes\nto spot typical mistakes in R code\n\nPlease carefully follow the instructions for setting up your working environment and ask other participants or use the forum if you face any problems.\n\n2.2.1 Setting up the working environment in RStudio\nYour first task is to open RStudio and create a new project for the course.\n\nClick the ‘File’ button in the menu, then ‘New Project’ (or the second icon in the bar below the menu “Create a project”).\nClick “New Directory”.\nClick “New Project”.\nType in the name of the directory to store your project, e.g. “IntroStatsR”.\n“Browse” to the folder on your computer where you want to have your project created.\nClick the “Create Project” button.\n\n\n\n\n\n\n\n\n\n\nFor all exercises during this week, use this project! You can open it via the file system as follows (please try this out now):\n\n(Exit RStudio).\nNavigate to the directory where you created your project.\nDouble click on the “IntroStatsR.Rproj” file in that directory.\n\nYou should now be back to RStudio in your project.\nIn the directory of the R project, generate a folder “scripts” and a folder “data”. You can do this either in the file directory or in RStudio. For the latter:\n\nGo to the “Files” panel in R Studio (bottom right panel).\nClick the icon “New Folder” in the upper left corner.\nEnter the folder name.\nThe new folder is now visible in your project directory.\n\nThe idea is that you will create an R script for each exercise and save all these files in the scripts folder. You can do this as follows:\n\nClick the “File” button in the menu, then “New File” and “R Script” (or the first icon in the bar below the menu and then “R Script” in the dropdown menu).\nClick the “File” button in the menu, then “Save” (or the “Save” icon in the menu).\nNavigate to your scripts folder.\nEnter the file name, e.g. “Exercise_01.R”.\nSave the file.\n\n\n\n2.2.2 A few hints before you can start\nRemember the different ways of running code:\n\nclick the “Run” button in the top right corner of the top left panel (code editor) OR\nhit “Ctrl”+“Enter” (MAC: “Cmd”+“Return”)\n\nRStudio will then run\n\nthe code that is currently marked OR\nthe line of code where the text cursor currently is (simply click into that line)\n\nIf you face any problems with executing the code, check the following:\n\nall brackets closed?\ncapital letters instead of small letters?\ncomma is missing?\nif RStudio shows red attention signs (next to the code line number), take it seriously\ndo you see a “+” (instead of a “&gt;”) in the console? stop executions with “esc” key and then try again.\n\nHave a look at the shortcuts by clicking “Tools” and than “Keybord Shortcuts Help”!!\n\n\n2.2.3 Getting an overview of a dataset\nWe work with the airquality dataset:\n\ndat = airquality\n\nCopy the code into your code editor and execute it.\nBefore working with a dataset, you should always get an overview of it. Helpful functions for this are:\n\nstr()\nView()\nhead() and tail()\n\nTry out these functions and provide answers to the following questions on elearning-extern (“01_Test for Exercise in R”):\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat is the most common atomic class in the airquality dataset?\nHow many rows does the dataset have?\nWhat is the last value in the column “Temp”?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWhat is the most common atomic class in the airquality dataset?\n\ninteger\nfunction str() helps to find this out\n\nHow many rows does the dataset have?\n\n153\nthis is easiest to see when using the function str(dat)\ndim(dat) or nrow(dat) give the same information\n\nWhat is the last value in the column “Temp”?\n\n68\ntail(dat) helps to find this out very fast\n\nTo see all this, run\n\ndat = airquality\nView(dat)\nstr(dat)\nhead(dat)\ntail(dat)\n\n\n\n\n\n\n2.2.4 Accessing rows and columns of a data frame\nYou have seen how you can use squared brackets [ ] and the dollar sign $ to extract parts of your data. Some people find this confusing, so let’s repeat the basic concepts:\n\nsquared brackets are used as follows: data[rowNumber, columnNumber]\nthe dollar sign helps to extract colums with their name (good for readability): data$columnName\nthis syntax can also be used to assign new columns, simply use a new column name and the assign operator: data$newColName &lt;-)\n\nThe following lines of code assess parts of the data frame. Try out what they do and sort the code lines and their meaning on elearning-extern.\nHint: Some of the code lines actually do the same; chose the preferred way in these cases.\n\n\n\n\n\n\nQuestion\n\n\n\nWhich of the following commands\n\ndat[2, ]\ndat[, 2]\ndat[, 1]\ndat$Ozone\nnew = dat[, 3] + dat[, 4]\ndat$new = dat[, 3] + dat[, 4]\ndat$NAs = NA\nNA -&gt; dat$NAs \n\nwill get you\n\nget the second row\nget column Ozone\ngenerate a new column with NA’s\ncalculate the sum of columns 3 and 4 and assign to a new column\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nget second row\n\ndat[2, ] is correct\ndat[, 2] gives the second column\n\nget column Ozone\n\ndat$Ozone is the best option\ndat[, 1] gives the same result, but is much harder to understand later on\n\ngenerate a new column with NA’s\n\ndat$NAs = NA is the best option\nNA -&gt; dat$NAs does the same, but the preferred syntax in R is having the new variable on the left hand side (the arrow should face to the left not right)\n\ncalculate the sum of columns 3 and 4 and assign to a new column\n\ndat$new = dat[, 3] + dat[, 4] is correct\nnew = dat[, 3] + dat[, 4] creates a new object but not a new column in the existing data frame\n\n\n\n\n\n\n2.2.5 Filtering data\nTo use the data, you must also be able to filter it. For example, we may be interested in hot days in July and August only. Hot days are typically defined as days with a temperature equal or &gt; 30°C (or 86°F as in the dataset here). Imagine, your colleague tried to query the data accordingly. She/he also found a mistake in each of the first 4 rows and wants to exclude these, but she/he is very new to R and made a few common errors in the following code:\n\n# Return only rows where the temperature is exactly is 86\ndat[dat$Temp = 86, ]\n\n# Return only rows where the temperature is equal or larger than 86\ndat[dat$Temp &gt;= 86]\n\n# Exclude rows 1 through 4\ndat[-1:4, ]\n\n# Return only rows for the months 7 or 8\ndat[dat$Month == 7 | 8, ]\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you fix his/her mistakes? These hints may help you:\n\nrows or columns can be excluded, if the numbers are given as negative numbers\n== means “equals”\n& means “AND”\n“|” means “OR” (press “AltGr”+“&lt;” to produce |, or “option”+“7” on MacOS)\nexecuting the erroneous code may help you to spot the problem\nrun parts of the code if you don’t understand what the code does\nthe last question is a bit trickier, no problem if you don’t find a solution\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the corrected code:\n\n# Return only rows where the temperature is exactly is 86\ndat[dat$Temp == 86, ]\n\n# Return only rows where the temperature is equal or larger than 86\ndat[dat$Temp &gt;= 86, ]\n\n# Exclude rows 1 through 4\ndat[-(1:4), ]\n\n# Return only rows for the months 7 or 8\ndat[dat$Month == 7 | dat$Month == 8, ]\ndat[dat$Month %in% 7:8, ] # alternative expression\n\n\n\n\n\n\n2.2.6 Last task - check if the EcoData package works for you\nDuring the course, we will use some datasets that we compiled in the \\(EcoData\\) package. You should have installed this package already(see instructions above). To check if this works, run\n\nlibrary(EcoData)\nsoep\n\nIf you see some data being displayed in your console, everything is fine.",
    "crumbs": [
      "Data input and manimulation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic data operations with R</span>"
    ]
  },
  {
    "objectID": "0-GettingStarted.html",
    "href": "0-GettingStarted.html",
    "title": "1  Getting Started with R",
    "section": "",
    "text": "1.1 Your R System\nIn this course, we work with the combination of R + RStudio.\nMake sure you have a recent version of R + RStudio installed on your computer. If you have never used RStudio, here is a good video introducing the basic system and how R and RStudio interact.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "0-GettingStarted.html#your-r-system",
    "href": "0-GettingStarted.html#your-r-system",
    "title": "1  Getting Started with R",
    "section": "",
    "text": "R is the calculation engine (language and enviromnet) that performs the computations.\nRStudio is the editor (IDE) that helps you sending inputs to R and collect outputs.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "0-GettingStarted.html#libraries-that-you-will-need",
    "href": "0-GettingStarted.html#libraries-that-you-will-need",
    "title": "1  Getting Started with R",
    "section": "1.2 Libraries that you will need",
    "text": "1.2 Libraries that you will need\nThe R engine comes with a number of base functions, but one of the great things about R is that you can extend these base functions by libraries that can be programmed by anyone. In principle, you can install libraries from any website or file. In practice, however, most commonly used libraries are distributed via two major repositories. For statistical methods, this is CRAN, and for bioinformatics, this is ioconductor](https://www.bioconductor.org/).\n\n\n\n\n\n\nClick to see more on installing libraries in R\n\n\n\n\n\nTo install a package from a library, use the command:\n\ninstall.packages(\"LIBRARY\")\n\nExchange “LIBRARY” with the name of the library you want to install. The default is to search the package in CRAN, but you can specify other repositories or file locations in the function. For Windows / Mac, R should work out of the box. For other UNIX based systems, may also need to install\nbuild-essential\ngfortran\nlibmagick++-dev\nr-base-dev\ncmake\nIf you are new to installing packages on Debian / Ubuntu, etc., type the following:\nsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev cmake\n\n\n\nIn this book, we will often use data sets from the EcoData package, which is not on CRAN, but on a GitHub page. To install the package, if you don’t have the devtools package installed already, first install devtools from CRAN by running:\n\ninstall.packages(\"devtools\")\n\nThen, install the EcoData package via:\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\",\n                         dependencies = T, build_vignettes = T)\n\nFor your convenience, the EcoData installation also forces the installation of most of the packages needed in this book, so this may take a while. If you want to load only the EcoData package, or if you encounter problems during the install, set dependencies = F, build_vignettes = F.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "1A-Exercise.html",
    "href": "1A-Exercise.html",
    "title": "Exercise - R basics",
    "section": "",
    "text": "Setting up the working environment in RStudio\nYour first task is to open RStudio and create a new project for the course.\nFor all exercises during this week, use this project! You can open it via the file system as follows (please try this out now):\nYou should now be back to RStudio in your project.\nIn the directory of the R project, generate a folder “scripts” and a folder “data”. You can do this either in the file directory or in RStudio. For the latter:\nThe idea is that you will create an R script for each exercise and save all these files in the scripts folder. You can do this as follows:",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1A-Exercise.html#setting-up-the-working-environment-in-rstudio",
    "href": "1A-Exercise.html#setting-up-the-working-environment-in-rstudio",
    "title": "Exercise - R basics",
    "section": "",
    "text": "Click the ‘File’ button in the menu, then ‘New Project’ (or the second icon in the bar below the menu “Create a project”).\nClick “New Directory”.\nClick “New Project”.\nType in the name of the directory to store your project, e.g. “IntroStatsR”.\n“Browse” to the folder on your computer where you want to have your project created.\nClick the “Create Project” button.\n\n\n\n\n(Exit RStudio).\nNavigate to the directory where you created your project.\nDouble click on the “IntroStatsR.Rproj” file in that directory.\n\n\n\n\nGo to the “Files” panel in R Studio (bottom right panel).\nClick the icon “New Folder” in the upper left corner.\nEnter the folder name.\nThe new folder is now visible in your project directory.\n\n\n\nClick the “File” button in the menu, then “New File” and “R Script” (or the first icon in the bar below the menu and then “R Script” in the dropdown menu).\nClick the “File” button in the menu, then “Save” (or the “Save” icon in the menu).\nNavigate to your scripts folder.\nEnter the file name, e.g. “Exercise_01.R”.\nSave the file.",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1A-Exercise.html#a-few-hints-before-you-can-start",
    "href": "1A-Exercise.html#a-few-hints-before-you-can-start",
    "title": "Exercise - R basics",
    "section": "A few hints before you can start",
    "text": "A few hints before you can start\nRemember the different ways of running code:\n\nclick the “Run” button in the top right corner of the top left panel (code editor) OR\nhit “Ctrl”+“Enter” (MAC: “Cmd”+“Return”)\n\nRStudio will then run\n\nthe code that is currently marked OR\nthe line of code where the text cursor currently is (simply click into that line)\n\nIf you face any problems with executing the code, check the following:\n\nall brackets closed?\ncapital letters instead of small letters?\ncomma is missing?\nif RStudio shows red attention signs (next to the code line number), take it seriously\ndo you see a “+” (instead of a “&gt;”) in the console? stop executions with “esc” key and then try again.\n\nHave a look at the shortcuts by clicking “Tools” and than “Keybord Shortcuts Help”!!",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1A-Exercise.html#basic-data-structures-in-r",
    "href": "1A-Exercise.html#basic-data-structures-in-r",
    "title": "Exercise - R basics",
    "section": "Basic data structures in R",
    "text": "Basic data structures in R\nBefore we work with real data, we should first recap important data structures in R\nA single value (type does not matter) is called a scalar (it is just one value):\n\na = 5\nprint(a)\n## [1] 5\n\nthis_letter = \"A\"\nprint(this_letter)\n## [1] \"A\"\n\n\n\n&lt;- and = are assignment operators, they are equivalent and are used to assign values, data, or objects to a variable.\nIn R you can use any type of name for a variable, you can even mix numbers and dots in the name: test5 or test.5, but there is one restriction, no special symbols (as they are usually operators or functions) and a name cannot start with a number, for example 5test will throw an error.\nHowever, usually we want to assign several values to a variable. For example, a dataset consists of several columns (=variables). We can use the function c(...) to connect (or concatenate) several values:\n\nage = c(20, 50, 30, 70)\nprint(age)\n## [1] 20 50 30 70\nnames = c(\"Anna\", \"Daniel\", \"Martin\", \"Laura\")\nprint(names)\n## [1] \"Anna\"   \"Daniel\" \"Martin\" \"Laura\"\n\n\nVectors\n\n\nYou can only concatenate values from the same data type! If they are different, all will be casted to the same data type!\n\nprint(c(\"Age\", 5, TRUE))\n## [1] \"Age\"  \"5\"    \"TRUE\"\n\nThe c(...) function returns a vector which is a one-dimensional array. You can access elements of the vector by using the square brackets [which_element]:\n\nage[2] # second element\n## [1] 50\nage[1] # first element\n## [1] 20\n\nThis is known as indexing. And there are a few tricks:\n\nUse [-n] to return all elements except for n:\n\nage[-2] # return all except for the second element\n## [1] 20 30 70\n\nUse another vector to return several elements at once:\n\nage[c(1, 3)] # return first and third elements\n## [1] 20 30\nage[-c(1,3)] # return all elements except for first and third elements\n## [1] 50 70\n\nUse &lt;- or = to re-assign/change elements in your vector\n\nage[2] = 99\nprint(age)\n## [1] 20 99 30 70\n\n\n\n\nThe : operator in R is not the division operator. It actually creates a range of integer values with start:end:\n\n1:5\n## [1] 1 2 3 4 5\n\nWhich is really useful for indexing:\n\nage[1:3]\n## [1] 20 99 30\n\n\n\nMatrix\nUsually a dataset consist not of only one variable/vector but of several variables (columns) and observations (rows), for example:\n\nage = c(20, 30, 32, 40)\nweight = c(60, 70, 72, 80)\n\nwe can use higher order data structures to combine these variables in a two dimensional array (like we would, for example, do in excel) using the matrix(...) function:\n\ndataset = matrix(NA, 4, 2)\ndataset # empty dataset\n##      [,1] [,2]\n## [1,]   NA   NA\n## [2,]   NA   NA\n## [3,]   NA   NA\n## [4,]   NA   NA\ndataset[,1] = age\ndataset[,2] = weight\ndataset\n##      [,1] [,2]\n## [1,]   20   60\n## [2,]   30   70\n## [3,]   32   72\n## [4,]   40   80\n\nSimilar to a vector we can index certain elements in the matrix or at the same time entire rows or columns. Since is has now two dimensions, we change [i] to [row_i, col_j]. The first argument specifies which row and the second argument which column should be returned. There are again a few handy tricks, above we left the rows empty (dataset[,1]) which will R interpret as “use all rows”, in that way we can print/return entire columns or rows:\n\ndataset[,1] # first column\n## [1] 20 30 32 40\ndataset[1,] # first row\n## [1] 20 60\n\n\n\nDon’t worry, you don’t have to create your own data sets like we did in this section. When you import your data into R, it is automatically returned as a matrix (or as data.frame, see below).\nA limitation of the matrix() is that is can only consist of one data type (like the vectors), if we mix the data types, all will be cast to the same data type:\n\ncbind(age, names)\n##      age  names   \n## [1,] \"20\" \"Anna\"  \n## [2,] \"30\" \"Daniel\"\n## [3,] \"32\" \"Martin\"\n## [4,] \"40\" \"Laura\"\n\n\n\ncbind() is a function that combines columns (“column binds”), it can be used as a shortcut to create a matrix from several vectors. Another important command is rbind(...) which combines vectors (or matrices) over their rows:\n\nrbind(age, names)\n##       [,1]   [,2]     [,3]     [,4]   \n## age   \"20\"   \"30\"     \"32\"     \"40\"   \n## names \"Anna\" \"Daniel\" \"Martin\" \"Laura\"\n\n\n\nData.frames\nThe data.frame() can handle variables with different data types. Data.frames are similar to matrices, they are two dimensional and the indexing is the same:\n\ndf = data.frame(age, names, weight)\ndf\n##   age  names weight\n## 1  20   Anna     60\n## 2  30 Daniel     70\n## 3  32 Martin     72\n## 4  40  Laura     80\nstr(df)\n## 'data.frame':    4 obs. of  3 variables:\n##  $ age   : num  20 30 32 40\n##  $ names : chr  \"Anna\" \"Daniel\" \"Martin\" \"Laura\"\n##  $ weight: num  60 70 72 80\n\n(we will talk below more about data.frames)",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1A-Exercise.html#getting-an-overview-of-a-dataset",
    "href": "1A-Exercise.html#getting-an-overview-of-a-dataset",
    "title": "Exercise - R basics",
    "section": "Getting an overview of a dataset",
    "text": "Getting an overview of a dataset\nWe work with the airquality dataset:\n\ndat = airquality\n\n\n\nSeveral example datasets are already available in R. The airquality dataset with daily air quality measurements (see ?airquality). Another famous dataset is the iris dataset with flower trait measurements for three species (see ?iris).\nCopy the code into your code editor and execute it.\nBefore working with a dataset, you should always get an overview of it. Helpful functions for this are:\n\nstr()\nView()\nhead() and tail()\n\nTry out these functions and provide answers to the following questions:\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhat is the most common atomic class in the airquality dataset? integernumericcharacterfactor\nHow many rows does the dataset have? \nWhat is the last value in the column “Temp”? \n\n\nTo see all this, run\n\ndat = airquality\nView(dat)\nstr(dat)\nhead(dat)\ntail(dat)\n\n\n\n\nHints:\n\nRun str(airquality)\nSee ?nrow or ?dim\nRun tail(airquality$Temp)\n\n\n\nClick here to see the solution\n\nWhat is the most common atomic class in the airquality dataset?\n\ninteger\nfunction str() helps to find this out\n\nHow many rows does the dataset have?\n\n153\nthis is easiest to see when using the function str(dat)\ndim(dat) or nrow(dat) give the same information\n\nWhat is the last value in the column “Temp”?\n\n68\ntail(dat) helps to find this out very fast",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1A-Exercise.html#accessing-rows-and-columns-of-a-data-frame",
    "href": "1A-Exercise.html#accessing-rows-and-columns-of-a-data-frame",
    "title": "Exercise - R basics",
    "section": "Accessing rows and columns of a data frame",
    "text": "Accessing rows and columns of a data frame\nYou have seen how you can use squared brackets [ ] and the dollar sign $ to extract parts of your data. Some people find this confusing, so let’s repeat the basic concepts:\n\nsquared brackets are used as follows: data[rowNumber, columnNumber]\nthe dollar sign helps to extract colums with their name (good for readability): data$columnName\nthis syntax can also be used to assign new columns, simply use a new column name and the assign operator: data$newColName &lt;-)\n\n\n\n\n\n\n\nQuestion\n\n\n\nThe following lines of code assess parts of the data frame. Try out what they do and sort the code lines and their meaning:\nWhich of the following commands\n\ndat[2, ]\ndat[, 2]\ndat[, 1]\ndat$Ozone\nnew = dat[, 3] + dat[, 4]\ndat$new = dat[, 3] + dat[, 4]\ndat$NAs = NA\nNA -&gt; dat$NAs \n\nwill get you\n\nget the second row\nget column Ozone\ngenerate a new column with NA’s\ncalculate the sum of columns 3 and 4 and assign to a new column\n\n\n\n\n\nHint: Some of the code lines actually do the same; chose the preferred way in these cases.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nget second row\n\ndat[2, ] is correct\ndat[, 2] gives the second column\n\nget column Ozone\n\ndat$Ozone is the best option\ndat[, 1] gives the same result, but is much harder to understand later on\n\ngenerate a new column with NA’s\n\ndat$NAs = NA is the best option\nNA -&gt; dat$NAs does the same, but the preferred syntax in R is having the new variable on the left hand side (the arrow should face to the left not right)\n\ncalculate the sum of columns 3 and 4 and assign to a new column\n\ndat$new = dat[, 3] + dat[, 4] is correct\nnew = dat[, 3] + dat[, 4] creates a new object but not a new column in the existing data frame",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1A-Exercise.html#filtering-data",
    "href": "1A-Exercise.html#filtering-data",
    "title": "Exercise - R basics",
    "section": "Filtering data",
    "text": "Filtering data\nTo use the data, you must also be able to filter it. For example, we may be interested in hot days in July and August only. Hot days are typically defined as days with a temperature equal or &gt; 30°C (or 86°F as in the dataset here). Imagine, your colleague tried to query the data accordingly. She/he also found a mistake in each of the first 4 rows and wants to exclude these, but she/he is very new to R and made a few common errors in the following code:\n\n# Return only rows where the temperature is exactly is 86\ndat[dat$Temp = 86, ]\n\n# Return only rows where the temperature is equal or larger than 86\ndat[dat$Temp &gt;= 86]\n\n# Exclude rows 1 through 4\ndat[-1:4, ]\n\n# Return only rows for the months 7 or 8\ndat[dat$Month == 7 | 8, ]\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you fix his/her mistakes? These hints may help you:\n\nrows or columns can be excluded, if the numbers are given as negative numbers\n== means “equals”\n& means “AND”\n| means “OR” (press “AltGr”+“&lt;” to produce |, or “option”+“7” on MacOS)\nexecuting the erroneous code may help you to spot the problem\nrun parts of the code if you don’t understand what the code does\nthe last question is a bit trickier, no problem if you don’t find a solution\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the corrected code:\n\n# Return only rows where the temperature is exactly is 86\ndat[dat$Temp == 86, ]\n\n# Return only rows where the temperature is equal or larger than 86\ndat[dat$Temp &gt;= 86, ]\n\n# Exclude rows 1 through 4\ndat[-(1:4), ]\n\n# Return only rows for the months 7 or 8\ndat[dat$Month == 7 | dat$Month == 8, ]\ndat[dat$Month %in% 7:8, ] # alternative expression\n\n\n\n\n\n\nThe %in% operator is useful when you want to check whether a value is inside a vector or not:\n\n5 %in% c(1, 2, 3, 4, 5)\n## [1] TRUE\n\nWe will discuss the results together.\nWhen you are finished, save your R script!",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1A-Exercise.html#last-task---install-ecodata-package",
    "href": "1A-Exercise.html#last-task---install-ecodata-package",
    "title": "Exercise - R basics",
    "section": "Last task - Install EcoData package",
    "text": "Last task - Install EcoData package\nDuring the course, we will use some datasets that we compiled in the \\(EcoData\\) package. To access the datasets, you need to install the package from github. To do this, you will also need to install the \\(devtools\\) package.\nTry the following code to install the two packages:\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = F, build_vignettes = F)\nlibrary(EcoData)\n\nRemember that you have to install a package only once. If you open R Studio the next time, it is enough to run library(EcoData).\n\nAlternative ways to get EcoData\nIf the installation didn’t work, download the package file manually from\nhttps://github.com/TheoreticalEcology/ecodata/releases/download/v0.2.1/EcoData_0.2.1.tar.gz\nStore the file on your computer in the same folder where you created your R project. Then run the following code:\n\ninstall.packages(\"EcoData_0.2.1.tar.gz\", \n                 repos = NULL, type = \"source\")\nlibrary(EcoData)\n\nIf this wasn’t successful either, you can download the combined datasets from elearning (see Organisation and every-day material)\nStore the file on your computer in the same folder where you created your R project. Then run the following code:\n\nload(\"EcoData.Rdata\")\n\n(Note that you will not be able to access the dataset descriptions when you use this option).",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1A-Exercise.html#bonus---advanced-programming",
    "href": "1A-Exercise.html#bonus---advanced-programming",
    "title": "Exercise - R basics",
    "section": "Bonus - Advanced programming",
    "text": "Bonus - Advanced programming\nUntil now we have only learned how to use functions and indexing of data structures. But what are functions?\n\nFunctions\nA functions are self contained blocks of code that do something, for example, the average of a vector is given by:\n\\[\nAverage = \\frac{1}{N} \\sum_{i=1}^N x_i\n\\]\nIn R we can easily calculate the sum over a vector by using the function sum():\n\nvalues = 1:10\nprint(values)\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\n# Average \nsum(values)/length(values)\n## [1] 5.5\n\nTo do that now more easily and in a comprehensive way for many different variables, we can define a function to calculate the mean:\n\naverage = function(x) {\n  average = sum(x)/length(x)\n  return(average)\n}\naverage(values)\n## [1] 5.5\n\nA function consists of: - An expressive name - Arguments function(arg1, arg2, arg3), the arguments can be used to pass the data to the function, or to change the behaviour of the function (see below) - A function body, inside curly brackets { } where the actual magic happens - return(...) what should be returned from the function\nThe advantages: - you can compress big code blocks within one function call - reproducibility, we avoid writing the same code again and again, if we want to change the way how we calculate the average, we have to change it only in one place - clarity, the name of the function can give us a hint about what the function is doing\nArguments\nArguments can be either used to pass data to the function or to change the behaviour of the function. Moreover, you can set default values to the function. If arguments have default values, they do not have to be specified (specifiying means that we have to fill this argument):\n\n# Should NAs be removed or not\naverage = function(x, remove_na) {\n  if(!remove_na) {\n    average = sum(x)/length(x)\n  } else {\n    average = sum(x, na.rm = TRUE)/length(x[complete.cases(x)])\n  }\n  return(average)\n}\n\nvalues = c(5, 4, 3, NA, 5, 2)\n\n# no default option for remove_na, we have to specify it!\naverage(values, remove_na = TRUE)\n## [1] 3.8\n\n# In this case, it is better to set a default option for remova_na:\naverage = function(x, remove_na = TRUE) {\n  if(!remove_na) {\n    average = sum(x)/length(x)\n  } else {\n    average = sum(x, na.rm = TRUE)/length(x[complete.cases(x)])\n  }\n  return(average)\n}\n\naverage(values)\n## [1] 3.8\n\n\n\nif(condition) {  } else { } the if/else statements runs code if a certain condition is true or not. If the condition is true, the first code block {  } is run, if it is false, the second (after the else) is run:\n\nvalues = 1:5\nif(length(values) == 5) {\n  print(\"This vector has length 5\")\n} else {\n  print(\"This vector has not length 5\")\n}\n## [1] \"This vector has length 5\"\n\nArguments are matched by the name or, if names are not specified, by the order:\nfunc(x1, x2, x3) will be interpreted as func(arg1 = x1, arg2 = x2, arg3 = x3)\nBut be careful, if you are unsure about the correct order, you should pass them by their name (func(arg1 = x1, arg2 = x2, arg3 = x3))\n\n\nLoops\nLoops are another important code structure. Example: We want to go over all values of a vector, calculate the square root of it, and overwrite the old value with the new value:\n\nvalues = c(20, 33, 25, 16)\nvalues[1] = sqrt(values[1])\nvalues[2] = sqrt(values[2])\nvalues[3] = sqrt(values[3])\nvalues[4] = sqrt(values[4])\n\nNow what should we do if we have thousands of observations? Loops are the solution! We can use them to automatically “run” a specific vector and then do something with it (well it sounds cryptic but it is actually quite easy):\n\nfor(i in 1:4) { # i in 1:4 means that i should be 1, 2, 3, and 4\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n\n# Let's use it to automatize the previous computation:\nfor(i in 1:4) {\n  values[i] = sqrt(values[i])\n}\nvalues\n## [1] 2.114743 2.396782 2.236068 2.000000\n\n# Even better: do not hardcode the length of the vector:\nfor(i in 1:length(values)) {\n  values[i] = sqrt(values[i])\n}\nvalues\n## [1] 1.454215 1.548154 1.495349 1.414214\n\nOur code will now always work, even if we change the length of the values variable!\n\n\n\n\n\n\nBonus Question\n\n\n\nWrite functions for:\n\nCalculate the sum for all values in a matrix given by (we want to write our own implementation of the internal sum(...) function):\n\nmy_matrix = matrix(1:200, 20, 10)\n\nUse the internal sum(...) function to check whether your function is correct!\nExtend the function with arguments that specify that the sum should be calculate over rows, columns, or both (if we calculate the sum over rows or columns, then a vector with n sums for n rows or n columns should be returned).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum_matrix function\n\n\n\nsum_matrix = function(X) {\n\n  n_row = nrow(X)\n  n_col = ncol(X)\n  result = 0\n  for(i in 1:n_row) {\n    for(j in 1:n_col) {\n      result = result + X[i,j]\n    }\n  }\n  return(result)\n}\n\n\nsum_matrix_extended function\n\n\n  sum_matrix_extended = function(X, which = \"both\") {\n  if(which == \"both\") {\n    result = sum_matrix(X)\n  } else if(which == \"row\") {\n    result = apply(X, 1, sum)\n  } else if(which == \"row\") {\n    result = apply(X, 2, sum)\n  }\n  return(result)\n}\n\nThe apply(...) function can be used to automatically loop over rows (MARGIN=1) or columns (MARGIN=2) and apply a function on each element (rows or columns) which can be specified via apply(data, MARGIN = 1, FUN = sum)",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "0-GettingStarted.html#extra-resources",
    "href": "0-GettingStarted.html#extra-resources",
    "title": "1  Getting Started with R",
    "section": "1.3 Extra resources",
    "text": "1.3 Extra resources\nTo get to know Rstudio better, here is the link for a brief tour from another “bookdown” page with simple and good explanations for each panel and the many resources.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started with R</span>"
    ]
  },
  {
    "objectID": "1B-PreparingData.html",
    "href": "1B-PreparingData.html",
    "title": "3  Preparing your data",
    "section": "",
    "text": "3.1 Importing data\nThe recommended data format for your raw data is csv. You can export to csv from excel. If you have a csv file in standard (international) format, the command to import is simply\ndat = read.csv(file = \"../data/myData.csv\")\nIf your csv file departs from standard settings (e.g. you use a , insted of a . as decimal points), you will have to modify the function. Go on the read.csv function and press F1 to get the help, which explains all that. Alternatively, you can use the import menu to the top right in RStudio.\nHere is a video Video of how to read in csv data in R.",
    "crumbs": [
      "Data input and manimulation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing your data</span>"
    ]
  },
  {
    "objectID": "1B-PreparingData.html#importing-data",
    "href": "1B-PreparingData.html#importing-data",
    "title": "3  Preparing your data",
    "section": "",
    "text": "You can open the documentation of a R function by pressing F1 while the cursor is on the function name or by runnin ?read.scv\n\n\n\n\n\n\n\n\nRemarks on data handling\n\n\n\n\n\nTypically, data will be recorded electronically with a measurement device, or you have to enter it manually using a spreadsheet program, e.g. MS Excel. The best format for data storage is csv (comma separated values) because it is long-term compatibility with all kinds of programs / systems (Excel can export to csv).\nAfter raw data is entered, it should never be manipulated by hand! If you modify data by hand, make a copy and document all changes (additional text file). Better: Make changes using a script\nData handling in R:\n\ncreate R script “dataprep.R” or similar and import dataset\npossibly combine different datasets\nclean data (remove NAs, impossible values etc.)\nsave as Rdata (derived data)\n\n\n\n\n\n\nR can also import data from nearly any data source, including xls or xlsx files. Here and here two websites with import explanations for many different data formats",
    "crumbs": [
      "Data input and manimulation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing your data</span>"
    ]
  },
  {
    "objectID": "1B-PreparingData.html#cleaning-the-data",
    "href": "1B-PreparingData.html#cleaning-the-data",
    "title": "3  Preparing your data",
    "section": "3.2 Cleaning the data",
    "text": "3.2 Cleaning the data\nChecking / cleaning means that you ensure that you have written in your data correctly, and that you resolve issues with the data. Most real data has some problems, e.g. missing values etc. The basic checks that I would recommend is:\nUsually, this will immediately uncover some problems. The exact solution will depend very much on the nature of the data, but common things are typos in the raw data (e.g. letters in a column that should be numeric, etc), but minimally you should\n\nLook at your data (double click in Rstudio, or view() to see if anything is weird)\nRun summary() and str() to check range, NAs, and type of all variables (e.g. categorical variables are often imported as character, change them to factors with the as.factor() function)\n\nHere is a video that shows an example of a cleaning process.",
    "crumbs": [
      "Data input and manimulation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing your data</span>"
    ]
  },
  {
    "objectID": "1B-PreparingData.html#subsetting-aggregating-or-re-structuring-your-data",
    "href": "1B-PreparingData.html#subsetting-aggregating-or-re-structuring-your-data",
    "title": "3  Preparing your data",
    "section": "3.3 Subsetting, aggregating or re-structuring your data",
    "text": "3.3 Subsetting, aggregating or re-structuring your data\nOften, you just want to use a part of your data, or copy, merge or split data. All you need to know is explained here ?sec-datamanipulation",
    "crumbs": [
      "Data input and manimulation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparing your data</span>"
    ]
  },
  {
    "objectID": "1A-basicOperations.html",
    "href": "1A-basicOperations.html",
    "title": "2  Basic data operations with R",
    "section": "",
    "text": "2.1 Data manipulation in R\nR works like a calculator:\n2+2\n## [1] 4\n5*4\n## [1] 20\n2^2\n## [1] 4\nWe can also use functions that perform specific calculations:\nsqrt(4)\n## [1] 2\nsum(c(2,2))\n## [1] 4\nWe can assign values/data to variables:\nobject.name &lt;- 1\nNote that both operators ‘&lt;-’ or “=” work. Functions in R (e.g. sum(), mean(), etc.) have arguments that control/change their behavior and are also used to pass the data to the function:\nmean(x = c(2, 2))\n## [1] 2",
    "crumbs": [
      "Data input and manimulation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic data operations with R</span>"
    ]
  },
  {
    "objectID": "1A-basicOperations.html#data-manipulation-in-r",
    "href": "1A-basicOperations.html#data-manipulation-in-r",
    "title": "2  Basic data operations with R",
    "section": "",
    "text": "Help\n\n\n\nA list and description of all arguments can be found in the help of a function (which can be accessed via ?mean or if you place the cursor on the function and press F1)\n\n\n\n2.1.1 Data types and data structures\nThere are four important data types in R (there are more but we focus on these 5):\n\nNumeric: 1, 2, 3, 4\nLogical: TRUE or FALSE\nCharacters: “A”, “B”,…\nFactors which are characters but we have to tell R explicitly that they are factors\nNot a number: NA, NaN (empty value)\n\nBased on the data types we can build data structures which contain either only specific data types or a mixture of data types:\n\nVector: Several values of one data type, can be created with the c function:\n\nc(5, 3, 5, 6) # numeric vector\nc(TRUE, TRUE, FALSE, TRUE) # logical vector\nc(\"A\", \"B\", \"C\") # character vector\nas.factor(c(\"A\", \"B\", \"C\")) # factor vector\n\nMatrix: two-dimensional data structure of one data type, can be created with the matrix function (we can pass a vector to the matrix function and tell it via arguments how the matrix should be constructed):\n\nmatrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2)\n##      [,1] [,2]\n## [1,]    1    4\n## [2,]    2    5\n## [3,]    3    6\n\nData.frame: Often our data has variables of different types which makes a matrix unsuitable data structure. Data.frames can handle different data types and is organized in columns (one column = one variables) and can be created with the data.frame function:\n\ndata.frame(A = c(1, 2, 3), B = c(\"A\", \"B\", \"C\"), C = c(TRUE, FALSE, FALSE))\n##   A B     C\n## 1 1 A  TRUE\n## 2 2 B FALSE\n## 3 3 C FALSE\n\n\n\n\n2.1.2 Data manipulation\nA vector is a one dimensional data structure and we can access the values by using [ ]:\n\nvec = c(1, 2, 3, 4, 5)\nvec[1] # access first element\n## [1] 1\nvec[5] # access last element\n## [1] 5\n\nA data.frame is a two dimensional data structure. Let’s define a data.frame from two vectors:\n\ndf = data.frame(\n  x = c(2,2,2,3,2,2,1), #add column named x with 2 elements\n  y = c(4,5,5,4,5,3,5) #add a second column named y\n)\n#Let's see how this looks like:\ndf\n##   x y\n## 1 2 4\n## 2 2 5\n## 3 2 5\n## 4 3 4\n## 5 2 5\n## 6 2 3\n## 7 1 5\n\nAccess parts of the data.frame:\n\ndf[1,2] #get element in row 1, column 1\n## [1] 4\ndf[7,1] #get element in row 7, column 1\n## [1] 1\ndf[2,] #get row 2\n##   x y\n## 2 2 5\ndf[,2] #get column 2\n## [1] 4 5 5 4 5 3 5\n#or use the $ sign to access columns:\ndf$y\n## [1] 4 5 5 4 5 3 5\ndf[2:4,1:2] #get rows 2 to 4 and only columns 1 and 2\n##   x y\n## 2 2 5\n## 3 2 5\n## 4 3 4\n\nWe can also set filters:\n\ndf[df$x &gt; 2,] # show only data where x is larger than 2\n##   x y\n## 4 3 4\ndf[df$y == 5,] #show only data where y equals 5\n##   x y\n## 2 2 5\n## 3 2 5\n## 5 2 5\n## 7 1 5\ndf[df$y == 5 & df$x == 1,] #show only data where y equals 5 AND x equals 1\n##   x y\n## 7 1 5\ndf[df$y == 5 | df$x == 3,] #show data where y equals 5 OR x equals 3\n##   x y\n## 2 2 5\n## 3 2 5\n## 4 3 4\n## 5 2 5\n## 7 1 5\n\n\n\n\n\n\n\nLogical operators\n\n\n\n\n\n\nLogical operators in R\n\n\nOperators\nMeaning\n\n\n\n\n&lt;\nLess than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;\nMore than\n\n\n&gt;=\nMore than or equal to\n\n\n==\nEqual to\n\n\n!=\nNot equal to\n\n\n!a\nNot a\n\n\na|b\na or b\n\n\na & b\na and b\n\n\nisTRUE(a)\nTest if a is true\n\n\n\n\n\n\nAdd an additional column with NA values:\n\ndf$NAs = NA #fills up a new column named NAs with all NA values\ndf\n##   x y NAs\n## 1 2 4  NA\n## 2 2 5  NA\n## 3 2 5  NA\n## 4 3 4  NA\n## 5 2 5  NA\n## 6 2 3  NA\n## 7 1 5  NA\n\n\n\n2.1.3 Data analysis workflow\nThis is a simple version of what you’re going to learn during this course:\n\nLet’s say we measured the size of individuals in two different treatment groups\n\ngroup1 = c(2,2,2,3,2,2,1.1)\ngroup2 = c(4,5,5,4,5,3,5.1) \n\nclass(group2)\n## [1] \"numeric\"\n\nDescriptive statistics and visualization\n\nmean(group1)\n## [1] 2.014286\nmean(group2)\n## [1] 4.442857\n\nboxplot(group1, group2)\n\n\n\n\n\n\n\n\nTesting for differences. Question: Is there a difference between group1 and group2?\n\nt.test(group1, group2)\n## \n##  Welch Two Sample t-test\n## \n## data:  group1 and group2\n## t = -6.6239, df = 10.628, p-value = 4.413e-05\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -3.238992 -1.618151\n## sample estimates:\n## mean of x mean of y \n##  2.014286  4.442857\n\nInterpretation of the results. Individuals in Group 2 were larger than those in group 1 (t test, t = -6.62, p &lt; 0.0001)\n\nIn the course we will work a lot with datasets implemented in R or in R packages which can be accessed via their name:\n\ndat = airquality\nhead(dat)\n##   Ozone Solar.R Wind Temp Month Day\n## 1    41     190  7.4   67     5   1\n## 2    36     118  8.0   72     5   2\n## 3    12     149 12.6   74     5   3\n## 4    18     313 11.5   62     5   4\n## 5    NA      NA 14.3   56     5   5\n## 6    28      NA 14.9   66     5   6",
    "crumbs": [
      "Data input and manimulation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic data operations with R</span>"
    ]
  },
  {
    "objectID": "1C-Exercise.html",
    "href": "1C-Exercise.html",
    "title": "Exercise - R basics",
    "section": "",
    "text": "Setting up the working environment in RStudio\nYour first task is to open RStudio and create a new project for the course.\nFor all exercises during this week, use this project! You can open it via the file system as follows (please try this out now):\nYou should now be back to RStudio in your project.\nIn the directory of the R project, generate a folder “scripts” and a folder “data”. You can do this either in the file directory or in RStudio. For the latter:\nThe idea is that you will create an R script for each exercise and save all these files in the scripts folder. You can do this as follows:",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1C-Exercise.html#setting-up-the-working-environment-in-rstudio",
    "href": "1C-Exercise.html#setting-up-the-working-environment-in-rstudio",
    "title": "Exercise - R basics",
    "section": "",
    "text": "Click the ‘File’ button in the menu, then ‘New Project’ (or the second icon in the bar below the menu “Create a project”).\nClick “New Directory”.\nClick “New Project”.\nType in the name of the directory to store your project, e.g. “IntroStatsR”.\n“Browse” to the folder on your computer where you want to have your project created.\nClick the “Create Project” button.\n\n\n\n\n(Exit RStudio).\nNavigate to the directory where you created your project.\nDouble click on the “IntroStatsR.Rproj” file in that directory.\n\n\n\n\nGo to the “Files” panel in R Studio (bottom right panel).\nClick the icon “New Folder” in the upper left corner.\nEnter the folder name.\nThe new folder is now visible in your project directory.\n\n\n\nClick the “File” button in the menu, then “New File” and “R Script” (or the first icon in the bar below the menu and then “R Script” in the dropdown menu).\nClick the “File” button in the menu, then “Save” (or the “Save” icon in the menu).\nNavigate to your scripts folder.\nEnter the file name, e.g. “Exercise_01.R”.\nSave the file.",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1C-Exercise.html#a-few-hints-before-you-can-start",
    "href": "1C-Exercise.html#a-few-hints-before-you-can-start",
    "title": "Exercise - R basics",
    "section": "A few hints before you can start",
    "text": "A few hints before you can start\nRemember the different ways of running code:\n\nclick the “Run” button in the top right corner of the top left panel (code editor) OR\nhit “Ctrl”+“Enter” (MAC: “Cmd”+“Return”)\n\nRStudio will then run\n\nthe code that is currently marked OR\nthe line of code where the text cursor currently is (simply click into that line)\n\nIf you face any problems with executing the code, check the following:\n\nall brackets closed?\ncapital letters instead of small letters?\ncomma is missing?\nif RStudio shows red attention signs (next to the code line number), take it seriously\ndo you see a “+” (instead of a “&gt;”) in the console? stop executions with “esc” key and then try again.\n\nHave a look at the shortcuts by clicking “Tools” and than “Keybord Shortcuts Help”!!",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1C-Exercise.html#basic-data-structures-in-r",
    "href": "1C-Exercise.html#basic-data-structures-in-r",
    "title": "Exercise - R basics",
    "section": "Basic data structures in R",
    "text": "Basic data structures in R\nBefore we work with real data, we should first recap important data structures in R\nA single value (type does not matter) is called a scalar (it is just one value):\n\na = 5\nprint(a)\n## [1] 5\n\nthis_letter = \"A\"\nprint(this_letter)\n## [1] \"A\"\n\n\n\n&lt;- and = are assignment operators, they are equivalent and are used to assign values, data, or objects to a variable.\nIn R you can use any type of name for a variable, you can even mix numbers and dots in the name: test5 or test.5, but there is one restriction, no special symbols (as they are usually operators or functions) and a name cannot start with a number, for example 5test will throw an error.\nHowever, usually we want to assign several values to a variable. For example, a dataset consists of several columns (=variables). We can use the function c(...) to connect (or concatenate) several values:\n\nage = c(20, 50, 30, 70)\nprint(age)\n## [1] 20 50 30 70\nnames = c(\"Anna\", \"Daniel\", \"Martin\", \"Laura\")\nprint(names)\n## [1] \"Anna\"   \"Daniel\" \"Martin\" \"Laura\"\n\n\nVectors\n\n\nYou can only concatenate values from the same data type! If they are different, all will be casted to the same data type!\n\nprint(c(\"Age\", 5, TRUE))\n## [1] \"Age\"  \"5\"    \"TRUE\"\n\nThe c(...) function returns a vector which is a one-dimensional array. You can access elements of the vector by using the square brackets [which_element]:\n\nage[2] # second element\n## [1] 50\nage[1] # first element\n## [1] 20\n\nThis is known as indexing. And there are a few tricks:\n\nUse [-n] to return all elements except for n:\n\nage[-2] # return all except for the second element\n## [1] 20 30 70\n\nUse another vector to return several elements at once:\n\nage[c(1, 3)] # return first and third elements\n## [1] 20 30\nage[-c(1,3)] # return all elements except for first and third elements\n## [1] 50 70\n\nUse &lt;- or = to re-assign/change elements in your vector\n\nage[2] = 99\nprint(age)\n## [1] 20 99 30 70\n\n\n\n\nThe : operator in R is not the division operator. It actually creates a range of integer values with start:end:\n\n1:5\n## [1] 1 2 3 4 5\n\nWhich is really useful for indexing:\n\nage[1:3]\n## [1] 20 99 30\n\n\n\nMatrix\nUsually a dataset consist not of only one variable/vector but of several variables (columns) and observations (rows), for example:\n\nage = c(20, 30, 32, 40)\nweight = c(60, 70, 72, 80)\n\nwe can use higher order data structures to combine these variables in a two dimensional array (like we would, for example, do in excel) using the matrix(...) function:\n\ndataset = matrix(NA, 4, 2)\ndataset # empty dataset\n##      [,1] [,2]\n## [1,]   NA   NA\n## [2,]   NA   NA\n## [3,]   NA   NA\n## [4,]   NA   NA\ndataset[,1] = age\ndataset[,2] = weight\ndataset\n##      [,1] [,2]\n## [1,]   20   60\n## [2,]   30   70\n## [3,]   32   72\n## [4,]   40   80\n\nSimilar to a vector we can index certain elements in the matrix or at the same time entire rows or columns. Since is has now two dimensions, we change [i] to [row_i, col_j]. The first argument specifies which row and the second argument which column should be returned. There are again a few handy tricks, above we left the rows empty (dataset[,1]) which will R interpret as “use all rows”, in that way we can print/return entire columns or rows:\n\ndataset[,1] # first column\n## [1] 20 30 32 40\ndataset[1,] # first row\n## [1] 20 60\n\n\n\nDon’t worry, you don’t have to create your own data sets like we did in this section. When you import your data into R, it is automatically returned as a matrix (or as data.frame, see below).\nA limitation of the matrix() is that is can only consist of one data type (like the vectors), if we mix the data types, all will be cast to the same data type:\n\ncbind(age, names)\n##      age  names   \n## [1,] \"20\" \"Anna\"  \n## [2,] \"30\" \"Daniel\"\n## [3,] \"32\" \"Martin\"\n## [4,] \"40\" \"Laura\"\n\n\n\ncbind() is a function that combines columns (“column binds”), it can be used as a shortcut to create a matrix from several vectors. Another important command is rbind(...) which combines vectors (or matrices) over their rows:\n\nrbind(age, names)\n##       [,1]   [,2]     [,3]     [,4]   \n## age   \"20\"   \"30\"     \"32\"     \"40\"   \n## names \"Anna\" \"Daniel\" \"Martin\" \"Laura\"\n\n\n\nData.frames\nThe data.frame() can handle variables with different data types. Data.frames are similar to matrices, they are two dimensional and the indexing is the same:\n\ndf = data.frame(age, names, weight)\ndf\n##   age  names weight\n## 1  20   Anna     60\n## 2  30 Daniel     70\n## 3  32 Martin     72\n## 4  40  Laura     80\nstr(df)\n## 'data.frame':    4 obs. of  3 variables:\n##  $ age   : num  20 30 32 40\n##  $ names : chr  \"Anna\" \"Daniel\" \"Martin\" \"Laura\"\n##  $ weight: num  60 70 72 80\n\n(we will talk below more about data.frames)",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1C-Exercise.html#getting-an-overview-of-a-dataset",
    "href": "1C-Exercise.html#getting-an-overview-of-a-dataset",
    "title": "Exercise - R basics",
    "section": "Getting an overview of a dataset",
    "text": "Getting an overview of a dataset\nWe work with the airquality dataset:\n\ndat = airquality\n\n\n\nSeveral example datasets are already available in R. The airquality dataset with daily air quality measurements (see ?airquality). Another famous dataset is the iris dataset with flower trait measurements for three species (see ?iris).\nCopy the code into your code editor and execute it.\nBefore working with a dataset, you should always get an overview of it. Helpful functions for this are:\n\nstr()\nView()\nhead() and tail()\n\nTry out these functions and provide answers to the following questions:\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhat is the most common atomic class in the airquality dataset? integernumericcharacterfactor\nHow many rows does the dataset have? \nWhat is the last value in the column “Temp”? \n\n\nTo see all this, run\n\ndat = airquality\nView(dat)\nstr(dat)\nhead(dat)\ntail(dat)\n\n\n\n\nHints:\n\nRun str(airquality)\nSee ?nrow or ?dim\nRun tail(airquality$Temp)\n\n\n\nClick here to see the solution\n\nWhat is the most common atomic class in the airquality dataset?\n\ninteger\nfunction str() helps to find this out\n\nHow many rows does the dataset have?\n\n153\nthis is easiest to see when using the function str(dat)\ndim(dat) or nrow(dat) give the same information\n\nWhat is the last value in the column “Temp”?\n\n68\ntail(dat) helps to find this out very fast",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1C-Exercise.html#accessing-rows-and-columns-of-a-data-frame",
    "href": "1C-Exercise.html#accessing-rows-and-columns-of-a-data-frame",
    "title": "Exercise - R basics",
    "section": "Accessing rows and columns of a data frame",
    "text": "Accessing rows and columns of a data frame\nYou have seen how you can use squared brackets [ ] and the dollar sign $ to extract parts of your data. Some people find this confusing, so let’s repeat the basic concepts:\n\nsquared brackets are used as follows: data[rowNumber, columnNumber]\nthe dollar sign helps to extract colums with their name (good for readability): data$columnName\nthis syntax can also be used to assign new columns, simply use a new column name and the assign operator: data$newColName &lt;-)\n\n\n\n\n\n\n\nQuestion\n\n\n\nThe following lines of code assess parts of the data frame. Try out what they do and sort the code lines and their meaning:\nWhich of the following commands\n\ndat[2, ]\ndat[, 2]\ndat[, 1]\ndat$Ozone\nnew = dat[, 3] + dat[, 4]\ndat$new = dat[, 3] + dat[, 4]\ndat$NAs = NA\nNA -&gt; dat$NAs \n\nwill get you\n\nget the second row\nget column Ozone\ngenerate a new column with NA’s\ncalculate the sum of columns 3 and 4 and assign to a new column\n\n\n\n\n\nHint: Some of the code lines actually do the same; chose the preferred way in these cases.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nget second row\n\ndat[2, ] is correct\ndat[, 2] gives the second column\n\nget column Ozone\n\ndat$Ozone is the best option\ndat[, 1] gives the same result, but is much harder to understand later on\n\ngenerate a new column with NA’s\n\ndat$NAs = NA is the best option\nNA -&gt; dat$NAs does the same, but the preferred syntax in R is having the new variable on the left hand side (the arrow should face to the left not right)\n\ncalculate the sum of columns 3 and 4 and assign to a new column\n\ndat$new = dat[, 3] + dat[, 4] is correct\nnew = dat[, 3] + dat[, 4] creates a new object but not a new column in the existing data frame",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1C-Exercise.html#filtering-data",
    "href": "1C-Exercise.html#filtering-data",
    "title": "Exercise - R basics",
    "section": "Filtering data",
    "text": "Filtering data\nTo use the data, you must also be able to filter it. For example, we may be interested in hot days in July and August only. Hot days are typically defined as days with a temperature equal or &gt; 30°C (or 86°F as in the dataset here). Imagine, your colleague tried to query the data accordingly. She/he also found a mistake in each of the first 4 rows and wants to exclude these, but she/he is very new to R and made a few common errors in the following code:\n\n# Return only rows where the temperature is exactly is 86\ndat[dat$Temp = 86, ]\n\n# Return only rows where the temperature is equal or larger than 86\ndat[dat$Temp &gt;= 86]\n\n# Exclude rows 1 through 4\ndat[-1:4, ]\n\n# Return only rows for the months 7 or 8\ndat[dat$Month == 7 | 8, ]\n\n\n\n\n\n\n\nQuestion\n\n\n\nCan you fix his/her mistakes? These hints may help you:\n\nrows or columns can be excluded, if the numbers are given as negative numbers\n== means “equals”\n& means “AND”\n| means “OR” (press “AltGr”+“&lt;” to produce |, or “option”+“7” on MacOS)\nexecuting the erroneous code may help you to spot the problem\nrun parts of the code if you don’t understand what the code does\nthe last question is a bit trickier, no problem if you don’t find a solution\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the corrected code:\n\n# Return only rows where the temperature is exactly is 86\ndat[dat$Temp == 86, ]\n\n# Return only rows where the temperature is equal or larger than 86\ndat[dat$Temp &gt;= 86, ]\n\n# Exclude rows 1 through 4\ndat[-(1:4), ]\n\n# Return only rows for the months 7 or 8\ndat[dat$Month == 7 | dat$Month == 8, ]\ndat[dat$Month %in% 7:8, ] # alternative expression\n\n\n\n\n\n\nThe %in% operator is useful when you want to check whether a value is inside a vector or not:\n\n5 %in% c(1, 2, 3, 4, 5)\n## [1] TRUE\n\nWe will discuss the results together.\nWhen you are finished, save your R script!",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1C-Exercise.html#last-task---install-ecodata-package",
    "href": "1C-Exercise.html#last-task---install-ecodata-package",
    "title": "Exercise - R basics",
    "section": "Last task - Install EcoData package",
    "text": "Last task - Install EcoData package\nDuring the course, we will use some datasets that we compiled in the \\(EcoData\\) package. To access the datasets, you need to install the package from github. To do this, you will also need to install the \\(devtools\\) package.\nTry the following code to install the two packages:\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\", \n                         dependencies = F, build_vignettes = F)\nlibrary(EcoData)\n\nRemember that you have to install a package only once. If you open R Studio the next time, it is enough to run library(EcoData).\n\nAlternative ways to get EcoData\nIf the installation didn’t work, download the package file manually from\nhttps://github.com/TheoreticalEcology/ecodata/releases/download/v0.2.1/EcoData_0.2.1.tar.gz\nStore the file on your computer in the same folder where you created your R project. Then run the following code:\n\ninstall.packages(\"EcoData_0.2.1.tar.gz\", \n                 repos = NULL, type = \"source\")\nlibrary(EcoData)\n\nIf this wasn’t successful either, you can download the combined datasets from elearning (see Organisation and every-day material)\nStore the file on your computer in the same folder where you created your R project. Then run the following code:\n\nload(\"EcoData.Rdata\")\n\n(Note that you will not be able to access the dataset descriptions when you use this option).",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "1C-Exercise.html#bonus---advanced-programming",
    "href": "1C-Exercise.html#bonus---advanced-programming",
    "title": "Exercise - R basics",
    "section": "Bonus - Advanced programming",
    "text": "Bonus - Advanced programming\nUntil now we have only learned how to use functions and indexing of data structures. But what are functions?\n\nFunctions\nA functions are self contained blocks of code that do something, for example, the average of a vector is given by:\n\\[\nAverage = \\frac{1}{N} \\sum_{i=1}^N x_i\n\\]\nIn R we can easily calculate the sum over a vector by using the function sum():\n\nvalues = 1:10\nprint(values)\n##  [1]  1  2  3  4  5  6  7  8  9 10\n\n# Average \nsum(values)/length(values)\n## [1] 5.5\n\nTo do that now more easily and in a comprehensive way for many different variables, we can define a function to calculate the mean:\n\naverage = function(x) {\n  average = sum(x)/length(x)\n  return(average)\n}\naverage(values)\n## [1] 5.5\n\nA function consists of: - An expressive name - Arguments function(arg1, arg2, arg3), the arguments can be used to pass the data to the function, or to change the behaviour of the function (see below) - A function body, inside curly brackets { } where the actual magic happens - return(...) what should be returned from the function\nThe advantages: - you can compress big code blocks within one function call - reproducibility, we avoid writing the same code again and again, if we want to change the way how we calculate the average, we have to change it only in one place - clarity, the name of the function can give us a hint about what the function is doing\nArguments\nArguments can be either used to pass data to the function or to change the behaviour of the function. Moreover, you can set default values to the function. If arguments have default values, they do not have to be specified (specifiying means that we have to fill this argument):\n\n# Should NAs be removed or not\naverage = function(x, remove_na) {\n  if(!remove_na) {\n    average = sum(x)/length(x)\n  } else {\n    average = sum(x, na.rm = TRUE)/length(x[complete.cases(x)])\n  }\n  return(average)\n}\n\nvalues = c(5, 4, 3, NA, 5, 2)\n\n# no default option for remove_na, we have to specify it!\naverage(values, remove_na = TRUE)\n## [1] 3.8\n\n# In this case, it is better to set a default option for remova_na:\naverage = function(x, remove_na = TRUE) {\n  if(!remove_na) {\n    average = sum(x)/length(x)\n  } else {\n    average = sum(x, na.rm = TRUE)/length(x[complete.cases(x)])\n  }\n  return(average)\n}\n\naverage(values)\n## [1] 3.8\n\n\n\nif(condition) {  } else { } the if/else statements runs code if a certain condition is true or not. If the condition is true, the first code block {  } is run, if it is false, the second (after the else) is run:\n\nvalues = 1:5\nif(length(values) == 5) {\n  print(\"This vector has length 5\")\n} else {\n  print(\"This vector has not length 5\")\n}\n## [1] \"This vector has length 5\"\n\nArguments are matched by the name or, if names are not specified, by the order:\nfunc(x1, x2, x3) will be interpreted as func(arg1 = x1, arg2 = x2, arg3 = x3)\nBut be careful, if you are unsure about the correct order, you should pass them by their name (func(arg1 = x1, arg2 = x2, arg3 = x3))\n\n\nLoops\nLoops are another important code structure. Example: We want to go over all values of a vector, calculate the square root of it, and overwrite the old value with the new value:\n\nvalues = c(20, 33, 25, 16)\nvalues[1] = sqrt(values[1])\nvalues[2] = sqrt(values[2])\nvalues[3] = sqrt(values[3])\nvalues[4] = sqrt(values[4])\n\nNow what should we do if we have thousands of observations? Loops are the solution! We can use them to automatically “run” a specific vector and then do something with it (well it sounds cryptic but it is actually quite easy):\n\nfor(i in 1:4) { # i in 1:4 means that i should be 1, 2, 3, and 4\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n\n# Let's use it to automatize the previous computation:\nfor(i in 1:4) {\n  values[i] = sqrt(values[i])\n}\nvalues\n## [1] 2.114743 2.396782 2.236068 2.000000\n\n# Even better: do not hardcode the length of the vector:\nfor(i in 1:length(values)) {\n  values[i] = sqrt(values[i])\n}\nvalues\n## [1] 1.454215 1.548154 1.495349 1.414214\n\nOur code will now always work, even if we change the length of the values variable!\n\n\n\n\n\n\nBonus Question\n\n\n\nWrite functions for:\n\nCalculate the sum for all values in a matrix given by (we want to write our own implementation of the internal sum(...) function):\n\nmy_matrix = matrix(1:200, 20, 10)\n\nUse the internal sum(...) function to check whether your function is correct!\nExtend the function with arguments that specify that the sum should be calculate over rows, columns, or both (if we calculate the sum over rows or columns, then a vector with n sums for n rows or n columns should be returned).\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum_matrix function\n\n\n\nsum_matrix = function(X) {\n\n  n_row = nrow(X)\n  n_col = ncol(X)\n  result = 0\n  for(i in 1:n_row) {\n    for(j in 1:n_col) {\n      result = result + X[i,j]\n    }\n  }\n  return(result)\n}\n\n\nsum_matrix_extended function\n\n\n  sum_matrix_extended = function(X, which = \"both\") {\n  if(which == \"both\") {\n    result = sum_matrix(X)\n  } else if(which == \"row\") {\n    result = apply(X, 1, sum)\n  } else if(which == \"row\") {\n    result = apply(X, 2, sum)\n  }\n  return(result)\n}\n\nThe apply(...) function can be used to automatically loop over rows (MARGIN=1) or columns (MARGIN=2) and apply a function on each element (rows or columns) which can be specified via apply(data, MARGIN = 1, FUN = sum)",
    "crumbs": [
      "Data input and manimulation",
      "Exercise - R basics"
    ]
  },
  {
    "objectID": "2A-DescriptiveStatistics.html",
    "href": "2A-DescriptiveStatistics.html",
    "title": "4  Plotting and describing data",
    "section": "",
    "text": "4.1 Plotting ONE variable",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting and describing data</span>"
    ]
  },
  {
    "objectID": "2A-DescriptiveStatistics.html#plotting",
    "href": "2A-DescriptiveStatistics.html#plotting",
    "title": "4  Plotting and describing data",
    "section": "",
    "text": "4.1.1 One variable\n\n4.1.1.1 Numerical variable - Histogram and Boxplot\n\nThe histogram plots the frequency of the values of a numerical variable with bins (otherwise each unique value will appear only once, the range will be cut in n elements). The number of bins is automatically inferred by the function but can be also changed by the user\nThe boxplot plots the distribution of a numerical variable based on summary statistics (the quantiles). The boxplot is particular useful for comparing/contrasting a numerical with a categorical variable (see below)\n\n\nBase Rggplot2\n\n\n\npar(mfrow = c(1,2)) # number of plots, one row, two columns\nhist(iris$Sepal.Length, \n     main = \"Histogram\", # title\n     xlab = \"Sepal.Length\", \n     ylab = \"Frequency\",\n     las = 1) # rotation of x and y values (las = 1, all of them should be horizontal)\n\nboxplot(iris$Sepal.Length,      \n        main = \"Boxplot\", # title\n        ylab = \"Values\")\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nplt1 = \n  ggplot(iris, aes(x = Sepal.Length)) +\n    geom_histogram() +\n    ggtitle(\"Histogram\") +\n    xlab(\"Sepal.Length\") +\n    ylab(\"Frequency\") +\n    theme_bw() # scientific theme (white background)\n\nplt2 = \n  ggplot(iris, aes(y = Sepal.Length)) +\n    geom_boxplot() +\n      ggtitle(\"Boxplot\") +\n      ylab(\"Values\") +\n      theme_bw() \n\n\nggarrange(plt1, plt2, ncol = 2L, nrow = 1L)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.1.2 Categorical variable - Barplot\n\nstr(mtcars)\n## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\ncounts = table(mtcars$cyl)\ncounts\n## \n##  4  6  8 \n## 11  7 14\n\n\nBase Rggplot2\n\n\n\nbarplot(counts, \n        main = \"Barplot of Cyl\",\n        ylab = \"Number of occurrences\",\n        xlab = \"Cyl levels\",\n        col = \"#4488AA\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(mtcars, aes(x = cyl)) +\n  geom_bar(fill = \"#4488AA\") +\n    ggtitle(\"Barplot of Cyl\") +\n    xlab(\"Number of occurrences\") +\n    ylab(\"Cyl levels\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 Two variables\nThe general idea of plotting is to look for correlations / associations between variables, i.e. is there a non-random pattern between the two variables.\n\n4.1.2.1 Numerical vs numerical variable - Scatterplot\n\nBase Rggplot2\n\n\n\n# Scatterplot\npar(mfrow = c(1,2))\nplot(airquality$Solar.R, airquality$Ozone)\n\n# plot(Ozone ~ Solar.R, data = airquality) #the same\n\n# different symbol for each month\nplot(Ozone ~ Solar.R, data = airquality, pch = Month)\n\n\n\n\n\n\n\n\nWe can also add other objects such as lines to our existing plot:\n\npar(mfrow = c(1,1))\nplot(Ozone ~ Solar.R, data = airquality)\nabline(h = 50)\n\n\n\n\n\n\n\n\n\n\n\nplt1 = ggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n        geom_point() +\n        theme_bw()\n\nplt2 = ggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n        geom_point(shape = airquality$Month) +\n        theme_bw()\n\nggarrange(plt1, plt2, ncol = 2L, nrow = 1L)\n## Warning: Removed 42 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n## Removed 42 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n\n\n\n\n\n\n\nWe can also add other objects such as lines to our existing plot:\n\nggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n  geom_point(shape = airquality$Month) +\n  geom_abline(intercept = 50, slope = 0) +\n    theme_bw()\n## Warning: Removed 42 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.2.2 Categorical vs numerical variable - Boxplot\nOften we have a numerical variable (e.g. weight/fitness) and a categorical vairable that tells us the group of the observation (e.g. control or treatment). To compare visually now the distributions of the numerical variable between the levels of the grouping variable, we can use a boxplot\n\nBase Rggplot2\n\n\n\nboxplot(mpg ~ cyl, mtcars, notch=TRUE) # formula notation\n## Warning in (function (z, notch = FALSE, width = NULL, varwidth = FALSE, :\n## alguns entalhes saíram fora das dobradiças ('caixa'): talvez definir\n## notch=FALSE\n\n\n\n\n\n\n\n# boxplot(x = mtcars$cyl, y = mtcars$mpg) # the same\n\n\n\n\nggplot(mtcars, aes(y = mpg, group = cyl)) +\n  geom_boxplot(notch=TRUE) +\n    theme_bw()\n## Notch went outside hinges\n## ℹ Do you want `notch = FALSE`?\n## Notch went outside hinges\n## ℹ Do you want `notch = FALSE`?",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting and describing data</span>"
    ]
  },
  {
    "objectID": "2A-DescriptiveStatistics.html#plotting-one-variable",
    "href": "2A-DescriptiveStatistics.html#plotting-one-variable",
    "title": "4  Plotting and describing data",
    "section": "",
    "text": "4.1.1 Numerical variable - Histogram and Boxplot\n\nThe histogram plots the frequency of the values of a numerical variable with bins (otherwise each unique value will appear only once, the range will be cut in n elements). The number of bins is automatically inferred by the function but can be also changed by the user\nThe boxplot plots the distribution of a numerical variable based on summary statistics (the quantiles). The boxplot is particular useful for comparing/contrasting a numerical with a categorical variable (see below)\n\n\n\nIn the base R code we introduced par() before plotting to create a panel with the plots side by side. In ggplot2 this is done with the package ggpubr.\n\nBase Rggplot2\n\n\n\npar(mfrow = c(1,2)) # number of plots, one row, two columns\nhist(iris$Sepal.Length, \n     main = \"Histogram\", # title\n     xlab = \"Sepal.Length\", \n     ylab = \"Frequency\",\n     las = 1) # rotation of x and y values (las = 1, all of them should be horizontal)\n\nboxplot(iris$Sepal.Length,      \n        main = \"Boxplot\", # title\n        ylab = \"Values\")\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(ggpubr)\n\nplt1 = \n  ggplot(iris, aes(x = Sepal.Length)) +\n    geom_histogram() +\n    ggtitle(\"Histogram\") +\n    xlab(\"Sepal.Length\") +\n    ylab(\"Frequency\") +\n    theme_bw() # scientific theme (white background)\n\nplt2 = \n  ggplot(iris, aes(y = Sepal.Length)) +\n    geom_boxplot() +\n      ggtitle(\"Boxplot\") +\n      ylab(\"Values\") +\n      theme_bw() \n\n\nggarrange(plt1, plt2, ncol = 2L, nrow = 1L)\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 Categorical variable - Barplot\n\nstr(mtcars)\n## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\ncounts = table(mtcars$cyl)\ncounts\n## \n##  4  6  8 \n## 11  7 14\n\n\nBase Rggplot2\n\n\n\nbarplot(counts, \n        main = \"Barplot of Cyl\",\n        ylab = \"Number of occurrences\",\n        xlab = \"Cyl levels\",\n        col = \"#4488AA\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(mtcars, aes(x = cyl)) +\n  geom_bar(fill = \"#4488AA\") +\n    ggtitle(\"Barplot of Cyl\") +\n    xlab(\"Number of occurrences\") +\n    ylab(\"Cyl levels\") +\n    theme_bw()",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting and describing data</span>"
    ]
  },
  {
    "objectID": "2A-DescriptiveStatistics.html#plotting-two-variables",
    "href": "2A-DescriptiveStatistics.html#plotting-two-variables",
    "title": "4  Plotting and describing data",
    "section": "4.2 Plotting TWO variables",
    "text": "4.2 Plotting TWO variables\nThe general idea of plotting is to look for correlations / associations between variables, i.e. is there a non-random pattern between the two variables.\n\n4.2.1 Numerical vs numerical variable - Scatterplot\n\nBase Rggplot2\n\n\n\n# Scatterplot\npar(mfrow = c(1,2))\nplot(airquality$Solar.R, airquality$Ozone)\n\n# plot(Ozone ~ Solar.R, data = airquality) #the same\n\n# different symbol for each month\nplot(Ozone ~ Solar.R, data = airquality, pch = Month)\n\n\n\n\n\n\n\n\nWe can also add other objects such as lines to our existing plot:\n\npar(mfrow = c(1,1))\nplot(Ozone ~ Solar.R, data = airquality)\nabline(h = 50)\n\n\n\n\n\n\n\n\n\n\n\nplt1 = ggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n        geom_point() +\n        theme_bw()\n\nplt2 = ggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n        geom_point(shape = airquality$Month) +\n        theme_bw()\n\nggarrange(plt1, plt2, ncol = 2L, nrow = 1L)\n## Warning: Removed 42 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n## Removed 42 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n\n\n\n\n\n\n\nWe can also add other objects such as lines to our existing plot:\n\nggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n  geom_point(shape = airquality$Month) +\n  geom_abline(intercept = 50, slope = 0) +\n    theme_bw()\n## Warning: Removed 42 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.2 Categorical vs numerical variable - Boxplot\nOften we have a numerical variable (e.g. weight/fitness) and a categorical vairable that tells us the group of the observation (e.g. control or treatment). To compare visually now the distributions of the numerical variable between the levels of the grouping variable, we can use a boxplot\n\nBase Rggplot2\n\n\n\nboxplot(mpg ~ cyl, mtcars, notch=TRUE) # formula notation\n## Warning in (function (z, notch = FALSE, width = NULL, varwidth = FALSE, :\n## alguns entalhes saíram fora das dobradiças ('caixa'): talvez definir\n## notch=FALSE\n\n\n\n\n\n\n\n# boxplot(x = mtcars$cyl, y = mtcars$mpg) # the same\n\n\n\n\nggplot(mtcars, aes(y = mpg, group = cyl)) +\n  geom_boxplot(notch=TRUE) +\n    theme_bw()\n## Notch went outside hinges\n## ℹ Do you want `notch = FALSE`?\n## Notch went outside hinges\n## ℹ Do you want `notch = FALSE`?",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Plotting and describing data</span>"
    ]
  }
]