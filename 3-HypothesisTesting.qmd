---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
set.seed(42)
```

# Hypothesis Tests

In this section you will:

-   get to know the most common hypothesis tests
-   learn how to choose an appropriate test and interpret its result
-   practice these tests in R
-   practice the simulation of data and error rates

### 

## A recipe for hypothesis testing

**Aim:** We want to know if there is a difference between the control and the treatment

1.  We introduce a **Null hypothesis H~0~** (e.g. no effect, no difference between control and treatment)
2.  We invent a **test statistic**
3.  We calculate the **expected distribution** of our test statistic given H~0~ (this is from our data-generating model)
4.  We calculate the **p-value** = probability of getting observed test statistic or more extreme given that H~0~ is true (there is no effect): $p-value = P(d\ge D_{obs} | H_0)$

::: {.callout-warning collapse="true"}
## Interpretation of the p-value

p-values make a statement on the probability of the data or more extreme values given H~0~ (no effect exists), **but not on the probability of H~0~ (no effect) and not on the size of the effect or on the probability of an effect!**
:::

**Example:**

Imagine we do an experiment with two groups, one treatment and one control group Test outcomes are binary (e.g. whether individuals are cured)

1.  We need a test statistic for example: number cured of total patients: treat/(treat+control)

2.  We need the distribution of this test statistic under the null Let's create a true world without effect:

```{r}
set.seed(123)
# Let's create a true world without effect:
PperGroup = 50 # number of replicates (e.g., persons per treatment group)
pC = 0.5 #probability of being cured in control group
pT = 0.5 #probability of being cured in treatment group; = same because we want to use these to get the distribution of the test statistic we define below when H0 is true (no effect)

# Let's draw a sample from this world without effect
control = rbinom(n = 1, size = PperGroup, prob = pC)
treat = rbinom(n = 1, size = PperGroup, prob = pT)
#calculate the test statistic: 
treat/(treat+control)
#and plot
barplot(c(control, treat), ylim = c(0, 50))
```

3.  Now, let's do this very often to get the distribution under H~0~

```{r}
testStatistic = rep(NA, 100000) 
for (i in 1:100000) {
  control = rbinom(n = 1, size = PperGroup, prob = pC)
  treat = rbinom(n = 1, size = PperGroup, prob = pT)
  testStatistic[i] = control/(treat+control) # test statistic 
}
hist(testStatistic, breaks = 50)
```

4.  We have now our test statistic + the frequency distribution of our statistic if the H~0~ = true now we make an experiment: Assume that we observed the following data: C = 30 T = 23

```{r}
hist(testStatistic, breaks = 50)
testStatisticData = 30/(30+23)
abline(v = testStatisticData, col = "red")

mean(testStatistic > testStatisticData)
#compare each value in our testStatistic 
#distribution with the observed value and calculate proportion of TRUE values 
#(where testStatistic > testStatisticData)
```

But we know actually that the test statistic follows a Chi^2^ distribution. So to get correct p-values we can use the `prop.test` for this test statistic:

```{r}
prop.test(c(30, 23), c(PperGroup, PperGroup))
# other test statistic with known distribution
# Pearson's chi-squared test statistic
# no need to simulate
```

We pass the data to the function which first calculates the test statistic and then calculates the p-value using the Chi^2^ distribution.

## T-test

Originally developed by Wiliam Sealy Gosset (1876-1937) who has worked in the Guinness brewery. He wanted to measure which ingredients result in a better beer. The aim was to compare two beer recipes and decide whether one of the recipes was better (e.g. to test if it results in more alcohol). He published under the pseudonym 'Student' because the company considered his statistical methods as a commercial secret.

::: callout-important
## T-test assumptions

-   Data in both groups is normally distributed

-   H~0~ : the means of both groups are equal
:::

The idea is that we have two normal distributions (e.g. alcohol distributions):

```{r}
#| code-fold: true
set.seed(1)
A = rnorm(100, mean = -.3)
B = rnorm(100, mean = .3)
plot(density(A), col = "red", xlim = c(-2, 2), ylim = c(0, 0.6))
lines(density(B))
abline(v = mean(A), col = "red")
abline(v = mean(B))
```

And our goals is know to test if the difference between the two means of the variables is statistically significant or not.

**Procedure:**

-   Calculate variances and means of both variables

    ```{r}
    A_m = mean(A)
    B_m = mean(B)
    A_v = var(A)
    B_v = var(B)
    ```

-   Calculate t-statistic (difference between means / (Standard deviation/sample size)

    ```{r}
    t_statistic = (A_m - B_m) / sqrt( A_v  / length(A) + B_v / length(B))
    t_statistic
    ```

-   Compare observed t with t distribution under H~0~ (which we can do by using the CDF function of the t-distribution:

    ```{r}
    pt( t_statistic,  # test statistic
       df = length(A)+length(B)-2, # degrees of freedom, roughly = n_obs - n_parameters
       lower.tail = TRUE
      )*2
    ```

::: callout-caution
## One-sided or two-sided

If we do NOT know if the dataset from one group is larger or smaller than the other, we must use two-sided tests (that's why we multiply the p-values with 2). Only if we are sure that the effect MUST be positive / negative, we can test for greater / less. Decide BEFORE you look at the data!
:::

Let's compare it to the output of the `t.test` function which does everything for us, we only need to pass the data to the function:

```{r}
t.test(A, B, var.equal = TRUE)
```

Usually we also have to test for normality of our data, which we can do with another test.

**Example airquality**

```{r}
# with real data
head(PlantGrowth)
boxplot(weight ~ group, data = PlantGrowth)

ctrl = PlantGrowth$weight[PlantGrowth$group == "ctrl"]
trt1 = PlantGrowth$weight[PlantGrowth$group == "trt1"]

# attention: t test assumes normal dirstribution of measurements in both groups!
# test normality before doing the t test:
shapiro.test(ctrl)
shapiro.test(trt1)


t.test(ctrl, trt1)
# note that this is a "Welch" t-test
# we will have a look at the differences among t-tests in the next large exercise

# What is H0? equal means
# What is the result? test is not significant, H0 is not rejected
# Explain the different values in the output!

```

::: callout-warning
## Shapiro - Test for normality

If you have a small sample size, the shapiro.test will always be non-significant (i.e. not significantly different from a normal distribution)! This is because small sample size leads to low power for rejecting H~0~ of normal distribution
:::

## Type I error rate

Let's start with a small simulation example:

```{r}
results = replicate(1000, {
  A = rnorm(100, mean = 0.0)
  B = rnorm(100, mean = 0.0)
  t.test(A, B)$p.value
})
hist(results)
```

What's happening here? We have no effect in our simulation but there are many p-values lower than $\alpha = 0.05$:

```{r}
mean(results < 0.05)
```

So in `r mean(results < 0.05)` of our experiments we would reject H~0~ even when there is no effect at all! This is called the type I error rate. Those are false positives.

::: {.callout-note collapse="true"}
## Type I error rate and multiple testing

If there is no effect, the probability of having a positive test result is equal to the significance level $\alpha$. If you test 20 things that don't have an effect, you will have one significant result on average when using a significance level of 0.05. If multiple tests are done, a correction for multiple testing should be used.

This problem is called multiple testing

-   e.g.: if you try 20 different analyses (Null hypotheses), on average one of them will be significant.

-   e.g.: if you test 1000 different genes for their association with cancer, and in reality, none of them is related to cancer, 50 out of the tests will still be significant.

-   If multiple tests are done, a correction for multiple testing should be used

-   increases the p-values for each test in a way that the overall alpha level is 0.05

```{r}
# conduct a t-test for each of the treatment combinations
# save each test as a new object (test 1 to 3)
control = PlantGrowth$weight[PlantGrowth$group == "ctrl"]
trt1 = PlantGrowth$weight[PlantGrowth$group == "trt1"]
trt2 = PlantGrowth$weight[PlantGrowth$group == "trt2"]

test1 = t.test(control, trt1)
test2 = t.test(control, trt2)
test3 = t.test(trt1, trt2)

c(test1$p.value, test2$p.value, test3$p.value)

# now adjust these values
p.adjust(c(test1$p.value, test2$p.value, test3$p.value)) # standard is holm, average conservative
p.adjust(c(test1$p.value, test2$p.value, test3$p.value), method = "bonferroni") # conservative
p.adjust(c(test1$p.value, test2$p.value, test3$p.value), method = "BH") # least conservative
# for details on the methods see help
```
:::

If multiple testing is a problem and if we want to avoid false positives (type I errors), why don't we use a smaller alpha level? Because if would increase the type II error rate



## Type II error rate

It can also happen the other way around:

```{r}
results = replicate(1000, {
  A = rnorm(100, mean = 0.0)
  B = rnorm(100, mean = 0.2) # effect is there
  t.test(A, B)$p.value
})
hist(results)
```

```{r}
mean(results < 0.05)
```

No we wouldn't reject the H~0~ in `r 1-mean(results < 0.05)`% of our experiments. This is the type II error rate (false negatives).

The type II error rate ($\beta$) is affected by

-   sample size $\uparrow$ , decreases $\beta$
-   true effect size $\uparrow$, decreases $\beta$
-   $\alpha$ $\uparrow$, decreases $\beta$
-   variability (variance) $\uparrow$, increases $\beta$

After the experiment, the only parameter we could change would be the significance level $\alpha$, but increasing it would result in too high Type I error rates.

## Statistical power

We can reduce $\alpha$ and we will get fewer type I errors (false positives), but type II errors (false negatives) will increase. So what can we do with this in practice?

1- $\beta$ is the so called statistical power which is the rate at which a test is significant if the effect truly exists. Power increases with stronger effect, smaller variability, (larget $\alpha$ ), and more data (sample size). So, collect more data? How much data do we need?

Before the experiment, you can estimate the effect size and the variability. Together with alpha (known), you can calculate the power depending on the sample size:

```{r}
#| code-fold: true
results = 
  sapply(seq(10, 500, by = 20), function(n) {
    results = replicate(100, {
      A = rnorm(n, mean = 0.0)
      B = rnorm(n, mean = 0.2) # effect is there
      t.test(A, B)$p.value
    })
    power = 1 - mean(results > 0.05)
    return(power)
  })
plot(seq(10, 500, by = 20), results, xlab = "Sample size", ylab = "Power", main = "")
```

We call that a power analysis and there's a function in R to do that:

```{r}
power.t.test(n = 10, delta = 1, sd = 1, type = "one.sample")

# Power increases with sample size (effect size constant, sd constant):
pow <- function(n) power.t.test(n, delta = 1, sd = 1, type = "one.sample")$power
plot(1:20, sapply(1:20, pow), xlab = "Sample size", ylab = "Power", pch = 20)

# Power increases with effect size
pow <- function(d) power.t.test(n = 20, delta = d, sd = 1, 
                                type = "one.sample")$power
plot(seq(0,1,0.05), sapply(seq(0,1,0.05), pow), xlab = "Effect size", 
     ylab = "Power", pch = 20)

# Power decreases with increasing standard deviation (or variance):
pow <- function(s) power.t.test(n = 20, delta = 1, sd = s, 
                                type = "one.sample")$power
plot(seq(0.5,1.5,0.05), sapply(seq(0.5,1.5,0.05), pow), 
     xlab = "Standard deviation", ylab = "Power", pch = 20)
```


## False discovery rate

You may have realized that if we do an experiment with a (weak) effect, we can get a significant result because of the effect but also significant results because of the Type I error rate. How to distinguish between those two? How can we decide whether a significant result is a false positive? This error rate is called the false discovery rate and to lower it we need to increase the power:

$$
FDR = \frac{p(H_0)\cdot\alpha}{p(H_0)\cdot\alpha + p(!H_0)\cdot(1-\beta)}
$$

$p(H_0)$ = probability of H~0~ (no effect); $p(!H_0)$ = probability of not H~0~ (effect exists). Both are unknown and the only parameters we can influence is $\alpha$ and $\beta$ . But decreasing $\alpha$ leads to too high false negatives, so $\beta$ is left.


## Statistical tests

The following three sections provide an overview over the most important hypothesis tests, a guideline to select an appropriate test (see decision tree) and the necessary code to apply these tests in R. It is not necessary to read and understand every detail. These explanations are also meant as an advice if you want to select an appropriate hypothesis test after the course. Note that some of the examples use simulated data instead of real observation (all functions that start with r=random, e.g. rnorm() or runif()). Simulated data is useful, because then we know the true distribution and its true mean.

In the last section at the end of this file, you find exercises that you should solve using the explanations above. It may be helpful to use the table of content and/or the search option to find the respective example in the explanations.

## Comparison of mean of two or more groups

Many tests aim at showing that variables are significantly different between groups, i.e. have different means/medians. In all these tests, H0 is that there is no difference between the groups. The following decision tree helps to select the appropriate test.

<!-- ![Fig. 1. Decision tree for the comparison of two groups](Decision%20tree.jpg) -->

**Remark 1**: Tests for 2 groups also work for one group only. Then they test whether the mean is equal to 0.

**Remark 2**: Paired / unpaired: this means that observations in the groups are linked to each other. An example for unpaired data is a typical experiment with 10 observations in the control group and 10 observations in the treatment group. An example for paired data is when the same individuals were exposed to the treatment and to the control. The observations of each individual would belong together (pairs).

**Remark 3**: Parametric: assumption of normal distribution. Non-parametric = no assumption for the distribution.

**Remark 4**: Blue text: If a test for more than two groups is significant, post-hoc tests are carried out in a second step. These check all possible comparisons of groups for significant differences by adjusting p-values for multiple testing.

### Tests for 2 groups

#### t-Test

The t-test can draw conclusions about the mean(s) of 1 or 2 normally-distributed groups.

```{r}
## Classical example: Student's sleep data
plot(extra ~ group, data = sleep)
```

**Be aware**: The line in the box plot does not show the mean but the median.

```{r}
## Formula interface
t.test(extra ~ group, data = sleep)
```

This output tells us, that the difference in means between the 2 groups is not significant(p-value â‰¥ 0.05, specifically: p-value = 0.07939), provided that our significance level is 0.05.\
The underlying Null-hypothesis is that the true difference in means is equal to 0. In the last two lines of the output you can see the means of the respective groups. Even though the means seem to be quite different, the difference is not significant, this could be due to the small sample size of only 10 students per group.

Let's look at different settings of the t-test:

##### t-test, H0: one group, mean = 0

The Null-hypothesis here is that the mean of the observed group is equal to 0.

```{r}
x = rnorm(20, mean = 2)
t.test(x)
```

p-value \< 0.05 means we can reject the Null-hypothesis, i.e. the mean of the observed group is significantly different from 0.

##### t-test, H0: two groups, equal means, equal variances

The Null-hypothesis here is that the two observed groups have the same mean and the same variance (specified by the argument *var.equal = T*).

```{r}
x1 = rnorm(20, mean = 2)
x2 = rnorm(20, mean = 3)
t.test(x1,x2, var.equal = T)
```

##### t-test, H0: two groups, equal means, variable variance

The Null-hypothesis here is that the two observed groups have the same mean and variable variances (the default setting of the argument *var.equal = F*).

```{r}
x1 = rnorm(20, mean = 2, sd = 1)
x2 = rnorm(20, mean = 3, sd = 2)
t.test(x1,x2)
```

##### t-test, H0: two groups, equal means, variance can be different (can also set to equal)

The Null-hypothesis here is that the two groups are paired observations (e.g. group 1 before treatment and group 2 after treatment) have the same mean and variable variance (specified by the argument *var.equal = F*, which is also the default setting).

```{r}
x1 = rnorm(20, mean = 2)
x2 = rnorm(20, mean = 3)
t.test(x1,x2, paired = T, var.equal = F)
```

#### Wilcoxon Rank Sum and Mann-Whitney U Test

In R, there is only one function for both tests together: wilcox.test(). The Wilcoxon rank sum test with (paired = F) is classically called Mann-Whitney U test.

##### Mann-Whitney U Test

```{r}
x1 = rnorm(20, mean = 2)
x2 = rlnorm(20, mean = 3)

wilcox.test(x1, x2)

```

##### Wilcoxon signed rank test

```{r}
x1 = rnorm(20, mean = 2)
x2 = rlnorm(20, mean = 3)

wilcox.test(x1, x2, paired = T)

```

### Tests for \> 2 groups

#### Anova, unpaired

H0 \>2 groups, normal distribution, equal variance, equal means, unpaired

```{r}
x = aov(weight ~ group, data = PlantGrowth)
summary(x)
```

An ANOVA only tests, if there is a difference, but not between which groups. To perform pairwise comparisons, you can use post-hoc tests. Common for ANOVA results is

```{r}
TukeyHSD(x)
```

Alternatively, you can also perform several tests each comparing two groups and then correct for multiple testing. This is what we did before.

Pairwise comparisons are often visualized using different letters to significantly different groups:

```{r, message=FALSE}
# install.packages("multcomp")
library(multcomp)
tuk = glht(x, linfct = mcp(group = "Tukey")) #performs Tukey pairwise comparisons
tuc.cld = cld(tuk) # assigns different letters to significantly different groups

old.par = par(mai = c(1, 1, 1.25, 1), no.readonly = T)
plot(tuc.cld) # draws boxplot + letters from cld function
par(old.par)
```

#### Anova, paired

aov is not good in doing repeated = paired ANOVA. In simple cases, you can just subtract the paired groups. In general, you should use so-called mixed models!

#### Kruskal-Wallis

Non-parametric test for differences in the mean of \>2 groups, unpaired

```{r}
boxplot(Ozone ~ Month, data = airquality)
kruskal.test(Ozone ~ Month, data = airquality)
```

#### Friedmann Test

Non-parametric test for differences in the mean of \>2 groups, paired.

```{r}
wb <- aggregate(warpbreaks$breaks,
                by = list(w = warpbreaks$wool,
                          t = warpbreaks$tension),
                FUN = mean)
#wb
friedman.test(wb$x, wb$w, wb$t)
# Alternative: friedman.test(x ~ w | t, data = wb)
# Note that x is the response, w is the group, and t are the blocks that are paired
```

## Comparison of variances

H0 in variance tests is always that the variances are equal.

### F-Test for two normally-distributed samples

```{r}
x <- rnorm(50, mean = 0, sd = 2)
y <- rnorm(30, mean = 1, sd = 1)
var.test(x, y)                  # Do x and y have the same variance? - Significantly different
```

### Bartlett test for more than two normally-distributed samples

```{r}
x <- rnorm(50, mean = 0, sd = 1)
y <- rnorm(30, mean = 1, sd = 1)
z <- rnorm(30, mean = 1, sd = 1)
bartlett.test(list(x, y, z))                # Do x, y and z have the same variance? - Not sigificantly different
```

## Comparison of discrete proportions

Discrete proportions are typically analyzed assuming the binomial model (k/n with probability p)

### Exact Binomial Test

H0 is that the data are binomially distributed with a fixed probability p.

```{r}
## Conover (1971), p. 97f.
## Under (the assumption of) simple Mendelian inheritance, a cross
##  between plants of two particular genotypes produces progeny 1/4 of
##  which are "dwarf" and 3/4 of which are "giant", respectively.
##  In an experiment to determine if this assumption is reasonable, a
##  cross results in progeny having 243 dwarf and 682 giant plants.
##  If "giant" is taken as success, the null hypothesis is that p =
##  3/4 and the alternative that p != 3/4.
binom.test(c(682, 243), p = 3/4)
binom.test(682, 682 + 243, p = 3/4)   # The same.
## => Data are in agreement with H0
```

### Test of Equal or Given Proportions

based on Chi-squared-test, H0 is that the data in two groups are binomially distributed with the same probability p.

```{r}
## Data from Fleiss (1981), p. 139.
## H0: The null hypothesis is that the four populations from which
##     the patients were drawn have the same true proportion of smokers.
## A:  The alternative is that this proportion is different in at
##     least one of the populations.
smokers  <- c( 83, 90, 129, 70 )
patients <- c( 86, 93, 136, 82 )
prop.test(smokers, patients)
##  => Data are not in agreement with H0
```

### Contingency tables

Chi-squared-test for count data, H0 is that the joint distribution of the cell counts in a 2-dimensional contingency table is the product of the row and column marginals

```{r}
## From Agresti(2007) p.39
M <- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))
dimnames(M) <- list(gender = c("F", "M"),
                    party = c("Democrat","Independent", "Republican"))
chisq.test(M)
```

## Distribution tests

Often we are interested in the distribution of a variable. This can be tested with distribution tests. All these tests are defined as follows: H0 is that the data follow a specific distribution. So in case H0 is rejected, the data significantly deviates from the specified distribution.

Often, we want to know whether a variable is normally distributed because this is an important assumption for parametric hypothesis tests. But data can follow many other distributions:

<!-- ![Fig. 2. Decision tree for distributions](distributions.png) -->

### Shapiro-Wilk Normality Test

Because many tests require normal distribution, this is the test needed most often.

```{r}
shapiro.test(rnorm(100, mean = 5, sd = 3))
```

### Kolmogorov-Smirnov Test

For everything else, the KS test can be used. It compares two different distributions, or a distribution against a reference.

```{r}
x <- rnorm(50)
y <- runif(30)
# Do x and y come from the same distribution?
ks.test(x, y)

# Does x come from a shifted gamma distribution with shape 3 and rate 2?
ks.test(x+2, "pgamma", 3, 2) # two-sided, exact
ks.test(x+2, "pgamma", 3, 2, exact = FALSE)
ks.test(x+2, "pgamma", 3, 2, alternative = "gr")
```

For an overview on distribution see here: http://www.stat.umn.edu/geyer/old/5101/rlook.html

## Other tests

### Correlation

A test for the significance of a correlation:

```{r}
cor.test(airquality$Ozone, airquality$Wind)
```

Interpretation: Ozone and Wind are significantly negatively correlated with a p-value \< 0.05 and a correlation coefficient of -0.6015465.

### Mantel test

The Mantel test compares two distance matrices

```{r message=FALSE}
library(vegan)
## Is vegetation related to environment?
data(varespec)
data(varechem)
veg.dist <- vegdist(varespec) # Bray-Curtis
env.dist <- vegdist(scale(varechem), "euclid")
mantel(veg.dist, env.dist)
mantel(veg.dist, env.dist, method="spear")
```

## Exercises

### Streams

The dataset 'streams' contains water measurements taken at different location along 16 rivers: 'up' and 'down' are water quality measurements of the same river taken before and after a water treatment filter, respectively. We want to find out if this water filter is effective. Use the decision tree to identify the appropriate test for this situation.

```{r}
dat = read.table("https://raw.githubusercontent.com/biometry/APES/master/Data/Simone/streams.txt", header = T)
```

Visualize and analyze the data and answer the following questions at **elearning-extern**.

1.  For identifying an appropriate test for the effect of the water treatment filter, what are your first two choices in the decision tree?
2.  The next decision you have to make is whether you can use a parametric test or not. Apply the Shapiro-Wilk test to check if the data are normally distributed. Are the tests significant and what does that tell you?
3.  Which test is appropriate for evaluating the effect of the filter?
4.  Does the filter influence the water quality? (The warnings are related to identical values, i.e. ties, and zero differences; we ignore these here)

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

You can visualize the data as follows:

```{r}
dat = read.table("https://raw.githubusercontent.com/biometry/APES/master/Data/Simone/streams.txt", header = T)
par(mfrow = c(1, 2))
boxplot(dat)
matplot(t(dat), type = "l") # each line is one river, left is down and right is upstream
par(mfrow = c(1, 1))
```

1.  The number of groups to compare is **two**, up versus down stream. The observations are **paired** because the water tested up and down stream of the filter is not independent from each other, i.e. the "same" water is measured twice!
2.  The Shapiro-Wilk test is significant (p \< 0.05) for down stream data, i.e. we reject H0 (the data is normally distributed). Thus, the data significantly deviate from a normal distribution. The test is not significant for upstream data; the data does not significantly deviate from a normal distribution.

```{r}
shapiro.test(dat$down)
shapiro.test(dat$up)
```

3.  We select a **Wilcoxon signed rank test** that is appropriate to compare not-normal, paired observations in two groups.
4.  H0 of the Wilcoxon signed rank test is that the location shift between the two groups equals zero, i.e. the difference between the pairs follows a symmetric distribution around zero. As p \< 0.05, we can reject H0. The filter significantly influences water quality. (In case of ties also see the function wilcox.test() in the package coin for exact, asymptotic and Monte Carlo conditional p-values)

```{r}
wilcox.test(dat$down, dat$up, paired = T)
```
:::

### Chicken

The 'chickwts' experiment was carried out to measure and compare the effectiveness of various feed supplements on the growth rate of chickens. We are interested in two questions: Does the feed type influence the chickens weight at all? Which feed types result in significantly different chicken weights?

```{r}
dat = chickwts
```

Analyze the data and answer the following questions at **elearning-extern**.

1.  Visualize the data. What is an appropriate plot for this kind of data?
2.  Can you apply an ANOVA to this data? What are the assumptions for an ANOVA? Remember: you have to test two things for the groups (for this exercise it is enough if you test the groups "casein" and "horsebean" only).
3.  Apply an ANOVA or the non-parametric test. How would you describe the result in a thesis or publication?
4.  Also apply the alternative test and compare p-values. Which of the tests has a higher power?
5.  Use the result of the ANOVA to carry out a post-hoc test. How many of the pairwise comparisons indicate significant differences between the groups?
6.  Which conclusion about the feed types 'meatmeal' and 'casein' is correct?

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

1.  An appropriate visualization for one numeric and one categorial variable is a **boxplot**. Using 'notch = T' in the function boxplot(), adds confidence interval for the median (the warning here indicates that we are not very confident in the estimates of the medians as the number of observations is rather small, you can see at the notches that go beyond the boxes).

```{r}
dat = chickwts
boxplot(weight ~ feed, data = dat)
boxplot(weight ~ feed, data = dat, notch = T)
```

2.  The two requirements for applying an ANOVA are 1) the data in each group are normally distributed, and 2) the variances of the different groups are equal. For 1) we again use a Shapiro-Wilk test. For 2) we can use the function var.test() or for all feed types the function bartlett.test(). All tests are not significant, and we thus have no indication to assume that the data is not-normally distributed or that the variances are different. We can use an ANOVA.

```{r}
# get data of each group
casein = dat$weight[dat$feed == "casein"]
horsebean = dat$weight[dat$feed == "horsebean"]

shapiro.test(casein)
shapiro.test(horsebean)
# H0 normally distributed
# not rejected, normality assumption is okay

var.test(casein, horsebean)
# H0 ratio of variances is 1 = groups have the same variance
# not rejected, same variances is okay


### Extra: testing the assumptions for all groups:

# Normality test using the dplyr package
library(dplyr)
dat %>% 
  group_by(feed) %>% 
  summarise(p = shapiro.test(weight)$p)

# Bartlett test for equal variances
bartlett.test(weight ~ feed, dat)

```

3.  H0 of the ANOVA is that feed has no influence on the chicken weight. As p \< 0.05, we reject H0. In the result section, we would write something like: "The feed type significantly influenced the chicken weight (ANOVA, p = 5.94e-10)."

```{r}
fit = aov(weight ~ feed, data = dat)
summary(fit)
```

4.  The non-parametric alternative of an ANOVA is the Kruskal-Wallis test, which should be applied if the data is not normally distributed. In this example, the test comes to the same conclusion: H0 is rejected, the feed type has a significant effect on the chicken weight. The p-value, however, is not as small as in the ANOVA. The reason for this is that non-parametric tests have a lower power than parametric ones as they only use the ranks of the data. Therefore, the ANOVA is preferred over the non-parametric alternative in case its assumptions are fulfilled.

```{r}
kruskal.test(chickwts$weight, chickwts$feed)
```

5.  From the 15 comparisons among feed types, 8 are significantly different.

```{r}
TukeyHSD(fit)
```

6.  The experiment did not reveal a significant weight difference between the feed types 'meatmeal' and 'casein'. Remember that we cannot prove or accept H0; we can only reject it.
:::

### Titanic

The dataset 'titanic' from the EcoData package (not to confuse with the dataset 'Titanic') provides information on individual passengers of the Titanic.

```{r echo=TRUE,message=FALSE, warning=FALSE,results='hide'}
library(EcoData) #or: load("EcoData.Rdata"), if you had problems with installing the package
dat = titanic
```

Answer the following questions at **elearning-extern**.

1.  We are interested in first and second class differences only. Reduce the dataset to these classes only. How can you do this in R?
2.  Does the survival rate between the first and second class differ? Hint: you can apply the test to a contigency table of passenger class versus survived, i.e. `table(dat$pclass, dat$survived)`.
3.  Is the variable passenger age normally distributed?
4.  Is the variable Body Identification Number (body) uniformly distributed?
5.  Is the correlation between fare and age significant?

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

1.  The dataset can be reduced in different ways. All three options result in a dataset with class 1 and 2 only.

```{r echo=TRUE,message=FALSE, warning=FALSE,results='hide'}
library(EcoData)
dat = titanic

dat = dat[dat$pclass == 1 | dat$pclass == 2, ]
dat = dat[dat$pclass %in% 1:2, ] # the same
dat = dat[dat$pclass != 3, ] # the same
```

2.  We use the test of equal proportions here. H0, proportions in the two groups are equal, is rejected. The survival probability in class 1 and class 2 is significantly different. Note that the estimated proportions are for mortality not for survival because 0=died is in the first column of the table. Thus it is considered the "success" in the prop.test().

```{r}
table(dat$pclass, dat$survived)
prop.test(table(dat$pclass, dat$survived))
```

3.  The distribution of passenger age significantly differs from normal.

```{r}
hist(dat$age, breaks = 20)
shapiro.test(dat$age)
```

4.  The distribution of body significantly differs from uniform.

```{r}
hist(dat$body, breaks = 20)
ks.test(dat$body, "punif")
```

5.  The correlation between fare and age is non-significant. You can also plot the data using the scatter.smooth function.

```{r}
cor.test(dat$fare, dat$age)
scatter.smooth(dat$fare, dat$age)
```
:::

### Simulation of Type I and II error

*This is an additional task for those who are fast! Please finish the other parts first and submit your solution in elearning-extern before you continue here!*

Analogously to the previous example of simulating the test statistic, we can also simulate error rates. Complete the code ...

```{r}
PperGroup = 50
pC = 0.5
pT = 0.5

pvalues = rep(NA, 1000)

for(i in 1:1000){
  control = rbinom(n = 1, size = PperGroup, prob = pC)
  treat = rbinom(n = 1, size = PperGroup, prob = pT)
  #XXXX
}

```

... and answer the following questions for the prop.test in R:

1.  How does the distribution of p-values and the number of false positive (Type I error) look like if pC = pT

2.  How does the distribution of p-values and the number of true positive (Power) look like if pC != pT, e.g. 0.5, 0.6

3.  How does the distribution of p-values and the number of false positive (Type I error) look like if you modify the for loop in a way that you first look at the data, and then decide if you test for greater or less?

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

Analogously to our previous example with simulating the test statistic, we can also simulate the error rates. This is the completed code the different examples:

1.  pC = pT

```{r}
PperGroup = 50
pC = 0.5
pT = 0.5

pvalues = rep(NA, 1000)
positives = rep(NA, 1000)

for(i in 1:1000){
  control = rbinom(1, PperGroup, prob = pC )
  treatment = rbinom(1, PperGroup, prob = pT )
  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value
  positives[i] = pvalues[i] <= 0.05
}
hist(pvalues)
table(positives)
mean(positives) 

# type I error rate = false positives (if data simulation etc. is performed several times, this should be on average 0.05 (alpha))

```

2.  pC != pT with difference 0.1

```{r}
PperGroup = 50
pC = 0.5
pT = 0.6

pvalues = rep(NA, 1000)
positives = rep(NA, 1000)

for(i in 1:1000){
  control = rbinom(1, PperGroup, prob = pC )
  treatment = rbinom(1, PperGroup, prob = pT )
  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value
  positives[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value < 0.05
}
hist(pvalues)
table(positives)
mean(pvalues < 0.05) # = power (rate at which effect is detected by the test)
# power = 1- beta > beta = 1-power = typeII error rate
1-mean(pvalues < 0.05)

## Factors increasing power and reducing type II errors:
# - increase sample size
# - larger real effect size (but this is usually fixed by the system)

```

3.  You first look at the data, and then decide if you test for greater or less:

```{r}
# ifelse(test,yes,no)
PperGroup = 50
pC = 0.5
pT = 0.5

for(i in 1:1000){
  control = rbinom(1, PperGroup, prob = pC )
  treatment = rbinom(1, PperGroup, prob = pT )
  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2), 
                        alternative= ifelse(mean(control)>mean(treatment),
                                            "greater","less"))$p.value
  positives[i] = prop.test(c(control, treatment), rep(PperGroup, 2),
                     alternative= ifelse(mean(control)>mean(treatment),
                                         "greater","less"))$p.value < 0.05
}
hist(pvalues)
table(positives)
mean(pvalues < 0.05) 
# higher false discovery rate
```
:::
