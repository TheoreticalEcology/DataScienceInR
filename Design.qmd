---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Design of experiments

## What do do about confounding variables

If we think there is a factor that could be confounding, we basically have three options

-   Best: control the value of these factors. Either fix the value (preferred if we are not interested in this factor), else vary the value in a controlled way (see below).
-   Second best: randomize and measure them
-   Third best: only randomize or only measure them

Randomization means that we try to ensure that the confounding factor is not systematically correlated with the variable of interest (but can still cause problems with interactions and nonlinear relationships).

Measuring allows us to account for the effect in a statistical analysis, but cost power (see below) and, and we can't measure everything.

::: column-margin
Variables that we include but that are not interesting to us are often called nuisance variables.
:::

## Definition and bias of variables

A common mistake at this step of the design is to take the variable definition and measurements for granted, and continue with considering the selection of replicates and so on. The step that is missing is to think about the following two questions.

::: column-margin
The consideration of these two questions is often referred to as construct validity, see also main RS script.
:::

-   Do my variables measure what I want to measure
-   What is the expected statistical (stochastic) error in my measurements, and what is the possible systematic error in my measurements

![Illustration of a randomized block design, the probably most widely used design on (observational) experiments to randomize the effect of unknown and unmeasured confounding variables. The idea of this design is that the unknown variables are likely correlated in space. By blocking all experimentally changing variables together, we avoid that they can become confounded with the unknown spatial variables.](RandomizedBlockDesign.png)

The first item may seem a bit odd, because one would think that we know what we measure. However, in many cases in ecological statistics and beyond, we do not measure directly the variable that we are interested in, but rather a proxy. So, for example, we want temperature on the plot, and we use temperature from a weather station 5 km away. Or, we want to look at functional diversity, but how can we exactly express this in terms of variables that we measure in the field.

The second questions relates to considering how much two measurements would differ if we do them repeatedly (stochastic), and how much measurements could be off systematically (e.g. because a method or instrument is systematically wrong, or because humans show particular biases).

::: column-margin 
See main RS script for more info on biases.
:::

## Selection of values for the independent (predictor) variables

If we have decided which variables to measure / vary, we have to decide for the values at which we want to measure them. 

In an experimental study, we usually vary variables systematically for a particular entity, e.g. a plant, a pot or a plot. This entity is called the experimental unit. Also observational studies have experimental units (the entities for which measurements are taken), but it usually not possible to completely control the variables. However, one usually has the option to make particular selections. Also in observational studies, it is key to ensure sufficient sufficient variation of the predictor variables across the experimental units to allow a meaningful statistical analysis.

::: column-margin  
The experimental unit is the entity that can be assigned a particular variable combination (e.g. treatment or control). Example: an individual plant, or a pot.
:::

Here are a few points to consider

### Vary all variables independently

A common problem in practice is that we have two variables,  but their values change in a correlated way. Imagine we test for the presence of a species, but we have only warm dry and cold wet sites. We say the two variables a collinear. In this case we don't know whether any observed effect is due to temperature or water availability. The bottom-line: if you want to separate two effects, the correlation between them must not be perfect - ideally, it would be zero, or failing that, as low as possible. 

### Interactions

To be able to detect interactions between variables, it's not enough to vary all, you also need to have certain combinations. The buzzword here is (fractional) factorial designs. Google will help you.

### Nonlinear effects

The connection of two points is a line. If you want to see whether the response to a variable is nonlinear, you therefore need more than two values of each variable.  


## How many replicates?

We said before that the significance level $\alpha$ is the probability of finding false positives. This is called the type I error. There is another error we can make: failing to find significance for a true effect. This is called the type II error, and the probability of finding an effect is called power. 

::: column-margin  
Power is the probability of finding significance for an effect if it's there. 
:::

For standard statistical methods, power can be calculated. You have to look it up for your particular method, but in general assume that 

* Power goes up with increasing effect size
* Power goes down with increasing variability in the response

This means that, unlike for the type I error which is fixed, calculation of power requires knowledge about the expected effect and the variability. This sounds really bad, but in most cases you can estimate from previous experience how much variation there will be, and in most cases you also know how big the effect has to be at least to be interesting. Based on that, you can then calculate how many samples you need.


