```{r, include=FALSE}
set.seed(42)
```

# Multiple regression

![](resources/mlm.png)

We want to understand what happens if there are more variables in the system that also affect the response and maybe also other predictors.

The lm can be then extended to:

$$
y = a_0 + a_1*x_1 + a_2*x_2
$$

This is important because of the omitted variable bias: If there is a confounder which has an effect on the predictor and the response, and we don't condition the model on it, the effect will be absorbed by the predictor, potentially causing a spurious correlation. Conditioning means that we need to include the variables even though we are not really interested in it! (Those variables are called *nuisance parameters*.)

In the worst case it can lead to a Simpson's paradox: An unobserved variable purports the effect of a predictor on the response variable and removes the predictor's effect or even changes its direction in the opposite direction to the true correlation.

## Confounder

Confounders have effects on the response and another predictor.

```{r}
Climate = runif(100)
Temp = Climate + rnorm(100, sd = 0.2)
Growth = 0.5*Temp - 1.0*Climate + rnorm(100, sd = 0.2)

summary(lm(Growth~Temp))
summary(lm(Growth~Temp+Climate)) # correct effects!!
```

Identifying confounders is the most important challenge in observational studies: For example, several studies showed that overweight adults have lower mortality. However, another study shows that these earlier results might have come up due to confounding: smoking!

-   smokers: higher mortality and lower BMI -\> people with lower BMI have higher mortality rates

-   When we correct for the confounder *smoking*, the correlation between BMI and mortality goes in the other direction, i.e. obese people have higher mortality!

Confounders can even lead to observed correlations where in reality there is no such correlation. This is called *spurious correlation*.

::: callout-warning
Conclusion: Confounders can cause correlations where no causal relationship exists.
:::

## Multiple LM

The multiple linear regression can deal with confounders:

-   Univariate (simple) linear regression describes how y depends on x using a polynomial of x1 e.g.: $$
    y = a_0 + a_1*x_1 + a_2*x_1^2
    $$

-   Multiple linear regression expands simple linear regression to a polynomial of several explanatory variables x1, x2... e.g.: $$
    y = a_0 + a_1*x_1 + a_2*x_2 + a_3*x_3
    $$

-   Idea: if we jointly consider "all" variables in the model formula, the influence of confounding variables is incorporated

```{r}
## first remove observations with NA values
newAirquality = airquality[complete.cases(airquality),]
summary(newAirquality)

# simple regression
m0 = lm(Ozone ~ Temp , data = newAirquality)
summary(m0)
plot(m0)
plot(Ozone ~ Temp , data = newAirquality)
abline(m0, col = "blue", lwd = 3)

# Today: multiple linear regression
m1 = lm(Ozone ~ Temp + Wind , data = newAirquality)
# have a look at the residuals:
op <- par(mfrow = c(2,2))
plot(m1)
par(op)

summary(m1)

# plotting multiple regression outputs
library(effects)
plot(allEffects(m1))


## Omitted variable bias
both = lm(Ozone ~ Wind + Temp, newAirquality)
wind = lm(Ozone ~ Wind , newAirquality)
temp = lm(Ozone ~ Temp, newAirquality)
summary(both)
summary(wind)

slopes <- data.frame(
  predictor = c("Wind", "Temp"),
  both.pred = round(coef(both)[2:3], digits = 2),
  only.wind = c(round(coef(wind)[2], digits = 2), "NA"),
  only.temp = c("NA", round(coef(temp)[2], digits = 2))
)
slopes

```

Omitting Wind makes the effect of Temperature larger.

**Problem:** Multiple regression can separate the effect of collinear explanatory variables, but only, if collinearity is not too strong.

**Solution:** If the correlation is really strong, we can omit one variable and interpret the remaining collinear variable as representing both.

## Interactions between variables

If one predictor influences the effect of the other predictor, we can include an interaction term into our model:

$$
y \sim a + b + a:b
$$

or:

$$
y \sim a*b
$$

```{r}
# Include interaction
m2 = lm(Ozone ~  scale(Wind)* scale(Temp) , data = newAirquality)
# if including interactions, always scale your predictor variables!
# scale: subtracts the mean and divides by standard deviation
summary(m2)
op <- par(mfrow = c(2,2))
plot(m2)
par(op)
```

The influence of temperature on growth depends on the amount of precipitation, or: If there's not enough water, also higher temperatures cannot increase growth.

**Example:**

```{r}
# How does everything change, if we have factorial predictors?
newAirquality$MonthFactor = as.factor(newAirquality$Month)

m4 = lm(sqrt(Ozone) ~ MonthFactor + scale(Wind) * scale(Temp) * scale(Solar.R) , 
        data = newAirquality)
summary(m4)

m5 = lm(sqrt(Ozone) ~ MonthFactor + scale(Wind) + scale(Temp) + scale(Solar.R) 
                      + scale(Wind):scale(Temp)
                      + scale(Wind):scale(Solar.R)
                      + scale(Temp):scale(Solar.R), 
        data = newAirquality)
summary(m5)

# short form for including only two-way interactions:

m5 = lm(sqrt(Ozone) ~ MonthFactor + (scale(Wind) + scale(Temp) + scale(Solar.R))^2,
        data = newAirquality)
summary(m5)
# get overall effect of Month:
anova(m5)
# this is doing a type I ANOVA = sequential
# order in which you include the predictors changes the estimates and p-values

# If you want to do a type II ANOVA, use ANova() from the car package
library(car)
Anova(m5) # Anova with capital A
#type II ANOVA: all other predictors have already been taken into account
# Does an additional predictor explain some of the variance on top of that?
```

## Model selection

We've learned that we should include variables in the model that are collinear, that is they correlate with other predictors, but how many and which factors should we include?

Famous example: [Female hurricanes are deadlier than male hurricanes (Jung et al., 2014)](https://www.pnas.org/doi/abs/10.1073/pnas.1402786111)

They have analyzed the number of fatalities of hurricane and claimed that there is an effect of femininity of the name on the number of deads (while correcting for confounders). They recommend to give hurricans only male names because it would considerably reduce the number of deads.

```{r}
library(DHARMa)
library(effects)
?hurricanes
str(hurricanes)

library(glmmTMB)

m1 = glmmTMB(alldeaths ~ MasFem*
                             (Minpressure_Updated_2014 + scale(NDAM)),
                           data = hurricanes, family = nbinom2)
summary(m1)
```

Interactions -\> we need to scale variables:

```{r}
m2 = glmmTMB(alldeaths ~ scale(MasFem)*
                             (scale(Minpressure_Updated_2014) + scale(NDAM)+scale(sqrt(NDAM))),
                           data = hurricanes, family = nbinom2)
summary(m2)
```

The effect of femininity is gone! Already with the scaled variables, but also with the transformation with the NDAM variable. The question was raised which of both is more reasonable, whether the relationship between damage and mortality isn't a straight line or that the gender of the hurricane names affect deaths ([Bob O'Hara and GrrlScientist](https://www.theguardian.com/science/grrlscientist/2014/jun/04/hurricane-gender-name-bias-sexism-statistics)). They argue that the model with the transformed variable fits the data better which brings us to the topic of this section, how to choose between different models? Answering this question if the goal of model selection.

Why not include all the variables we can measure in our model? Problem with the full model:

-   If you have more parameters than data points, the model cannot be fitted at all

-   Even with n (samples) \~ k (number of parameters), model properties become very unfavorable (high p-values and uncertainties/standard errors) --\> Overfitting

A "good model" depends on the goal of the analysis, do we want to optimize:

-   Predictive ability -- how well can we predict with the model?

-   Inferential ability -- do we identify the true values for the parameters (true effects), are the p-values correct, can we correctly say that a variable has an effect?

The more complex a model gets, the better it fits to the data, but there's a downside, the bias-variance tradeoff.

[Explanation bias-variance tradeoff](https://theoreticalecology.github.io/AdvancedRegressionModels/3C-ModelSelection.html#the-bias-variance-trade-off)

[Explanation LRT and AIC](https://theoreticalecology.github.io/AdvancedRegressionModels/3C-ModelSelection.html#model-selection-methods)

[Problem of p-hacking](https://theoreticalecology.github.io/AdvancedRegressionModels/3C-ModelSelection.html#pHacking)

**Example:**

```{r}
#| eval: false
# Compare different competing models:
# let's compare models m3 and m5 to decide which one explains our data better:
# 1. LRT
anova(m3, m5)
# RSS = residual sum of squares = variance not explained by the model
# smaller RSS = better model
# p-value

#2. AIC
AIC(m3)
AIC(m5)
# also here, model m5 is better


#### Demonstration: Why interpretation of effect sizes and p-values 
### after extensive model selection is not a good idea:
library(MASS)
set.seed(1)
#make up predictors:
dat = data.frame(matrix(runif(20000), ncol = 100))
# create a response variable
dat$y = rnorm(200)
fullModel = lm(y ~ ., data = dat)
sum <- summary(fullModel)
mean(sum$coefficients[,4] < 0.05)
# 0.019: less than 2 % false positives = type I error rate

selection = stepAIC(fullModel)
sum.sel <- summary(selection)
mean(sum.sel$coefficients[,4] < 0.05)
# 0.48: Now almost 50 % of our results are false positives!!!

```

## Exercises

| Formula      | Meaning                               | Details                                    |
|--------------|---------------------------------------|--------------------------------------------|
| `y~x_1`      | $y=a_0 +a_1*x_1$                      | Slope+Intercept                            |
| `y~x_1 - 1`  | $y=a_1*x_1$                           | Slope, no intercept                        |
| `y~I(x_1^2)` | $y=a_0 + a_1*(x_1^2)$                 | Quadratic effect                           |
| `y~x_1+x_2`  | $y=a_0+a_1*x_1+a_2*x_2$               | Multiple linear regression (two variables) |
| `y~x_1:x_2`  | $y=a_0+a_1*(x_1*x_2)$                 | Interaction between x~1~ and x~2~          |
| `y~x_1*x_2`  | $y=a_0+a_1*(x_1*x_2)+a_2*x_1+a_3*x_2$ | Interaction and main effects               |

: Formula syntax

In this exercise you will:

-   perform multiple linear regressions
-   interpret regression output and check the residuals
-   plot model predictions including interactions

Before you start, remember to clean your global environment (if you haven't already) using *rm(list=ls())*.

To conduct the exercise, please load the following packages:

```{r, eval=FALSE}
library(effects)
library(MuMIn)
```

You will work with the following datasets:

-   mtcars
-   Cement{MuMIn}

The second dataset is from the MuMIn package (as shown by the curly brackets).

#### Useful functions

for multiple linear regression

*lm()* - fit linear model\
*summary(fit)* - apply to fitted model object to display regression table\
*plot(fit)* - plot residuals for model validation\
*anova(fit)* - apply type I ANOVA (variables included sequentially) to model to test for effects all levels of a factor\
*Anova(fit)* - *car* package; use type II ANOVA (effects for predictors when all other predictors are already included) for overall effects\
*scale()* - scale variable\
*sqrt()* - square-root\
*log()* - calculates natural logarithm\
*plot(allEffects(fit))* - apply to fitted model object to plot marginal effect; *effects* package\
*par()* - change graphical parameters\
use *oldpar \<- par(mfrow = c(number_rows, number_cols))* to change figure layout including more than 1 plot per figure\
use *par(oldpar)* to reset graphic parameter

for model selection

*stepAIC(fullModel)* - perform stepwise AIC model selection; apply to full model object, *MASS* package\
*dredge(fullModel)* - perform global model selection, *MuMIn* package\
*model.avg()* - perform model averaging\
*AIC(fit)* - get AIC for a fitted model\
*anova(fit1, fit2)* - compare two fitted models via Likelihood Ratio Test (LRT)

### Analyzing the *mtcars* dataset

Imagine a start up company wants to rebuild a car with a nice retro look from the 70ies. The car should be modern though, meaning the fuel consumption should be as low as possible. They've discovered the *mtcars* dataset with all the necessary measurements and they've somehow heard about you and your R skills and asked you for help. And of course you promised to help, kind as you are.

The company wants you to find out which of the following characteristics affects the fuel consumption measured in miles per gallon (*mpg*). Lower values for *mpg* thus reflect a higher fuel consumption. The company wants you to include the following variables into your analysis:

-   number of cylinders (*cyl*)
-   weight (*wt*)
-   horsepower (*hp*)
-   whether the car is driven manually or with automatic (*am*)

In addition, Pawl, one of the founders of the company suggested that the effect of weight (*wt*) might be irrelevant for powerful cars (high *hp* values). You are thus asked to test for this interaction in your analysis as well.

::: {.callout-caution icon="false"}
#### Question

Carry out the following tasks:

-   Perform a multiple linear regression (change class for *cyl* and *am* to factor)
-   Check the model residuals
-   Interpret and plot all effects

You may need the following functions:

-   as.factor()
-   lm()
-   summary()
-   anova()
-   plot()
-   allEffects()

Use your results to **answer the questions on elearning-extern** (Q 1-2) [("05_Test for Exercise in R - multiple regression")](https://elearning.uni-regensburg.de/mod/quiz/view.php?id=2046701).
:::

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

This is the code that you need to interpret the results.

```{r mtcars}
# change am and cyl from numeric to factor
mtcars$am <- as.factor(mtcars$am)
mtcars$cyl <- as.factor(mtcars$cyl)

# multiple linear regression and results:
# (we need to scale (standardize) the predictors wt and hp, since we include their interaction)
carsfit <- lm(mpg ~ am + cyl + scale(wt) * scale(hp), dat = mtcars)
# weight is included as the first predictor in order to have
# it as the grouping factor in the allEffects plot

summary(carsfit)
# The first level of each factor is used as a reference, i.e. in this case a manual gear shift with 4 gears.
# From the coefficient cyl6 we see that there is no significant difference in fuel consumption (= our response) between 4 gears (the reference) and 6 gears.
# In contrast, the predictors weight (wt) and horsepower (hp) have a significant negative effect on the range (mpg), so that they both increase fuel consumption.

# check residuals
old.par = par(mfrow = c(2, 2))
plot(carsfit)
par(old.par)

# plot effects
plot(allEffects(carsfit))
# We can see in the wt*hp plot, that for high values of hp wt has no effect on the response mpg. We conclude that Pawl was right.
```
:::

### Model-selection with the *Cement* dataset

The process of cement hardening involves exogenous chemical reactions and thus produces heat. The amount of heat produced by the cement depends on the mixture of its constituents. The *Cement* dataset includes heat measurements for different types of cement that consist of different relative amounts of calcium aluminate (*X1*), tricalcium silicate (*X2*), tetracalcium alumino ferrite (*X3*) and dicalcium silicate (*X4*). A cement producing company wants to optimize the composition of its product and wants to know, which of these compounds are mainly responsible for heat production.

::: callout-warning
We only do a model selection here for educational reasons. For your analysis, and if your goal is not a predictive model, think about the model structure before you do the analysis and then stick to it! See here the section about [p-hacking](https://theoreticalecology.github.io/AdvancedRegressionModels/3C-ModelSelection.html#pHacking) (and also consider that AIC selection will/can remove confounders which will violate causality and can lead to spurious correlations!
:::

::: {.callout-caution icon="false"}
#### Questions

Carry out the following tasks:

-   Perform a multiple linear regression including all predictor variables and all two-way interactions (remember the notation *(var1 + var2 + var3)\^2*.
-   Perform forward, backward, and global model selection and compare the results
-   Fit the model considered optimal by global model selection and compare it with the full model based on AIC (or AICc) and LRT.

You may need the following functions:

-   lm()
-   summary()
-   stepAIC()
-   options()
-   dredge()
-   AIC() or AICc() (for small datasets)
-   anova()

Use your results to **answer the questions on elearning-extern** (Q 3-5) [("05_Test for Exercise in R - multiple regression")](https://elearning.uni-regensburg.de/mod/quiz/view.php?id=2046701).
:::

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

This is the code that you need to obtain the results.

```{r  Cement}
#| eval: false

library(MuMIn)
library(MASS)

# full model ->  has 11 coefficients
full = lm(y ~ (X1 + X2 + X3 + X4)^2, data = Cement)
summary(full)

# forward model selection
ms_forw = stepAIC(full, direction = "forward")
summary(ms_forw)
# lists 11 coefficients (i.e. selects full model)

# backward model selection
ms_back = stepAIC(full, direction = "backward")
summary(ms_back)
# lists 10 coefficients

# global model selection
options(na.action = "na.fail")
dd = dredge(full)
head(dd)
# The first row lists the best performing model: it includes only the intercept and effects for X1 and X2 (= 3 coefficients).

# Fit the model considered optimal by global model selection and compare it with the full model based on AIC (or AICc) and LRT:
opt = lm(y ~ X1 + X2, data = Cement)
summary(opt)

AIC(opt,full) # full model is better according to AIC (lower AIC)
anova(opt, full) # -> LRT: no significant difference between the models

# sample size in the Cement dataset:
str(Cement)  # or
nrow(Cement)

# If the sample size is low, a corrected version of the AIC is recommended to avoid overfitting:
AICc(opt,full) # This is inf! -> optimal model is better according to AICc

```
:::
