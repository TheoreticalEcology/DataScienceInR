```{r, include=FALSE}
set.seed(42)
```

# Multiple regression

![](resources/mlm.png)

We want to understand what happens if there are more variables in the system that also affect the response and maybe also other predictors.

The lm can be then extended to:

$$
y = a_0 + a_1*x_1 + a_2*x_2
$$

This is important because of the omitted variable bias: If there is a confounder which has an effect on the predictor and the response, and we don't condition the model on it, the effect will be absorbed by the predictor, potentially causing a spurious correlation. Conditioning means that we need to include the variables even though we are not really interested in it! (Those variables are called *nuisance parameters*.)

In the worst case it can lead to a Simpson's paradox: An unobserved variable purports the effect of a predictor on the response variable and removes the predictor's effect or even changes its direction in the opposite direction to the true correlation.

## Confounder

Confounders have effects on the response and another predictor.

```{r}
Climate = runif(100)
Temp = Climate + rnorm(100, sd = 0.2)
Growth = 0.5*Temp - 1.0*Climate + rnorm(100, sd = 0.2)

summary(lm(Growth~Temp))
summary(lm(Growth~Temp+Climate)) # correct effects!!
```

Identifying confounders is the most important challenge in observational studies: For example, several studies showed that overweight adults have lower mortality. However, another study shows that these earlier results might have come up due to confounding: smoking!

-   smokers: higher mortality and lower BMI -\> people with lower BMI have higher mortality rates

-   When we correct for the confounder *smoking*, the correlation between BMI and mortality goes in the other direction, i.e. obese people have higher mortality!

Confounders can even lead to observed correlations where in reality there is no such correlation. This is called *spurious correlation*.

::: callout-warning
Conclusion: Confounders can cause correlations where no causal relationship exists.
:::

## Multiple LM

::: callout-note
A linear regression with a quadratic effect (or any polinomial) of X is a "multiple regression" in the sense that the squared X can be considered another "variable": $$  y = a_0 + a_1*x_1 + a_2*x_1^2  $$
:::

Multiple linear regression expands simple linear regression to a polynomial of several explanatory variables x1, x2... e.g.: $$
    y = a_0 + a_1*x_1 + a_2*x_2 + a_3*x_3
    $$

-   Idea: if we jointly consider "all" variables in the model formula, the influence of confounding variables is incorporated.

Let's see an example with the `airquality` data:

```{r}
## first remove observations with NA values
newAirquality = airquality[complete.cases(airquality),]
summary(newAirquality)
```

A simple regression:

```{r}
# simple regression
m0 = lm(Ozone ~ Temp , data = newAirquality)
summary(m0)
# have a look at the residuals:
op <- par(mfrow = c(2,2))
plot(m0)
par(op)

plot(Ozone ~ Temp , data = newAirquality)
abline(m0, col = "blue", lwd = 3)
```

A multiple linear regression of Ozone affected by temperature and wind:

```{r}
# multiple linear regression
m1 = lm(Ozone ~ Temp + Wind , data = newAirquality)
summary(m1)
# have a look at the residuals:
op <- par(mfrow = c(2,2))
plot(m1)
par(op)
```

To vizualize the effects of each predictor on the ozone, we can use the package `effects`. The package will plot the effects of each variable separately (plots called partial slopes) "controling" for the other variables in the model. Controling here means that for ploting the effect of `Temp`, the variable `Wind` was fixed at it's average value (the fixed value can be changed by the user, but the default is to take the mean of a continuous variable).

A predictor eﬀect plot summarizes the role of a selected focal predictor in a fitted regression model. These graphs are an alternative to tables of fitted coeﬃcients, which can be much harder to interpret than predictor eﬀect plots. (Info taken from the vignette of the `effects`package, take a look at it!).

```{r}
# plotting multiple regression outputs
library(effects)
plot(allEffects(m1))
```

Let's interprete the partial slope plot for `Temp`. It shows that there is a positive relationship of temperature and Ozone. The shaded blue area

The intercept of the line aﬀects only the height of the line, and is determined by the choices made for averaging over the fixed predictors, but for any choice of averaging method, the slope of the line would be the same (because the model has no interactions between variables - next topic). The shaded area is a pointwise confidence band for the fitted values, based on standard errors computed from the covariance matrix of the fitted regression coeﬃcients. The rug plot at the bottom of the graph shows the location of the `Temp` data values.

If we omit `Wind` will we have a different effect of `Temp` in Ozone?

```{r}
## Omitted variable bias
both = lm(Ozone ~ Wind + Temp, newAirquality)
wind = lm(Ozone ~ Wind , newAirquality)
temp = lm(Ozone ~ Temp, newAirquality)
#summary(both)
summary(wind)

slopes <- data.frame(
  predictor = c("Wind", "Temp"),
  both.pred = round(coef(both)[2:3], digits = 2),
  only.wind = c(round(coef(wind)[2], digits = 2), "NA"),
  only.temp = c("NA", round(coef(temp)[2], digits = 2))
)
slopes

```

Yes, omitting Wind makes the effect of Temperature larger.

**Problem:** Multiple regression can separate the effect of collinear explanatory variables, but only, if collinearity is not too strong.

**Solution:** If the correlation is really strong, we can omit one variable and interpret the remaining collinear variable as representing both.

## Interactions between variables

If one predictor influences the effect of the other predictor, we can include an interaction term into our model:

$$
y \sim a + b + a:b
$$

or:

$$
y \sim a*b
$$

Let's incude the interaction between `Wind` and `Temp`

```{r}
# Include interaction
m2 = lm(Ozone ~  scale(Wind)* scale(Temp) , data = newAirquality)
# if including interactions, always scale your predictor variables!
# scale: subtracts the mean and divides by standard deviation
summary(m2)
op <- par(mfrow = c(2,2))
plot(m2)
par(op)
```

The influence of temperature on ozone depends on the amount of wind. When wind is low, the relationship is strongly positive, but when wind is high this relationship becomes slightly negative.

```{r}
plot(predictorEffect("Temp", m2,xlevels=3))
```

## Model selection

We've learned that we should include variables in the model that are collinear, that is they correlate with other predictors, but how many and which factors should we include?

Famous example: [Female hurricanes are deadlier than male hurricanes (Jung et al., 2014)](https://www.pnas.org/doi/abs/10.1073/pnas.1402786111)

They have analyzed the number of fatalities of hurricane and claimed that there is an effect of femininity of the name on the number of deads (while correcting for confounders). They recommend to give hurricans only male names because it would considerably reduce the number of deads.

```{r, warning = F}
# loading packages
library(EcoData)
library(DHARMa)
library(glmmTMB) # package for the model
library(effects)
#?hurricanes
#str(hurricanes)
```

We will model the number of deaths, so we are going to use a different distribution (Negative Binomial). This is a generalized linear model (GLM), that you will learn about in our next class. So, let's focus on the model selection produre:

```{r, warning=F}
m1 = glmmTMB(alldeaths ~ MasFem*
                      (Minpressure_Updated_2014 + scale(NDAM)),
                           data = hurricanes, family = nbinom2)
summary(m1)
```

We have **Interactions** -\> we need to scale variables:

```{r}
m2 = glmmTMB(alldeaths ~ scale(MasFem)*
                             (scale(Minpressure_Updated_2014) + scale(NDAM)+scale(sqrt(NDAM))),
                           data = hurricanes, family = nbinom2)
summary(m2)
```

The effect of femininity is gone! Already with the scaled variables, but also with the transformation with the NDAM variable. The question was raised which of both models is more reasonable, whether the relationship between damage and mortality isn't a straight line or that the gender of the hurricane names affect deaths ([Bob O'Hara and GrrlScientist](https://www.theguardian.com/science/grrlscientist/2014/jun/04/hurricane-gender-name-bias-sexism-statistics)). They argue that the model with the transformed variable fits the data better which brings us to the topic of this section, how to choose between different models? Answering this question if the goal of model selection.

Why not include all the variables we can measure in our model? Problem with the full model:

-   If you have more parameters than data points, the model cannot be fitted at all

-   Even with n (samples) \~ k (number of parameters), model properties become very unfavorable (high p-values and uncertainties/standard errors) --\> Overfitting

A "good model" depends on the goal of the analysis, do we want to optimize:

-   Predictive ability -- how well can we predict with the model?

-   Inferential ability -- do we identify the true values for the parameters (true effects), are the p-values correct, can we correctly say that a variable has an effect?

The more complex a model gets, the better it fits to the data, but there's a downside, the bias-variance tradeoff.

[Explanation bias-variance tradeoff](https://theoreticalecology.github.io/AdvancedRegressionModels/3C-ModelSelection.html#the-bias-variance-trade-off)

[Explanation LRT and AIC](https://theoreticalecology.github.io/AdvancedRegressionModels/3C-ModelSelection.html#model-selection-methods)

[Problem of p-hacking](https://theoreticalecology.github.io/AdvancedRegressionModels/3C-ModelSelection.html#pHacking)

```{r, echo=F}
#| eval: false
# this script doesn't make sense in this file - maybe some leftovers> there is no m3 or m5 before

# Compare different competing models:
# let's compare models m3 and m5 to decide which one explains our data better:
# 1. LRT
anova(m3, m5)
# RSS = residual sum of squares = variance not explained by the model
# smaller RSS = better model
# p-value

#2. AIC
AIC(m3)
AIC(m5)
# also here, model m5 is better
```

**Demonstration**: Why interpretation of effect sizes and p-values after extensive model selection is not a good idea:

```{r}
#| eval: false
####:
library(MASS)
set.seed(1)
#make up predictors:
dat = data.frame(matrix(runif(20000), ncol = 100))
# create a response variable
dat$y = rnorm(200)
fullModel = lm(y ~ ., data = dat)
sum <- summary(fullModel)
mean(sum$coefficients[,4] < 0.05)
# 0.019: less than 2 % false positives = type I error rate

selection = stepAIC(fullModel)
sum.sel <- summary(selection)
mean(sum.sel$coefficients[,4] < 0.05)
# 0.48: Now almost 50 % of our results are false positives!!!

```

## Formula syntax

| Formula | Meaning | Details |
|----|----|----|
| `y~x_1` | $y=a_0 +a_1*x_1$ | Slope+Intercept |
| `y~x_1 - 1` | $y=a_1*x_1$ | Slope, no intercept |
| `y~I(x_1^2)` | $y=a_0 + a_1*(x_1^2)$ | Quadratic effect |
| `y~x_1+x_2` | $y=a_0+a_1*x_1+a_2*x_2$ | Multiple linear regression (two variables) |
| `y~x_1:x_2` | $y=a_0+a_1*(x_1*x_2)$ | Interaction between x~1~ and x~2~ |
| `y~x_1*x_2` | $y=a_0+a_1*(x_1*x_2)+a_2*x_1+a_3*x_2$ | Interaction and main effects |

: Formula syntax
