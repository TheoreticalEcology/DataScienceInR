---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
set.seed(42)
```

# Exercise - NHST and statistical tests {-}

[Decision tree for statistical tests](https://elearning.uni-regensburg.de/course/view.php?id=54766#coursecontentcollapse3)

## Streams

The dataset 'streams' contains water measurements taken at different location along 16 rivers: 'up' and 'down' are water quality measurements of the same river taken before and after a water treatment filter, respectively. We want to find out if this water filter is effective. Use the decision tree to identify the appropriate test for this situation.

The filters were installed between upstream ('up') and downstream ('down').

::: {.column-margin}

**Hint:**
Are the up and down observations independent? (paired or unpaired)

:::

```{r}
dat = read.table("https://raw.githubusercontent.com/biometry/APES/master/Data/Simone/streams.txt", header = T)
```

::: {.callout-warning}
### 1. Task


**1. Visualize the data**

Hints: categorical and numerical..(two column == two levels)

`r hide("Click here to see the solution")`

`matplot()` can be used to plot several lines at once

```{r}
#| warning: false
par(mfrow = c(1,2))
boxplot(dat, notch = TRUE)

col_num = as.integer(dat[,2] > dat[,1]) + 1

matplot(t(dat), 
        type = "l", 
        las = 1, 
        lty = 1, 
        col = c("#FA00AA", "#1147AA")[col_num])
legend("topleft", legend = c("worse", "better"), lty = 1, col = c("#FA00AA", "#1147AA"), bty = "n")
par(mfrow = c(1,1))
```

`r unhide()`

:::

::: {.callout-warning}
### 2. Task

**2.  For identifying an appropriate test for the effect of the water treatment filter, what are your first two choices in the decision tree?**

```{r}
#| include: false
opts_p <- c(
   "Two groups, unpaired observations",
   answer = "Two groups, paired observations",
   "Three or more groups, unpaired observations",
   "Three or more groups, paired observations"
)
```
  `r longmcq(opts_p)`
  
`r hide("Feedback")`
The number of groups to compare is **two**, up versus down stream. The observations are **paired** because the water tested up and down stream of the filter is not independent from each other, i.e. the "same" water is measured twice!
`r unhide()`

:::

::: {.callout-warning}
### 3. Task

**3.  The next decision you have to make is whether you can use a parametric test or not. Apply the Shapiro-Wilk test to check if the data are normally distributed. Are the tests significant and what does that tell you?**
```{r}
#| include: false
opts_p <- c(
   answer = "One test is significant. The downstream data significantly deviate from a normal distribution. The upstream data does not significantly deviate from a normal distribution.",
   "Both tests are significant. Both data significantly deviate from a normal distribution.",
   "One test is significant. The downstream data significantly deviate from a normal distribution. The upstream data does not significantly deviate from a normal distribution."
)
```
`r longmcq(opts_p)`

`r hide("Click here to see the solution")`

The Shapiro-Wilk test is significant (p < 0.05) for down stream data, i.e. we reject H0 (the data is normally distributed). Thus, the data significantly deviate from a normal distribution. The test is not significant for upstream data; the data does not significantly deviate from a normal distribution.

```{r}
shapiro.test(dat$down)
shapiro.test(dat$up)
```

`r unhide()`

:::

::: {.callout-warning}
### 4. Task

**4.  Which test is appropriate for evaluating the effect of the filter?**

```{r}
#| include: false
opts_p <- c(
  answer="Wilcoxon signed rank test",
  "Student t-test",
  "Welch t-test",
  "Mann-Whitney U test",
  "Paired t-test"
)
```
`r longmcq(opts_p)`

`r hide("Feedback")`
We select a **Wilcoxon signed rank test** that is appropriate to compare not-normal, paired observations in two groups.
`r unhide()`

:::

::: {.callout-warning}
### 5. Task

**5.  Does the filter influence the water quality? (The warnings are related to identical values, i.e. ties, and zero differences; we ignore these here) Use to appropriate test to answer this question!**

```{r}
#| include: false
opts_p <- c(
  answer="The filter significantly influences water quality (Wilcoxon signed rank test, p = 0.00497).",
  answer="Equal or greater rank differences between the pairs would occur under H0 (no effect of the filter) only with a probability of 0.497 %.",
  "We could prove an effect of the water filter with >95% certainty.",
  "The filter has no significant influence on the water quality (Wilcoxon signed rank test, p = 0.00497)."
)
```
`r longmcq(opts_p)`

`r hide("Click here to see the solution")`

H0 of the Wilcoxon signed rank test is that the location shift between the two groups equals zero, i.e. the difference between the pairs follows a symmetric distribution around zero. As p < 0.05, we can reject H0. The filter significantly influences water quality. (In case of ties also see the function `wilcox.test()` in the package coin for exact, asymptotic and Monte Carlo conditional p-values)

```{r}
#| warning: false
wilcox.test(dat$down, dat$up, paired = TRUE)
```


`r unhide()`

:::

::: {column-margin}
H_0_ Hypothesis:

- Shapiro: Data is normal distributed
- Wilcoxon: No differences in their ranks
:::


## Chicken

The 'chickwts' experiment was carried out to measure and compare the effectiveness of various feed supplements on the growth rate of chickens. We are interested in two questions: Does the feed type influence the chickens weight at all? Which feed types result in significantly different chicken weights?

```{r}
dat = chickwts
```

::: {.callout-warning}
### 1. Task

Analyze the data and answer the following questions.

**1.  Visualize the data. What is an appropriate plot for this kind of data?**
```{r}
#| include: false
opts_p <- c(
  "Histogram",
  "Mosaicplot",
  "Scatterplot",
  answer = "Boxplot"
)
```
`r longmcq(opts_p)`

`r hide("Click here to see the solution")`

An appropriate visualization for one numeric and one categorial variable is a **boxplot**. Using `notch = T` in the function boxplot(), adds confidence interval for the median (the warning here indicates that we are not very confident in the estimates of the medians as the number of observations is rather small, you can see at the notches that go beyond the boxes).

```{r}
dat = chickwts
boxplot(weight ~ feed, data = dat)
boxplot(weight ~ feed, data = dat, notch = T)
```

`r unhide()`

:::

::: {.callout-warning}
### 2. Task

**2.  Can you apply an ANOVA to this data? What are the assumptions for an ANOVA? Remember: you have to test two things for the groups (for this exercise it is enough if you test the groups "casein" and "horsebean" only).**

```{r}
#| include: false
opts_p <- c(
  "We have no indication to assume that the data is not-normally distributed or that the variances are different. We should use a Kruskal-Wallis test.",
  "The Shapiro-Wilk test of normality and the test for equal variances are not signficant. We cannot use an ANOVA.",
  answer = "We have no indication to assume that the data is not-normally distributed or that the variances are different. We can use an ANOVA.",
  "Boxplot"
)
```
`r longmcq(opts_p)`

`r hide("Click here to see the solution")`
The two requirements for applying an ANOVA are 1) the data in each group are normally distributed, and 2) the variances of the different groups are equal. For 1) we again use a Shapiro-Wilk test. For 2) we can use the function var.test() or for all feed types the function bartlett.test(). All tests are not significant, and we thus have no indication to assume that the data is not-normally distributed or that the variances are different. We can use an ANOVA.

```{r}
# get data of each group
casein = dat$weight[dat$feed == "casein"]
horsebean = dat$weight[dat$feed == "horsebean"]

shapiro.test(casein)
shapiro.test(horsebean)
# H0 normally distributed
# not rejected, normality assumption is okay

var.test(casein, horsebean)
# H0 ratio of variances is 1 = groups have the same variance
# not rejected, same variances is okay


### Extra: testing the assumptions for all groups:

# Normality test using the dplyr package
library(dplyr)
dat %>% 
  group_by(feed) %>% 
  summarise(p = shapiro.test(weight)$p)

# Bartlett test for equal variances
bartlett.test(weight ~ feed, dat)

```
`r unhide()`

:::

::: {.callout-warning}
### 3. Task

**3.  Apply an ANOVA or the non-parametric test. How would you describe the result in a thesis or publication?**

```{r}
#| include: false
opts_p <- c(
  "The feed type significantly influences the chicken weight (ANOVA, p = 5.94e-10).",
  "The chicken weights differ significantly between the six feed types (ANOVA, p = 5.94e-10).",
  answer = "The feed type significantly influences the chicken weight (ANOVA, p = 5.94e-10).",
  "Boxplot"
)
```
`r longmcq(opts_p)`


`r hide("Click here to see the solution")`

H0 of the ANOVA is that feed has no influence on the chicken weight. As p < 0.05, we reject H0. In the result section, we would write something like: "The feed type significantly influenced the chicken weight (ANOVA, p = 5.94e-10)."

```{r}
fit = aov(weight ~ feed, data = dat)
summary(fit)
```

The answer "The chicken weights differ significantly between the six feed types (ANOVA, p = 5.94e-10)." is not precise enough - there are significant differences, but an ANOVA doesn't test if this is true for all comparisons. ANOVA only tests globally.

`r unhide()`

:::

::: {.callout-warning}
### 4. Task

**4.  Also apply the alternative test and compare p-values. Which of the tests has a higher power?**

```{r}
#| include: false
opts_p <- c(
  answer = "ANOVA",
  "The alternative test",
  "Both have the same power",
  "The p-value doesn't help us to identify the power."
)
```
`r longmcq(opts_p)`

`r hide("Click here to see the solution")`

The non-parametric alternative of an ANOVA is the Kruskal-Wallis test, which should be applied if the data is not normally distributed. In this example, the test comes to the same conclusion: H0 is rejected, the feed type has a significant effect on the chicken weight. The p-value, however, is not as small as in the ANOVA. The reason for this is that non-parametric tests have a lower power than parametric ones as they only use the ranks of the data. Therefore, the ANOVA is preferred over the non-parametric alternative in case its assumptions are fulfilled. 

```{r}
kruskal.test(chickwts$weight, chickwts$feed)
```

The p-value in the Kruskal-Wallis test is not as small as in the ANOVA. The reason for this is that non-parametric tests have a lower power than parametric ones as they only use the ranks of the data. Therefore, the ANOVA is preferred over the non-parametric alternative in case its assumptions are fulfilled.


`r unhide()`

:::

::: {.callout-warning}
### 5. Task

**5.  Use the result of the ANOVA to carry out a post-hoc test. How many of the pairwise comparisons indicate** **significant differences between the groups?** `r fitb(8)`


`r hide("Click here to see the solution")`

```{r}
TukeyHSD(fit)
```

You can also summarize this more formally:

```{r}
aov_post <- TukeyHSD(fit)
sum(aov_post$feed[,4] < 0.05)
```

`r unhide()`

:::

::: {.callout-warning}
### 6. Task

**6.  Which conclusion about the feed types 'meatmeal' and 'casein' is correct?**

```{r}
#| include: false
opts_p <- c(
  "It is statistically proven that the feed types 'meatmeal' and 'casein' produce the same chicken weight.",
  "The Null-hypothesis, i.e. there is no weight difference between the feed types 'meatmeal' and 'casein', is accepted.",
  answer = "The experiment did not reveal a significant weight difference between the feed types 'meatmeal' and 'casein'."
)
```
`r longmcq(opts_p)`

`r hide("Feedback")`
The experiment did not reveal a significant weight difference between the feed types 'meatmeal' and 'casein'. Remember that we cannot prove or accept H0; we can only reject it. 



You can also visualize the comparisons using the function `glht()` from the multcomp package.

```{r}
library(multcomp) # install.packages("multcomp")
tuk <- glht(fit, linfct = mcp(feed = "Tukey"))

# extract information
tuk.cld <- cld(tuk)

# use sufficiently large upper margin
old.par <- par(mai=c(1,1,1.25,1), no.readonly = TRUE)

# plot
plot(tuk.cld)
par(old.par)
```

`r unhide()`

:::

## Titanic

The dataset 'titanic' from the EcoData package (not to confuse with the dataset 'Titanic') provides information on individual passengers of the Titanic.

```{r echo=TRUE,message=FALSE, warning=FALSE,results='hide'}
library(EcoData) #or: load("EcoData.Rdata"), if you had problems with installing the package
dat = titanic
```

Answer the following questions.

:::{.callout-warning}
### 1. Task

1.  We are interested in first and second class differences only. Reduce the dataset to these classes only. How can you do this in R?

`r hide("Click here to see the solution")`

The dataset can be reduced in different ways. All three options result in a dataset with class 1 and 2 only. 

```{r echo=TRUE,message=FALSE, warning=FALSE,results='hide'}
library(EcoData)
dat = titanic

dat = dat[dat$pclass == 1 | dat$pclass == 2, ]
dat = dat[dat$pclass %in% 1:2, ] # the same
dat = dat[dat$pclass != 3, ] # the same
```

`r unhide()`

:::

:::{.callout-warning}
### 2. Task

2.  Does the survival rate between the first and second class differ? Hint: you can apply the test to a contigency table of passenger class versus survived, i.e. `table(dat$pclass, dat$survived)`. `r torf(TRUE)`


`r hide("Click here to see the solution")`

We use the test of equal proportions here. H0, proportions in the two groups are equal, is rejected. The survival probability in class 1 and class 2 is significantly different. Note that the estimated proportions are for mortality not for survival because 0=died is in the first column of the table. Thus it is considered the "success" in the prop.test().

```{r}
table(dat$pclass, dat$survived)
prop.test(table(dat$pclass, dat$survived))
```

`r unhide()`


:::


:::{.callout-warning}
### 3. Task

3.  Is the variable passenger age normally distributed?

```{r}
#| include: false
opts_p <- c(
  "The data seems normally distributed.",
  answer = "The distribution significantly differs from normal."
)
```
`r longmcq(opts_p)`

`r hide("Click here to see the solution")`

The distribution of passenger age significantly differs from normal.

```{r}
hist(dat$age, breaks = 20)
shapiro.test(dat$age)
```

`r unhide()`

:::

:::{.callout-warning}
### 4. Task

4.  Is the variable Body Identification Number (body) uniformly distributed?

Uniform distribution == all values within a range are equally distributed:

```{r}
hist(runif(10000))
```

**Hint**:
`ks.test()` (check documentation, see examples)


```{r}
#| include: false
opts_p <- c(
  answer = "The distribution significantly differs from uniform.",
  "The distribution looks uniform."
)
```
`r longmcq(opts_p)`

`r hide("Click here to see the solution")`

The distribution of body significantly differs from uniform.

```{r}
hist(dat$body, breaks = 20)
ks.test(dat$body, "punif")
```

`r unhide()`

:::

:::{.callout-warning}
### 5. Task

5.  Is the correlation between fare and age significant? `r torf(FALSE)`

`r hide("Click here to see the solution")`

The correlation between fare and age is non-significant. You can also plot the data using the scatter.smooth function.

```{r}
cor.test(dat$fare, dat$age)
scatter.smooth(dat$fare, dat$age)
```

`r unhide()`
:::


## Simulation of Type I and II error

*This is an additional task for those who are fast! Please finish the other parts first before you continue here!*

Analogously to the previous example of simulating the test statistic, we can also simulate error rates. Complete the code ...

```{r}
PperGroup = 50
pC = 0.5
pT = 0.5

pvalues = rep(NA, 1000)

for(i in 1:1000){
  control = rbinom(n = 1, size = PperGroup, prob = pC)
  treat = rbinom(n = 1, size = PperGroup, prob = pT)
  #XXXX
}

```

... and answer the following questions for the `prop.test` in R:

:::{.callout-warning}
### 1. Task


1.  How does the distribution of p-values and the number of false positive (Type I error) look like if pC = pT

`r hide("Click here to see the solution")`


```{r}
PperGroup = 50
pC = 0.5
pT = 0.5

pvalues = rep(NA, 1000)
positives = rep(NA, 1000)

for(i in 1:1000){
  control = rbinom(1, PperGroup, prob = pC )
  treatment = rbinom(1, PperGroup, prob = pT )
  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value
  positives[i] = pvalues[i] <= 0.05
}
hist(pvalues)
table(positives)
mean(positives) 

# type I error rate = false positives (if data simulation etc. is performed several times, this should be on average 0.05 (alpha))

```

`r unhide()`

:::

:::{.callout-warning}
### 2. Task

2.  How does the distribution of p-values and the number of true positive (Power) look like if pC != pT, e.g. 0.5, 0.6

`r hide("Click here to see the solution")`

pC != pT with difference 0.1

```{r}
PperGroup = 50
pC = 0.5
pT = 0.6

pvalues = rep(NA, 1000)
positives = rep(NA, 1000)

for(i in 1:1000){
  control = rbinom(1, PperGroup, prob = pC )
  treatment = rbinom(1, PperGroup, prob = pT )
  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value
  positives[i] = prop.test(c(control, treatment), rep(PperGroup, 2))$p.value < 0.05
}
hist(pvalues)
table(positives)
mean(pvalues < 0.05) # = power (rate at which effect is detected by the test)
# power = 1- beta > beta = 1-power = typeII error rate
1-mean(pvalues < 0.05)

## Factors increasing power and reducing type II errors:
# - increase sample size
# - larger real effect size (but this is usually fixed by the system)

```

`r unhide()`

:::

:::{.callout-warning}
### 3. Task

3.  How does the distribution of p-values and the number of false positive (Type I error) look like if you modify the for loop in a way that you first look at the data, and then decide if you test for greater or less?

`r hide("Click here to see the solution")`

You first look at the data, and then decide if you test for greater or less:

```{r}
# ifelse(test,yes,no)
PperGroup = 50
pC = 0.5
pT = 0.5

for(i in 1:1000){
  control = rbinom(1, PperGroup, prob = pC )
  treatment = rbinom(1, PperGroup, prob = pT )
  pvalues[i] = prop.test(c(control, treatment), rep(PperGroup, 2), 
                        alternative= ifelse(mean(control)>mean(treatment),
                                            "greater","less"))$p.value
  positives[i] = prop.test(c(control, treatment), rep(PperGroup, 2),
                     alternative= ifelse(mean(control)>mean(treatment),
                                         "greater","less"))$p.value < 0.05
}
hist(pvalues)
table(positives)
mean(pvalues < 0.05) 
# higher false discovery rate
```


`r unhide()`

:::