---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
set.seed(42)
```

# Exercise

## birdabundance dataset

```{r}
library(EcoData)
library(randomForest)
set.seed(42)
indices = sample.int(nrow(birdabundance), 30)
train = birdabundance[-indices,]
test = birdabundance[indices,]
# ABUND is the response variable
```

:::{.callout-warning}

Task:

-   Fit random forest on train data

-   Predict for test data

-   Calculate R2

-   Do the same with a lm and compare the predictive performance of both models
:::

`r hide("Click here to see the solution")`
```{r rf_birdab}
rf = randomForest(ABUND~., data = train)
m = lm(ABUND~., data = train)

pred1 = predict(rf, newdata = test)
pred2 = predict(m, newdata = test)

cor(pred1, test$ABUND)**2
cor(pred2, test$ABUND)**2
```

RF clearly outperforms the linear regression model!
`r unhide()`

## titantic dataset

```{r}
library(EcoData)
library(randomForest)
library(dplyr)
set.seed(42)
titanic_sub = titanic %>% select(survived, age, pclass, sex, fare)
titanic_sub = titanic_sub[complete.cases(titanic_sub),]

indices = sample.int(nrow(titanic_sub), 200)
train = titanic_sub[-indices,]
test = titanic_sub[indices,]
```

:::{.callout-warning}

Task:

-   Fit random forest on train data

-   Predict for test data

-   Calculate Accuracy

-   Do the same with a glm (binomial) and compare the predictive performance of both models

-   What is the most important variable?

:::

`r hide("Click here to see the solution")`

```{r rf_titanic}
rf = randomForest(as.factor(survived)~., data = train)
m = glm(survived~., data = train, family = binomial)

pred1 = predict(rf, newdata = test)
pred2 = predict(m, newdata = test, type = "response")

# pred2 are probabilities, we have to change them to levels
pred2 = ifelse(pred2 < 0.5, 0, 1)

mean(pred1 == test$survived) # RF
mean(pred2 == test$survived) # glm

```

RF is better than the glm!

```{r}
varImpPlot(rf)
```

Sex is the most important variable!
`r unhide()`

## Bias-variance tradeoff

An important concept of statistics and, in particular, ML is the concept of the bias-variance tradeoff - or in other words, finding the right complexity of the model. So how flexible should our model be so that it generalizes well to other/new observations. Many ML algorithms have complexity parameters (e.g. nodesize or mtry in RF) that control their complexity. Have a look at the following youtube video about the bias-variance tradeoff:

```         
{{< video https://www.youtube.com/watch?v=EuBBz3bI-aA >}}
```

Let's see how we can control the complexity in the Random Forest algorithm:

```{r}
library(randomForest)
set.seed(123)

data = airquality[complete.cases(airquality),]

rf = randomForest(Ozone~., data = data)

pred = predict(rf, data)
importance(rf)
#>         IncNodePurity
#> Solar.R      17969.59
#> Wind         31978.36
#> Temp         34176.71
#> Month        10753.73
#> Day          15436.47
cat("RMSE: ", sqrt(mean((data$Ozone - pred)^2)), "\n")
#> RMSE:  9.507848

plot(data$Temp, data$Ozone)
lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = "red")
```

:::{.callout-warning}

Try different values for the nodesize and mtry and describe how the predictions depend on these parameters. (randomForest(..., nodesize = ..., mtry = ...) ([the exercise was taken from the ML course book](https://theoreticalecology.github.io/machinelearning/fundamental.html#exercises-3))

:::

`r hide("Click here to see the solution")`

```{r rf_complexity}
library(randomForest)
set.seed(123)

data = airquality[complete.cases(airquality),]


for(nodesize in c(1, 5, 15, 50, 100)){
  for(mtry in c(1, 3, 5)){
    rf = randomForest(Ozone~., data = data, mtry = mtry, nodesize = nodesize)
    
    pred = predict(rf, data)
    
    plot(data$Temp, data$Ozone, main = paste0(
        "mtry: ", mtry, "    nodesize: ", nodesize,
        "\nRMSE: ", round(sqrt(mean((data$Ozone - pred)^2)), 2)
      )
    )
    lines(data$Temp[order(data$Temp)], pred[order(data$Temp)], col = "red")
  }
}
```
`r unhide()`

## ML pipeline

If you want to know more about a typical ML pipeline/workflow, [read this chapter from the ML course!](https://theoreticalecology.github.io/machinelearning/workflow.html)
